2021-07-26 22:05:20,204 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-26 22:05:20,344 device: cuda n_gpu: 4
2021-07-26 22:06:09,354 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-26 22:06:09,476 device: cuda n_gpu: 4
2021-07-26 22:06:17,674 Writing example 0 of 505555
2021-07-26 22:06:17,676 *** Example ***
2021-07-26 22:06:17,676 guid: aug-0
2021-07-26 22:06:17,676 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-26 22:06:17,676 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:06:17,676 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:06:17,676 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:06:17,676 label: neutral
2021-07-26 22:06:17,676 label_id: 2
2021-07-26 22:06:22,381 Writing example 10000 of 505555
2021-07-26 22:06:26,998 Writing example 20000 of 505555
2021-07-26 22:06:31,492 Writing example 30000 of 505555
2021-07-26 22:06:36,277 Writing example 40000 of 505555
2021-07-26 22:06:41,056 Writing example 50000 of 505555
2021-07-26 22:06:45,587 Writing example 60000 of 505555
2021-07-26 22:06:50,286 Writing example 70000 of 505555
2021-07-26 22:06:54,835 Writing example 80000 of 505555
2021-07-26 22:06:59,731 Writing example 90000 of 505555
2021-07-26 22:07:04,398 Writing example 100000 of 505555
2021-07-26 22:07:08,975 Writing example 110000 of 505555
2021-07-26 22:07:13,433 Writing example 120000 of 505555
2021-07-26 22:07:18,080 Writing example 130000 of 505555
2021-07-26 22:07:23,368 Writing example 140000 of 505555
2021-07-26 22:07:27,845 Writing example 150000 of 505555
2021-07-26 22:07:32,611 Writing example 160000 of 505555
2021-07-26 22:07:37,285 Writing example 170000 of 505555
2021-07-26 22:07:41,864 Writing example 180000 of 505555
2021-07-26 22:07:46,458 Writing example 190000 of 505555
2021-07-26 22:07:51,016 Writing example 200000 of 505555
2021-07-26 22:07:56,426 Writing example 210000 of 505555
2021-07-26 22:08:00,978 Writing example 220000 of 505555
2021-07-26 22:08:05,776 Writing example 230000 of 505555
2021-07-26 22:08:10,303 Writing example 240000 of 505555
2021-07-26 22:08:14,919 Writing example 250000 of 505555
2021-07-26 22:08:19,474 Writing example 260000 of 505555
2021-07-26 22:08:23,949 Writing example 270000 of 505555
2021-07-26 22:08:28,518 Writing example 280000 of 505555
2021-07-26 22:08:34,096 Writing example 290000 of 505555
2021-07-26 22:08:38,754 Writing example 300000 of 505555
2021-07-26 22:08:43,290 Writing example 310000 of 505555
2021-07-26 22:08:47,776 Writing example 320000 of 505555
2021-07-26 22:08:52,438 Writing example 330000 of 505555
2021-07-26 22:08:56,881 Writing example 340000 of 505555
2021-07-26 22:09:01,458 Writing example 350000 of 505555
2021-07-26 22:09:05,954 Writing example 360000 of 505555
2021-07-26 22:09:10,359 Writing example 370000 of 505555
2021-07-26 22:09:14,849 Writing example 380000 of 505555
2021-07-26 22:09:20,927 Writing example 390000 of 505555
2021-07-26 22:09:25,527 Writing example 400000 of 505555
2021-07-26 22:09:30,134 Writing example 410000 of 505555
2021-07-26 22:09:34,728 Writing example 420000 of 505555
2021-07-26 22:09:39,098 Writing example 430000 of 505555
2021-07-26 22:09:43,482 Writing example 440000 of 505555
2021-07-26 22:09:48,176 Writing example 450000 of 505555
2021-07-26 22:09:52,763 Writing example 460000 of 505555
2021-07-26 22:09:57,316 Writing example 470000 of 505555
2021-07-26 22:10:01,961 Writing example 480000 of 505555
2021-07-26 22:10:06,544 Writing example 490000 of 505555
2021-07-26 22:10:11,094 Writing example 500000 of 505555
2021-07-26 22:10:18,693 Writing example 0 of 9815
2021-07-26 22:10:18,693 *** Example ***
2021-07-26 22:10:18,693 guid: dev_matched-0
2021-07-26 22:10:18,693 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-26 22:10:18,693 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:10:18,693 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:10:18,694 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:10:18,694 label: neutral
2021-07-26 22:10:18,694 label_id: 2
2021-07-26 22:10:23,036 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-26 22:10:25,459 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-26 22:10:28,111 loading model...
2021-07-26 22:10:28,454 done!
2021-07-26 22:10:28,454 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-26 22:10:28,454 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-26 22:10:35,415 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-26 22:10:36,915 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-07-26 22:10:37,917 loading model...
2021-07-26 22:10:37,930 done!
2021-07-26 22:10:37,930 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-07-26 22:10:37,930 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-07-26 22:10:37,997 ***** Running training *****
2021-07-26 22:10:37,997   Num examples = 505555
2021-07-26 22:10:37,997   Batch size = 32
2021-07-26 22:10:37,997   Num steps = 47394
2021-07-26 22:10:37,998 n: module.bert.embeddings.word_embeddings.weight
2021-07-26 22:10:37,998 n: module.bert.embeddings.position_embeddings.weight
2021-07-26 22:10:37,998 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-26 22:10:37,998 n: module.bert.embeddings.LayerNorm.weight
2021-07-26 22:10:37,998 n: module.bert.embeddings.LayerNorm.bias
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-26 22:10:37,998 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-26 22:10:37,999 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-26 22:10:38,000 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-26 22:10:38,001 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-26 22:10:38,002 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-26 22:10:38,003 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-26 22:10:38,003 n: module.bert.pooler.dense.weight
2021-07-26 22:10:38,003 n: module.bert.pooler.dense.bias
2021-07-26 22:10:38,003 n: module.classifier.weight
2021-07-26 22:10:38,003 n: module.classifier.bias
2021-07-26 22:10:38,003 n: module.fit_dense.weight
2021-07-26 22:10:38,003 n: module.fit_dense.bias
2021-07-26 22:10:38,003 Total parameters: 67547907
2021-07-26 22:24:33,701 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-26 22:24:33,823 device: cuda n_gpu: 4
2021-07-26 22:24:41,683 Writing example 0 of 505555
2021-07-26 22:24:41,685 *** Example ***
2021-07-26 22:24:41,685 guid: aug-0
2021-07-26 22:24:41,685 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-26 22:24:41,685 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:24:41,685 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:24:41,685 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:24:41,685 label: neutral
2021-07-26 22:24:41,685 label_id: 2
2021-07-26 22:24:46,478 Writing example 10000 of 505555
2021-07-26 22:24:51,184 Writing example 20000 of 505555
2021-07-26 22:24:55,745 Writing example 30000 of 505555
2021-07-26 22:25:00,713 Writing example 40000 of 505555
2021-07-26 22:25:05,521 Writing example 50000 of 505555
2021-07-26 22:25:10,028 Writing example 60000 of 505555
2021-07-26 22:25:14,673 Writing example 70000 of 505555
2021-07-26 22:25:19,252 Writing example 80000 of 505555
2021-07-26 22:25:24,136 Writing example 90000 of 505555
2021-07-26 22:25:28,750 Writing example 100000 of 505555
2021-07-26 22:25:33,357 Writing example 110000 of 505555
2021-07-26 22:25:37,825 Writing example 120000 of 505555
2021-07-26 22:25:42,497 Writing example 130000 of 505555
2021-07-26 22:25:47,808 Writing example 140000 of 505555
2021-07-26 22:25:52,242 Writing example 150000 of 505555
2021-07-26 22:25:57,035 Writing example 160000 of 505555
2021-07-26 22:26:01,734 Writing example 170000 of 505555
2021-07-26 22:26:06,423 Writing example 180000 of 505555
2021-07-26 22:26:11,041 Writing example 190000 of 505555
2021-07-26 22:26:15,622 Writing example 200000 of 505555
2021-07-26 22:26:21,035 Writing example 210000 of 505555
2021-07-26 22:26:25,503 Writing example 220000 of 505555
2021-07-26 22:26:30,178 Writing example 230000 of 505555
2021-07-26 22:26:34,722 Writing example 240000 of 505555
2021-07-26 22:26:39,481 Writing example 250000 of 505555
2021-07-26 22:26:44,055 Writing example 260000 of 505555
2021-07-26 22:26:48,559 Writing example 270000 of 505555
2021-07-26 22:26:53,094 Writing example 280000 of 505555
2021-07-26 22:26:58,713 Writing example 290000 of 505555
2021-07-26 22:27:03,521 Writing example 300000 of 505555
2021-07-26 22:27:08,069 Writing example 310000 of 505555
2021-07-26 22:27:12,492 Writing example 320000 of 505555
2021-07-26 22:27:17,274 Writing example 330000 of 505555
2021-07-26 22:27:21,880 Writing example 340000 of 505555
2021-07-26 22:27:26,421 Writing example 350000 of 505555
2021-07-26 22:27:30,923 Writing example 360000 of 505555
2021-07-26 22:27:35,331 Writing example 370000 of 505555
2021-07-26 22:27:39,824 Writing example 380000 of 505555
2021-07-26 22:27:45,860 Writing example 390000 of 505555
2021-07-26 22:27:50,483 Writing example 400000 of 505555
2021-07-26 22:27:55,036 Writing example 410000 of 505555
2021-07-26 22:27:59,616 Writing example 420000 of 505555
2021-07-26 22:28:04,086 Writing example 430000 of 505555
2021-07-26 22:28:08,616 Writing example 440000 of 505555
2021-07-26 22:28:13,389 Writing example 450000 of 505555
2021-07-26 22:28:17,994 Writing example 460000 of 505555
2021-07-26 22:28:22,570 Writing example 470000 of 505555
2021-07-26 22:28:27,289 Writing example 480000 of 505555
2021-07-26 22:28:31,937 Writing example 490000 of 505555
2021-07-26 22:28:36,543 Writing example 500000 of 505555
2021-07-26 22:28:43,862 Writing example 0 of 9815
2021-07-26 22:28:43,863 *** Example ***
2021-07-26 22:28:43,863 guid: dev_matched-0
2021-07-26 22:28:43,863 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-26 22:28:43,863 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:28:43,863 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:28:43,863 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 22:28:43,863 label: neutral
2021-07-26 22:28:43,863 label_id: 2
2021-07-26 22:28:48,214 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-26 22:28:50,620 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-26 22:28:50,807 loading model...
2021-07-26 22:28:51,151 done!
2021-07-26 22:28:51,152 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-26 22:28:51,152 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-26 22:28:53,711 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-26 22:28:55,196 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-07-26 22:28:55,315 loading model...
2021-07-26 22:28:55,328 done!
2021-07-26 22:28:55,328 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-07-26 22:28:55,328 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-07-26 22:28:55,395 ***** Running training *****
2021-07-26 22:28:55,395   Num examples = 505555
2021-07-26 22:28:55,395   Batch size = 32
2021-07-26 22:28:55,395   Num steps = 47394
2021-07-26 22:28:55,396 n: module.bert.embeddings.word_embeddings.weight
2021-07-26 22:28:55,397 n: module.bert.embeddings.position_embeddings.weight
2021-07-26 22:28:55,397 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-26 22:28:55,397 n: module.bert.embeddings.LayerNorm.weight
2021-07-26 22:28:55,397 n: module.bert.embeddings.LayerNorm.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-26 22:28:55,397 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-26 22:28:55,398 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-26 22:28:55,399 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-26 22:28:55,400 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-26 22:28:55,401 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-26 22:28:55,401 n: module.bert.pooler.dense.weight
2021-07-26 22:28:55,402 n: module.bert.pooler.dense.bias
2021-07-26 22:28:55,402 n: module.classifier.weight
2021-07-26 22:28:55,402 n: module.classifier.bias
2021-07-26 22:28:55,402 n: module.fit_dense.weight
2021-07-26 22:28:55,402 n: module.fit_dense.bias
2021-07-26 22:28:55,402 Total parameters: 67547907
2021-07-26 23:31:37,983 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-26 23:31:38,112 device: cuda n_gpu: 4
2021-07-26 23:31:46,796 Writing example 0 of 505555
2021-07-26 23:31:46,797 *** Example ***
2021-07-26 23:31:46,797 guid: aug-0
2021-07-26 23:31:46,797 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-26 23:31:46,797 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:31:46,797 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:31:46,797 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:31:46,797 label: neutral
2021-07-26 23:31:46,797 label_id: 2
2021-07-26 23:31:51,481 Writing example 10000 of 505555
2021-07-26 23:31:56,082 Writing example 20000 of 505555
2021-07-26 23:32:00,635 Writing example 30000 of 505555
2021-07-26 23:32:05,355 Writing example 40000 of 505555
2021-07-26 23:32:10,152 Writing example 50000 of 505555
2021-07-26 23:32:14,638 Writing example 60000 of 505555
2021-07-26 23:32:19,258 Writing example 70000 of 505555
2021-07-26 23:32:23,807 Writing example 80000 of 505555
2021-07-26 23:32:28,733 Writing example 90000 of 505555
2021-07-26 23:32:33,297 Writing example 100000 of 505555
2021-07-26 23:32:37,880 Writing example 110000 of 505555
2021-07-26 23:32:42,223 Writing example 120000 of 505555
2021-07-26 23:32:46,877 Writing example 130000 of 505555
2021-07-26 23:32:52,166 Writing example 140000 of 505555
2021-07-26 23:32:56,615 Writing example 150000 of 505555
2021-07-26 23:33:01,382 Writing example 160000 of 505555
2021-07-26 23:33:06,034 Writing example 170000 of 505555
2021-07-26 23:33:10,609 Writing example 180000 of 505555
2021-07-26 23:33:15,199 Writing example 190000 of 505555
2021-07-26 23:33:19,759 Writing example 200000 of 505555
2021-07-26 23:33:25,172 Writing example 210000 of 505555
2021-07-26 23:33:29,751 Writing example 220000 of 505555
2021-07-26 23:33:34,364 Writing example 230000 of 505555
2021-07-26 23:33:38,885 Writing example 240000 of 505555
2021-07-26 23:33:43,532 Writing example 250000 of 505555
2021-07-26 23:33:48,106 Writing example 260000 of 505555
2021-07-26 23:33:52,582 Writing example 270000 of 505555
2021-07-26 23:33:57,121 Writing example 280000 of 505555
2021-07-26 23:34:02,673 Writing example 290000 of 505555
2021-07-26 23:34:07,334 Writing example 300000 of 505555
2021-07-26 23:34:11,842 Writing example 310000 of 505555
2021-07-26 23:34:16,234 Writing example 320000 of 505555
2021-07-26 23:34:20,893 Writing example 330000 of 505555
2021-07-26 23:34:25,307 Writing example 340000 of 505555
2021-07-26 23:34:29,843 Writing example 350000 of 505555
2021-07-26 23:34:34,474 Writing example 360000 of 505555
2021-07-26 23:34:38,916 Writing example 370000 of 505555
2021-07-26 23:34:43,381 Writing example 380000 of 505555
2021-07-26 23:34:49,392 Writing example 390000 of 505555
2021-07-26 23:34:53,978 Writing example 400000 of 505555
2021-07-26 23:34:58,515 Writing example 410000 of 505555
2021-07-26 23:35:03,031 Writing example 420000 of 505555
2021-07-26 23:35:07,446 Writing example 430000 of 505555
2021-07-26 23:35:11,830 Writing example 440000 of 505555
2021-07-26 23:35:16,433 Writing example 450000 of 505555
2021-07-26 23:35:21,016 Writing example 460000 of 505555
2021-07-26 23:35:25,562 Writing example 470000 of 505555
2021-07-26 23:35:30,246 Writing example 480000 of 505555
2021-07-26 23:35:34,819 Writing example 490000 of 505555
2021-07-26 23:35:39,347 Writing example 500000 of 505555
2021-07-26 23:35:46,735 Writing example 0 of 9815
2021-07-26 23:35:46,735 *** Example ***
2021-07-26 23:35:46,735 guid: dev_matched-0
2021-07-26 23:35:46,735 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-26 23:35:46,735 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:35:46,735 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:35:46,735 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:35:46,735 label: neutral
2021-07-26 23:35:46,736 label_id: 2
2021-07-26 23:35:51,060 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-26 23:35:53,518 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-26 23:35:55,562 loading model...
2021-07-26 23:35:55,907 done!
2021-07-26 23:35:55,907 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-26 23:35:55,907 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-26 23:36:03,886 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-26 23:36:05,372 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-07-26 23:36:06,734 loading model...
2021-07-26 23:36:06,747 done!
2021-07-26 23:36:06,747 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-07-26 23:36:06,747 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-07-26 23:36:06,814 ***** Running training *****
2021-07-26 23:36:06,814   Num examples = 505555
2021-07-26 23:36:06,814   Batch size = 32
2021-07-26 23:36:06,814   Num steps = 47394
2021-07-26 23:36:06,815 n: module.bert.embeddings.word_embeddings.weight
2021-07-26 23:36:06,815 n: module.bert.embeddings.position_embeddings.weight
2021-07-26 23:36:06,815 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-26 23:36:06,815 n: module.bert.embeddings.LayerNorm.weight
2021-07-26 23:36:06,815 n: module.bert.embeddings.LayerNorm.bias
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-26 23:36:06,815 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-26 23:36:06,816 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-26 23:36:06,817 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-26 23:36:06,818 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-26 23:36:06,819 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-26 23:36:06,820 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-26 23:36:06,820 n: module.bert.pooler.dense.weight
2021-07-26 23:36:06,820 n: module.bert.pooler.dense.bias
2021-07-26 23:36:06,820 n: module.classifier.weight
2021-07-26 23:36:06,820 n: module.classifier.bias
2021-07-26 23:36:06,820 n: module.fit_dense.weight
2021-07-26 23:36:06,820 n: module.fit_dense.bias
2021-07-26 23:36:06,820 Total parameters: 67547907
2021-07-26 23:36:30,380 ***** Running evaluation *****
2021-07-26 23:36:30,381   Epoch = 0 iter 99 step
2021-07-26 23:36:30,381   Num examples = 9815
2021-07-26 23:36:30,381   Batch size = 32
2021-07-26 23:36:42,881 ***** Eval results *****
2021-07-26 23:36:42,881   acc = 0.31818644931227713
2021-07-26 23:36:42,881   att_loss = 0.0
2021-07-26 23:36:42,881   cls_loss = 0.3652353428228937
2021-07-26 23:36:42,881   eval_loss = 1.1078337807608738
2021-07-26 23:36:42,881   global_step = 99
2021-07-26 23:36:42,881   loss = 0.3652353428228937
2021-07-26 23:36:42,881   rep_loss = 0.0
2021-07-26 23:36:42,882 ***** Save model *****
2021-07-26 23:36:48,177 Writing example 0 of 9832
2021-07-26 23:36:48,178 *** Example ***
2021-07-26 23:36:48,178 guid: dev_matched-0
2021-07-26 23:36:48,178 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:36:48,178 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:36:48,178 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:36:48,178 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:36:48,178 label: contradiction
2021-07-26 23:36:48,178 label_id: 0
2021-07-26 23:36:52,738 ***** Running mm evaluation *****
2021-07-26 23:36:52,738   Num examples = 9832
2021-07-26 23:36:52,738   Batch size = 32
2021-07-26 23:37:03,510 ***** Eval results *****
2021-07-26 23:37:03,510   acc = 0.318246541903987
2021-07-26 23:37:03,510   eval_loss = 1.1084900893174208
2021-07-26 23:37:03,510   global_step = 99
2021-07-26 23:37:22,037 ***** Running evaluation *****
2021-07-26 23:37:22,038   Epoch = 0 iter 199 step
2021-07-26 23:37:22,038   Num examples = 9832
2021-07-26 23:37:22,038   Batch size = 32
2021-07-26 23:37:32,806 ***** Eval results *****
2021-07-26 23:37:32,806   acc = 0.31804312449145644
2021-07-26 23:37:32,806   att_loss = 0.0
2021-07-26 23:37:32,806   cls_loss = 0.36479755412394077
2021-07-26 23:37:32,806   eval_loss = 1.11004492601791
2021-07-26 23:37:32,806   global_step = 199
2021-07-26 23:37:32,806   loss = 0.36479755412394077
2021-07-26 23:37:32,806   rep_loss = 0.0
2021-07-26 23:37:49,591 ***** Running evaluation *****
2021-07-26 23:37:49,591   Epoch = 0 iter 299 step
2021-07-26 23:37:49,591   Num examples = 9832
2021-07-26 23:37:49,591   Batch size = 32
2021-07-26 23:38:02,363 ***** Eval results *****
2021-07-26 23:38:02,363   acc = 0.3174328722538649
2021-07-26 23:38:02,363   att_loss = 0.0
2021-07-26 23:38:02,363   cls_loss = 0.3646396059655027
2021-07-26 23:38:02,363   eval_loss = 1.1106762634469318
2021-07-26 23:38:02,363   global_step = 299
2021-07-26 23:38:02,363   loss = 0.3646396059655027
2021-07-26 23:38:02,363   rep_loss = 0.0
2021-07-26 23:38:19,134 ***** Running evaluation *****
2021-07-26 23:38:19,135   Epoch = 0 iter 399 step
2021-07-26 23:38:19,135   Num examples = 9832
2021-07-26 23:38:19,135   Batch size = 32
2021-07-26 23:38:31,776 ***** Eval results *****
2021-07-26 23:38:31,777   acc = 0.3169243287225387
2021-07-26 23:38:31,777   att_loss = 0.0
2021-07-26 23:38:31,777   cls_loss = 0.36455480997126205
2021-07-26 23:38:31,777   eval_loss = 1.109208929461318
2021-07-26 23:38:31,777   global_step = 399
2021-07-26 23:38:31,777   loss = 0.36455480997126205
2021-07-26 23:38:31,777   rep_loss = 0.0
2021-07-26 23:38:48,612 ***** Running evaluation *****
2021-07-26 23:38:48,612   Epoch = 0 iter 499 step
2021-07-26 23:38:48,612   Num examples = 9832
2021-07-26 23:38:48,613   Batch size = 32
2021-07-26 23:39:01,103 ***** Eval results *****
2021-07-26 23:39:01,103   acc = 0.31804312449145644
2021-07-26 23:39:01,103   att_loss = 0.0
2021-07-26 23:39:01,103   cls_loss = 0.3644975623888578
2021-07-26 23:39:01,103   eval_loss = 1.110458555546674
2021-07-26 23:39:01,103   global_step = 499
2021-07-26 23:39:01,103   loss = 0.3644975623888578
2021-07-26 23:39:01,103   rep_loss = 0.0
2021-07-26 23:39:17,886 ***** Running evaluation *****
2021-07-26 23:39:17,887   Epoch = 0 iter 599 step
2021-07-26 23:39:17,887   Num examples = 9832
2021-07-26 23:39:17,887   Batch size = 32
2021-07-26 23:39:28,646 ***** Eval results *****
2021-07-26 23:39:28,646   acc = 0.31804312449145644
2021-07-26 23:39:28,646   att_loss = 0.0
2021-07-26 23:39:28,646   cls_loss = 0.36445177327810424
2021-07-26 23:39:28,646   eval_loss = 1.1110105057815454
2021-07-26 23:39:28,646   global_step = 599
2021-07-26 23:39:28,646   loss = 0.36445177327810424
2021-07-26 23:39:28,646   rep_loss = 0.0
2021-07-26 23:39:47,288 ***** Running evaluation *****
2021-07-26 23:39:47,289   Epoch = 0 iter 699 step
2021-07-26 23:39:47,289   Num examples = 9832
2021-07-26 23:39:47,289   Batch size = 32
2021-07-26 23:39:58,068 ***** Eval results *****
2021-07-26 23:39:58,068   acc = 0.3175345809601302
2021-07-26 23:39:58,068   att_loss = 0.0
2021-07-26 23:39:58,068   cls_loss = 0.3644139117352099
2021-07-26 23:39:58,068   eval_loss = 1.1103976214086855
2021-07-26 23:39:58,068   global_step = 699
2021-07-26 23:39:58,068   loss = 0.3644139117352099
2021-07-26 23:39:58,068   rep_loss = 0.0
2021-07-26 23:40:16,520 ***** Running evaluation *****
2021-07-26 23:40:16,520   Epoch = 0 iter 799 step
2021-07-26 23:40:16,520   Num examples = 9832
2021-07-26 23:40:16,520   Batch size = 32
2021-07-26 23:40:27,275 ***** Eval results *****
2021-07-26 23:40:27,275   acc = 0.318246541903987
2021-07-26 23:40:27,275   att_loss = 0.0
2021-07-26 23:40:27,275   cls_loss = 0.364384938465042
2021-07-26 23:40:27,275   eval_loss = 1.1099772948723334
2021-07-26 23:40:27,275   global_step = 799
2021-07-26 23:40:27,275   loss = 0.364384938465042
2021-07-26 23:40:27,275   rep_loss = 0.0
2021-07-26 23:40:27,275 ***** Save model *****
2021-07-26 23:40:28,038 Writing example 0 of 9832
2021-07-26 23:40:28,039 *** Example ***
2021-07-26 23:40:28,039 guid: dev_matched-0
2021-07-26 23:40:28,039 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:40:28,039 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:40:28,039 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:40:28,039 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:40:28,039 label: contradiction
2021-07-26 23:40:28,039 label_id: 0
2021-07-26 23:40:32,589 ***** Running mm evaluation *****
2021-07-26 23:40:32,589   Num examples = 9832
2021-07-26 23:40:32,589   Batch size = 32
2021-07-26 23:40:43,339 ***** Eval results *****
2021-07-26 23:40:43,339   acc = 0.318246541903987
2021-07-26 23:40:43,339   eval_loss = 1.1099772948723334
2021-07-26 23:40:43,339   global_step = 799
2021-07-26 23:41:01,857 ***** Running evaluation *****
2021-07-26 23:41:01,858   Epoch = 0 iter 899 step
2021-07-26 23:41:01,858   Num examples = 9832
2021-07-26 23:41:01,858   Batch size = 32
2021-07-26 23:41:12,592 ***** Eval results *****
2021-07-26 23:41:12,592   acc = 0.3175345809601302
2021-07-26 23:41:12,592   att_loss = 0.0
2021-07-26 23:41:12,592   cls_loss = 0.36436649845359853
2021-07-26 23:41:12,592   eval_loss = 1.111609671410028
2021-07-26 23:41:12,592   global_step = 899
2021-07-26 23:41:12,592   loss = 0.36436649845359853
2021-07-26 23:41:12,592   rep_loss = 0.0
2021-07-26 23:41:29,346 ***** Running evaluation *****
2021-07-26 23:41:29,347   Epoch = 0 iter 999 step
2021-07-26 23:41:29,347   Num examples = 9832
2021-07-26 23:41:29,347   Batch size = 32
2021-07-26 23:41:42,096 ***** Eval results *****
2021-07-26 23:41:42,096   acc = 0.31783970707892595
2021-07-26 23:41:42,096   att_loss = 0.0
2021-07-26 23:41:42,096   cls_loss = 0.3643493945295508
2021-07-26 23:41:42,096   eval_loss = 1.1123163142761627
2021-07-26 23:41:42,096   global_step = 999
2021-07-26 23:41:42,096   loss = 0.3643493945295508
2021-07-26 23:41:42,096   rep_loss = 0.0
2021-07-26 23:41:58,860 ***** Running evaluation *****
2021-07-26 23:41:58,861   Epoch = 0 iter 1099 step
2021-07-26 23:41:58,861   Num examples = 9832
2021-07-26 23:41:58,861   Batch size = 32
2021-07-26 23:42:09,613 ***** Eval results *****
2021-07-26 23:42:09,613   acc = 0.3184499593165175
2021-07-26 23:42:09,613   att_loss = 0.0
2021-07-26 23:42:09,613   cls_loss = 0.3643365210842501
2021-07-26 23:42:09,613   eval_loss = 1.1119196682007282
2021-07-26 23:42:09,613   global_step = 1099
2021-07-26 23:42:09,613   loss = 0.3643365210842501
2021-07-26 23:42:09,613   rep_loss = 0.0
2021-07-26 23:42:09,614 ***** Save model *****
2021-07-26 23:42:10,401 Writing example 0 of 9832
2021-07-26 23:42:10,401 *** Example ***
2021-07-26 23:42:10,401 guid: dev_matched-0
2021-07-26 23:42:10,401 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:42:10,401 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:10,401 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:10,402 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:10,402 label: contradiction
2021-07-26 23:42:10,402 label_id: 0
2021-07-26 23:42:14,966 ***** Running mm evaluation *****
2021-07-26 23:42:14,966   Num examples = 9832
2021-07-26 23:42:14,966   Batch size = 32
2021-07-26 23:42:25,704 ***** Eval results *****
2021-07-26 23:42:25,704   acc = 0.3184499593165175
2021-07-26 23:42:25,704   eval_loss = 1.1119196682007282
2021-07-26 23:42:25,704   global_step = 1099
2021-07-26 23:42:44,143 ***** Running evaluation *****
2021-07-26 23:42:44,144   Epoch = 0 iter 1199 step
2021-07-26 23:42:44,144   Num examples = 9832
2021-07-26 23:42:44,144   Batch size = 32
2021-07-26 23:42:54,900 ***** Eval results *****
2021-07-26 23:42:54,900   acc = 0.3189585028478438
2021-07-26 23:42:54,900   att_loss = 0.0
2021-07-26 23:42:54,900   cls_loss = 0.3643220240767545
2021-07-26 23:42:54,900   eval_loss = 1.110270540822636
2021-07-26 23:42:54,900   global_step = 1199
2021-07-26 23:42:54,900   loss = 0.3643220240767545
2021-07-26 23:42:54,900   rep_loss = 0.0
2021-07-26 23:42:54,901 ***** Save model *****
2021-07-26 23:42:55,594 Writing example 0 of 9832
2021-07-26 23:42:55,595 *** Example ***
2021-07-26 23:42:55,595 guid: dev_matched-0
2021-07-26 23:42:55,595 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:42:55,595 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:55,595 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:55,595 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:42:55,596 label: contradiction
2021-07-26 23:42:55,596 label_id: 0
2021-07-26 23:43:00,161 ***** Running mm evaluation *****
2021-07-26 23:43:00,161   Num examples = 9832
2021-07-26 23:43:00,161   Batch size = 32
2021-07-26 23:43:10,957 ***** Eval results *****
2021-07-26 23:43:10,958   acc = 0.3189585028478438
2021-07-26 23:43:10,958   eval_loss = 1.110270540822636
2021-07-26 23:43:10,958   global_step = 1199
2021-07-26 23:43:29,795 ***** Running evaluation *****
2021-07-26 23:43:29,795   Epoch = 0 iter 1299 step
2021-07-26 23:43:29,795   Num examples = 9832
2021-07-26 23:43:29,795   Batch size = 32
2021-07-26 23:43:40,585 ***** Eval results *****
2021-07-26 23:43:40,586   acc = 0.31865337672904803
2021-07-26 23:43:40,586   att_loss = 0.0
2021-07-26 23:43:40,586   cls_loss = 0.36431368804602005
2021-07-26 23:43:40,586   eval_loss = 1.1113828130356678
2021-07-26 23:43:40,586   global_step = 1299
2021-07-26 23:43:40,586   loss = 0.36431368804602005
2021-07-26 23:43:40,586   rep_loss = 0.0
2021-07-26 23:43:57,435 ***** Running evaluation *****
2021-07-26 23:43:57,435   Epoch = 0 iter 1399 step
2021-07-26 23:43:57,435   Num examples = 9832
2021-07-26 23:43:57,436   Batch size = 32
2021-07-26 23:44:08,224 ***** Eval results *****
2021-07-26 23:44:08,224   acc = 0.3169243287225387
2021-07-26 23:44:08,224   att_loss = 0.0
2021-07-26 23:44:08,224   cls_loss = 0.3643023017221387
2021-07-26 23:44:08,224   eval_loss = 1.1126814147868713
2021-07-26 23:44:08,224   global_step = 1399
2021-07-26 23:44:08,224   loss = 0.3643023017221387
2021-07-26 23:44:08,224   rep_loss = 0.0
2021-07-26 23:44:26,733 ***** Running evaluation *****
2021-07-26 23:44:26,734   Epoch = 0 iter 1499 step
2021-07-26 23:44:26,734   Num examples = 9832
2021-07-26 23:44:26,734   Batch size = 32
2021-07-26 23:44:37,537 ***** Eval results *****
2021-07-26 23:44:37,538   acc = 0.31865337672904803
2021-07-26 23:44:37,538   att_loss = 0.0
2021-07-26 23:44:37,538   cls_loss = 0.36429494050759487
2021-07-26 23:44:37,538   eval_loss = 1.1105832283373003
2021-07-26 23:44:37,538   global_step = 1499
2021-07-26 23:44:37,538   loss = 0.36429494050759487
2021-07-26 23:44:37,538   rep_loss = 0.0
2021-07-26 23:44:54,376 ***** Running evaluation *****
2021-07-26 23:44:54,376   Epoch = 0 iter 1599 step
2021-07-26 23:44:54,376   Num examples = 9832
2021-07-26 23:44:54,376   Batch size = 32
2021-07-26 23:45:05,146 ***** Eval results *****
2021-07-26 23:45:05,146   acc = 0.31783970707892595
2021-07-26 23:45:05,146   att_loss = 0.0
2021-07-26 23:45:05,146   cls_loss = 0.36428908602083526
2021-07-26 23:45:05,146   eval_loss = 1.1111014160242947
2021-07-26 23:45:05,146   global_step = 1599
2021-07-26 23:45:05,146   loss = 0.36428908602083526
2021-07-26 23:45:05,146   rep_loss = 0.0
2021-07-26 23:45:22,001 ***** Running evaluation *****
2021-07-26 23:45:22,002   Epoch = 0 iter 1699 step
2021-07-26 23:45:22,002   Num examples = 9832
2021-07-26 23:45:22,002   Batch size = 32
2021-07-26 23:45:34,758 ***** Eval results *****
2021-07-26 23:45:34,759   acc = 0.318246541903987
2021-07-26 23:45:34,759   att_loss = 0.0
2021-07-26 23:45:34,759   cls_loss = 0.3642834719487539
2021-07-26 23:45:34,759   eval_loss = 1.1107105082505708
2021-07-26 23:45:34,759   global_step = 1699
2021-07-26 23:45:34,759   loss = 0.3642834719487539
2021-07-26 23:45:34,759   rep_loss = 0.0
2021-07-26 23:45:51,657 ***** Running evaluation *****
2021-07-26 23:45:51,657   Epoch = 0 iter 1799 step
2021-07-26 23:45:51,658   Num examples = 9832
2021-07-26 23:45:51,658   Batch size = 32
2021-07-26 23:46:02,447 ***** Eval results *****
2021-07-26 23:46:02,448   acc = 0.3193653376729048
2021-07-26 23:46:02,448   att_loss = 0.0
2021-07-26 23:46:02,448   cls_loss = 0.3642766142037261
2021-07-26 23:46:02,448   eval_loss = 1.110194018134823
2021-07-26 23:46:02,448   global_step = 1799
2021-07-26 23:46:02,448   loss = 0.3642766142037261
2021-07-26 23:46:02,448   rep_loss = 0.0
2021-07-26 23:46:02,448 ***** Save model *****
2021-07-26 23:46:03,269 Writing example 0 of 9832
2021-07-26 23:46:03,270 *** Example ***
2021-07-26 23:46:03,270 guid: dev_matched-0
2021-07-26 23:46:03,270 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:46:03,270 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:46:03,270 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:46:03,270 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:46:03,270 label: contradiction
2021-07-26 23:46:03,270 label_id: 0
2021-07-26 23:46:07,828 ***** Running mm evaluation *****
2021-07-26 23:46:07,828   Num examples = 9832
2021-07-26 23:46:07,828   Batch size = 32
2021-07-26 23:46:20,252 ***** Eval results *****
2021-07-26 23:46:20,252   acc = 0.3193653376729048
2021-07-26 23:46:20,252   eval_loss = 1.110194018134823
2021-07-26 23:46:20,252   global_step = 1799
2021-07-26 23:46:37,064 ***** Running evaluation *****
2021-07-26 23:46:37,065   Epoch = 0 iter 1899 step
2021-07-26 23:46:37,065   Num examples = 9832
2021-07-26 23:46:37,065   Batch size = 32
2021-07-26 23:46:49,721 ***** Eval results *****
2021-07-26 23:46:49,721   acc = 0.3188567941415785
2021-07-26 23:46:49,721   att_loss = 0.0
2021-07-26 23:46:49,722   cls_loss = 0.3642709161370224
2021-07-26 23:46:49,722   eval_loss = 1.1109570469949153
2021-07-26 23:46:49,722   global_step = 1899
2021-07-26 23:46:49,722   loss = 0.3642709161370224
2021-07-26 23:46:49,722   rep_loss = 0.0
2021-07-26 23:47:06,539 ***** Running evaluation *****
2021-07-26 23:47:06,539   Epoch = 0 iter 1999 step
2021-07-26 23:47:06,539   Num examples = 9832
2021-07-26 23:47:06,539   Batch size = 32
2021-07-26 23:47:18,821 ***** Eval results *****
2021-07-26 23:47:18,821   acc = 0.31855166802278273
2021-07-26 23:47:18,821   att_loss = 0.0
2021-07-26 23:47:18,821   cls_loss = 0.364264944587844
2021-07-26 23:47:18,821   eval_loss = 1.1105993912591563
2021-07-26 23:47:18,821   global_step = 1999
2021-07-26 23:47:18,821   loss = 0.364264944587844
2021-07-26 23:47:18,821   rep_loss = 0.0
2021-07-26 23:47:35,645 ***** Running evaluation *****
2021-07-26 23:47:35,645   Epoch = 0 iter 2099 step
2021-07-26 23:47:35,645   Num examples = 9832
2021-07-26 23:47:35,645   Batch size = 32
2021-07-26 23:47:46,398 ***** Eval results *****
2021-07-26 23:47:46,398   acc = 0.31855166802278273
2021-07-26 23:47:46,398   att_loss = 0.0
2021-07-26 23:47:46,398   cls_loss = 0.36426072651116603
2021-07-26 23:47:46,398   eval_loss = 1.1083492791497862
2021-07-26 23:47:46,398   global_step = 2099
2021-07-26 23:47:46,398   loss = 0.36426072651116603
2021-07-26 23:47:46,398   rep_loss = 0.0
2021-07-26 23:48:04,875 ***** Running evaluation *****
2021-07-26 23:48:04,875   Epoch = 0 iter 2199 step
2021-07-26 23:48:04,875   Num examples = 9832
2021-07-26 23:48:04,875   Batch size = 32
2021-07-26 23:48:15,637 ***** Eval results *****
2021-07-26 23:48:15,637   acc = 0.3188567941415785
2021-07-26 23:48:15,637   att_loss = 0.0
2021-07-26 23:48:15,637   cls_loss = 0.3642568663880737
2021-07-26 23:48:15,637   eval_loss = 1.1124882520019235
2021-07-26 23:48:15,637   global_step = 2199
2021-07-26 23:48:15,637   loss = 0.3642568663880737
2021-07-26 23:48:15,637   rep_loss = 0.0
2021-07-26 23:48:34,328 ***** Running evaluation *****
2021-07-26 23:48:34,328   Epoch = 0 iter 2299 step
2021-07-26 23:48:34,328   Num examples = 9832
2021-07-26 23:48:34,328   Batch size = 32
2021-07-26 23:48:45,092 ***** Eval results *****
2021-07-26 23:48:45,092   acc = 0.31865337672904803
2021-07-26 23:48:45,092   att_loss = 0.0
2021-07-26 23:48:45,092   cls_loss = 0.36425254327051015
2021-07-26 23:48:45,093   eval_loss = 1.1081608828012046
2021-07-26 23:48:45,093   global_step = 2299
2021-07-26 23:48:45,093   loss = 0.36425254327051015
2021-07-26 23:48:45,093   rep_loss = 0.0
2021-07-26 23:49:03,415 ***** Running evaluation *****
2021-07-26 23:49:03,416   Epoch = 0 iter 2399 step
2021-07-26 23:49:03,416   Num examples = 9832
2021-07-26 23:49:03,416   Batch size = 32
2021-07-26 23:49:14,184 ***** Eval results *****
2021-07-26 23:49:14,185   acc = 0.3189585028478438
2021-07-26 23:49:14,185   att_loss = 0.0
2021-07-26 23:49:14,185   cls_loss = 0.36424917477078617
2021-07-26 23:49:14,185   eval_loss = 1.1120300668400604
2021-07-26 23:49:14,185   global_step = 2399
2021-07-26 23:49:14,185   loss = 0.36424917477078617
2021-07-26 23:49:14,185   rep_loss = 0.0
2021-07-26 23:49:31,041 ***** Running evaluation *****
2021-07-26 23:49:31,042   Epoch = 0 iter 2499 step
2021-07-26 23:49:31,042   Num examples = 9832
2021-07-26 23:49:31,042   Batch size = 32
2021-07-26 23:49:43,412 ***** Eval results *****
2021-07-26 23:49:43,412   acc = 0.3198738812042311
2021-07-26 23:49:43,412   att_loss = 0.0
2021-07-26 23:49:43,413   cls_loss = 0.3642452448761525
2021-07-26 23:49:43,413   eval_loss = 1.1099283253991759
2021-07-26 23:49:43,413   global_step = 2499
2021-07-26 23:49:43,413   loss = 0.3642452448761525
2021-07-26 23:49:43,413   rep_loss = 0.0
2021-07-26 23:49:43,413 ***** Save model *****
2021-07-26 23:49:44,178 Writing example 0 of 9832
2021-07-26 23:49:44,179 *** Example ***
2021-07-26 23:49:44,179 guid: dev_matched-0
2021-07-26 23:49:44,179 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:49:44,179 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:49:44,179 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:49:44,179 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:49:44,179 label: contradiction
2021-07-26 23:49:44,179 label_id: 0
2021-07-26 23:49:48,754 ***** Running mm evaluation *****
2021-07-26 23:49:48,754   Num examples = 9832
2021-07-26 23:49:48,754   Batch size = 32
2021-07-26 23:49:59,540 ***** Eval results *****
2021-07-26 23:49:59,540   acc = 0.3198738812042311
2021-07-26 23:49:59,540   eval_loss = 1.1099283253991759
2021-07-26 23:49:59,540   global_step = 2499
2021-07-26 23:50:16,345 ***** Running evaluation *****
2021-07-26 23:50:16,345   Epoch = 0 iter 2599 step
2021-07-26 23:50:16,346   Num examples = 9832
2021-07-26 23:50:16,346   Batch size = 32
2021-07-26 23:50:27,113 ***** Eval results *****
2021-07-26 23:50:27,113   acc = 0.3187550854353133
2021-07-26 23:50:27,113   att_loss = 0.0
2021-07-26 23:50:27,113   cls_loss = 0.364241147509168
2021-07-26 23:50:27,113   eval_loss = 1.1093539158245185
2021-07-26 23:50:27,113   global_step = 2599
2021-07-26 23:50:27,113   loss = 0.364241147509168
2021-07-26 23:50:27,113   rep_loss = 0.0
2021-07-26 23:50:43,890 ***** Running evaluation *****
2021-07-26 23:50:43,891   Epoch = 0 iter 2699 step
2021-07-26 23:50:43,891   Num examples = 9832
2021-07-26 23:50:43,891   Batch size = 32
2021-07-26 23:50:56,547 ***** Eval results *****
2021-07-26 23:50:56,547   acc = 0.31814483319772174
2021-07-26 23:50:56,548   att_loss = 0.0
2021-07-26 23:50:56,548   cls_loss = 0.3642364182420111
2021-07-26 23:50:56,548   eval_loss = 1.108939724696147
2021-07-26 23:50:56,548   global_step = 2699
2021-07-26 23:50:56,548   loss = 0.3642364182420111
2021-07-26 23:50:56,548   rep_loss = 0.0
2021-07-26 23:51:13,399 ***** Running evaluation *****
2021-07-26 23:51:13,399   Epoch = 0 iter 2799 step
2021-07-26 23:51:13,399   Num examples = 9832
2021-07-26 23:51:13,399   Batch size = 32
2021-07-26 23:51:24,168 ***** Eval results *****
2021-07-26 23:51:24,168   acc = 0.3202807160292921
2021-07-26 23:51:24,168   att_loss = 0.0
2021-07-26 23:51:24,168   cls_loss = 0.3642334646414586
2021-07-26 23:51:24,168   eval_loss = 1.109800220309914
2021-07-26 23:51:24,168   global_step = 2799
2021-07-26 23:51:24,168   loss = 0.3642334646414586
2021-07-26 23:51:24,168   rep_loss = 0.0
2021-07-26 23:51:24,169 ***** Save model *****
2021-07-26 23:51:24,957 Writing example 0 of 9832
2021-07-26 23:51:24,958 *** Example ***
2021-07-26 23:51:24,958 guid: dev_matched-0
2021-07-26 23:51:24,958 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-26 23:51:24,958 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:51:24,958 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:51:24,958 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-26 23:51:24,958 label: contradiction
2021-07-26 23:51:24,958 label_id: 0
2021-07-26 23:51:29,595 ***** Running mm evaluation *****
2021-07-26 23:51:29,595   Num examples = 9832
2021-07-26 23:51:29,595   Batch size = 32
2021-07-26 23:51:40,431 ***** Eval results *****
2021-07-26 23:51:40,431   acc = 0.3202807160292921
2021-07-26 23:51:40,431   eval_loss = 1.109800220309914
2021-07-26 23:51:40,431   global_step = 2799
2021-07-26 23:51:59,229 ***** Running evaluation *****
2021-07-26 23:51:59,229   Epoch = 0 iter 2899 step
2021-07-26 23:51:59,229   Num examples = 9832
2021-07-26 23:51:59,229   Batch size = 32
2021-07-26 23:52:10,016 ***** Eval results *****
2021-07-26 23:52:10,016   acc = 0.31783970707892595
2021-07-26 23:52:10,016   att_loss = 0.0
2021-07-26 23:52:10,016   cls_loss = 0.3642314905008393
2021-07-26 23:52:10,016   eval_loss = 1.1096162497997284
2021-07-26 23:52:10,016   global_step = 2899
2021-07-26 23:52:10,016   loss = 0.3642314905008393
2021-07-26 23:52:10,016   rep_loss = 0.0
2021-07-26 23:52:26,831 ***** Running evaluation *****
2021-07-26 23:52:26,832   Epoch = 0 iter 2999 step
2021-07-26 23:52:26,832   Num examples = 9832
2021-07-26 23:52:26,832   Batch size = 32
2021-07-26 23:52:39,093 ***** Eval results *****
2021-07-26 23:52:39,093   acc = 0.31865337672904803
2021-07-26 23:52:39,093   att_loss = 0.0
2021-07-26 23:52:39,093   cls_loss = 0.36422834325011627
2021-07-26 23:52:39,093   eval_loss = 1.1092418319993205
2021-07-26 23:52:39,093   global_step = 2999
2021-07-26 23:52:39,093   loss = 0.36422834325011627
2021-07-26 23:52:39,093   rep_loss = 0.0
2021-07-27 00:09:44,548 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 00:09:44,764 device: cpu n_gpu: 0
2021-07-27 00:11:46,076 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 00:11:46,272 device: cpu n_gpu: 0
2021-07-27 00:11:55,929 Writing example 0 of 505555
2021-07-27 00:11:55,931 *** Example ***
2021-07-27 00:11:55,931 guid: aug-0
2021-07-27 00:11:55,931 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-27 00:11:55,931 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:11:55,931 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:11:55,931 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:11:55,931 label: neutral
2021-07-27 00:11:55,931 label_id: 2
2021-07-27 00:12:01,151 Writing example 10000 of 505555
2021-07-27 00:12:06,269 Writing example 20000 of 505555
2021-07-27 00:12:11,013 Writing example 30000 of 505555
2021-07-27 00:12:15,948 Writing example 40000 of 505555
2021-07-27 00:12:21,082 Writing example 50000 of 505555
2021-07-27 00:12:25,767 Writing example 60000 of 505555
2021-07-27 00:12:30,591 Writing example 70000 of 505555
2021-07-27 00:12:35,663 Writing example 80000 of 505555
2021-07-27 00:12:41,510 Writing example 90000 of 505555
2021-07-27 00:12:46,609 Writing example 100000 of 505555
2021-07-27 00:12:51,562 Writing example 110000 of 505555
2021-07-27 00:12:56,321 Writing example 120000 of 505555
2021-07-27 00:13:01,423 Writing example 130000 of 505555
2021-07-27 00:13:07,263 Writing example 140000 of 505555
2021-07-27 00:13:11,937 Writing example 150000 of 505555
2021-07-27 00:13:16,984 Writing example 160000 of 505555
2021-07-27 00:13:21,859 Writing example 170000 of 505555
2021-07-27 00:13:26,606 Writing example 180000 of 505555
2021-07-27 00:13:31,392 Writing example 190000 of 505555
2021-07-27 00:13:36,158 Writing example 200000 of 505555
2021-07-27 00:13:42,282 Writing example 210000 of 505555
2021-07-27 00:13:46,996 Writing example 220000 of 505555
2021-07-27 00:13:51,867 Writing example 230000 of 505555
2021-07-27 00:13:56,636 Writing example 240000 of 505555
2021-07-27 00:14:01,442 Writing example 250000 of 505555
2021-07-27 00:14:06,200 Writing example 260000 of 505555
2021-07-27 00:14:10,875 Writing example 270000 of 505555
2021-07-27 00:14:15,484 Writing example 280000 of 505555
2021-07-27 00:14:21,456 Writing example 290000 of 505555
2021-07-27 00:14:26,299 Writing example 300000 of 505555
2021-07-27 00:14:31,041 Writing example 310000 of 505555
2021-07-27 00:14:35,651 Writing example 320000 of 505555
2021-07-27 00:14:40,512 Writing example 330000 of 505555
2021-07-27 00:14:45,192 Writing example 340000 of 505555
2021-07-27 00:14:49,947 Writing example 350000 of 505555
2021-07-27 00:14:54,710 Writing example 360000 of 505555
2021-07-27 00:14:59,347 Writing example 370000 of 505555
2021-07-27 00:15:04,064 Writing example 380000 of 505555
2021-07-27 00:15:10,548 Writing example 390000 of 505555
2021-07-27 00:15:15,870 Writing example 400000 of 505555
2021-07-27 00:15:20,601 Writing example 410000 of 505555
2021-07-27 00:15:25,224 Writing example 420000 of 505555
2021-07-27 00:15:29,763 Writing example 430000 of 505555
2021-07-27 00:15:34,375 Writing example 440000 of 505555
2021-07-27 00:15:39,154 Writing example 450000 of 505555
2021-07-27 00:15:44,062 Writing example 460000 of 505555
2021-07-27 00:15:48,935 Writing example 470000 of 505555
2021-07-27 00:15:53,819 Writing example 480000 of 505555
2021-07-27 00:15:58,516 Writing example 490000 of 505555
2021-07-27 00:16:03,207 Writing example 500000 of 505555
2021-07-27 00:16:11,217 Writing example 0 of 9815
2021-07-27 00:16:11,218 *** Example ***
2021-07-27 00:16:11,218 guid: dev_matched-0
2021-07-27 00:16:11,218 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-27 00:16:11,218 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:16:11,218 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:16:11,218 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:16:11,218 label: neutral
2021-07-27 00:16:11,218 label_id: 2
2021-07-27 00:16:15,723 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-27 00:16:18,373 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-27 00:16:20,288 loading model...
2021-07-27 00:20:21,795 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 00:20:21,919 device: cuda n_gpu: 4
2021-07-27 00:20:29,585 Writing example 0 of 505555
2021-07-27 00:20:29,587 *** Example ***
2021-07-27 00:20:29,587 guid: aug-0
2021-07-27 00:20:29,587 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-27 00:20:29,587 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:20:29,587 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:20:29,587 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:20:29,587 label: neutral
2021-07-27 00:20:29,587 label_id: 2
2021-07-27 00:20:34,283 Writing example 10000 of 505555
2021-07-27 00:20:38,869 Writing example 20000 of 505555
2021-07-27 00:20:43,367 Writing example 30000 of 505555
2021-07-27 00:20:48,048 Writing example 40000 of 505555
2021-07-27 00:20:52,825 Writing example 50000 of 505555
2021-07-27 00:20:57,448 Writing example 60000 of 505555
2021-07-27 00:21:02,054 Writing example 70000 of 505555
2021-07-27 00:21:06,688 Writing example 80000 of 505555
2021-07-27 00:21:11,536 Writing example 90000 of 505555
2021-07-27 00:21:16,092 Writing example 100000 of 505555
2021-07-27 00:21:20,658 Writing example 110000 of 505555
2021-07-27 00:21:25,015 Writing example 120000 of 505555
2021-07-27 00:21:29,776 Writing example 130000 of 505555
2021-07-27 00:21:35,056 Writing example 140000 of 505555
2021-07-27 00:21:39,455 Writing example 150000 of 505555
2021-07-27 00:21:44,205 Writing example 160000 of 505555
2021-07-27 00:21:48,966 Writing example 170000 of 505555
2021-07-27 00:21:53,527 Writing example 180000 of 505555
2021-07-27 00:21:58,130 Writing example 190000 of 505555
2021-07-27 00:22:02,776 Writing example 200000 of 505555
2021-07-27 00:22:08,189 Writing example 210000 of 505555
2021-07-27 00:22:12,682 Writing example 220000 of 505555
2021-07-27 00:22:17,289 Writing example 230000 of 505555
2021-07-27 00:22:21,800 Writing example 240000 of 505555
2021-07-27 00:22:26,426 Writing example 250000 of 505555
2021-07-27 00:22:30,976 Writing example 260000 of 505555
2021-07-27 00:22:35,445 Writing example 270000 of 505555
2021-07-27 00:22:40,059 Writing example 280000 of 505555
2021-07-27 00:22:45,657 Writing example 290000 of 505555
2021-07-27 00:22:50,317 Writing example 300000 of 505555
2021-07-27 00:22:54,820 Writing example 310000 of 505555
2021-07-27 00:22:59,288 Writing example 320000 of 505555
2021-07-27 00:23:03,998 Writing example 330000 of 505555
2021-07-27 00:23:08,429 Writing example 340000 of 505555
2021-07-27 00:23:12,921 Writing example 350000 of 505555
2021-07-27 00:23:17,388 Writing example 360000 of 505555
2021-07-27 00:23:21,763 Writing example 370000 of 505555
2021-07-27 00:23:26,237 Writing example 380000 of 505555
2021-07-27 00:23:32,395 Writing example 390000 of 505555
2021-07-27 00:23:36,985 Writing example 400000 of 505555
2021-07-27 00:23:41,519 Writing example 410000 of 505555
2021-07-27 00:23:45,946 Writing example 420000 of 505555
2021-07-27 00:23:50,311 Writing example 430000 of 505555
2021-07-27 00:23:54,753 Writing example 440000 of 505555
2021-07-27 00:23:59,447 Writing example 450000 of 505555
2021-07-27 00:24:04,090 Writing example 460000 of 505555
2021-07-27 00:24:08,648 Writing example 470000 of 505555
2021-07-27 00:24:13,263 Writing example 480000 of 505555
2021-07-27 00:24:17,966 Writing example 490000 of 505555
2021-07-27 00:24:22,667 Writing example 500000 of 505555
2021-07-27 00:24:29,822 Writing example 0 of 9815
2021-07-27 00:24:29,823 *** Example ***
2021-07-27 00:24:29,823 guid: dev_matched-0
2021-07-27 00:24:29,823 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-27 00:24:29,823 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:24:29,823 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:24:29,823 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:24:29,823 label: neutral
2021-07-27 00:24:29,823 label_id: 2
2021-07-27 00:24:34,102 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-27 00:24:36,497 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-27 00:24:36,684 loading model...
2021-07-27 00:24:37,020 done!
2021-07-27 00:24:37,020 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-27 00:24:37,020 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-27 00:24:39,564 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-27 00:24:41,058 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-07-27 00:24:41,413 loading model...
2021-07-27 00:24:41,427 done!
2021-07-27 00:24:41,427 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-07-27 00:24:41,427 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-07-27 00:24:41,494 ***** Running training *****
2021-07-27 00:24:41,494   Num examples = 505555
2021-07-27 00:24:41,494   Batch size = 32
2021-07-27 00:24:41,494   Num steps = 47394
2021-07-27 00:24:41,495 n: module.bert.embeddings.word_embeddings.weight
2021-07-27 00:24:41,495 n: module.bert.embeddings.position_embeddings.weight
2021-07-27 00:24:41,495 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-27 00:24:41,496 n: module.bert.embeddings.LayerNorm.weight
2021-07-27 00:24:41,496 n: module.bert.embeddings.LayerNorm.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-27 00:24:41,496 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-27 00:24:41,497 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-27 00:24:41,498 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-27 00:24:41,499 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-27 00:24:41,500 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-27 00:24:41,501 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-27 00:24:41,501 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-27 00:24:41,501 n: module.bert.pooler.dense.weight
2021-07-27 00:24:41,501 n: module.bert.pooler.dense.bias
2021-07-27 00:24:41,501 n: module.classifier.weight
2021-07-27 00:24:41,501 n: module.classifier.bias
2021-07-27 00:24:41,501 n: module.fit_dense.weight
2021-07-27 00:24:41,501 n: module.fit_dense.bias
2021-07-27 00:24:41,501 Total parameters: 67547907
2021-07-27 00:25:05,022 ***** Running evaluation *****
2021-07-27 00:25:05,022   Epoch = 0 iter 99 step
2021-07-27 00:25:05,023   Num examples = 9815
2021-07-27 00:25:05,023   Batch size = 32
2021-07-27 00:25:17,642 ***** Eval results *****
2021-07-27 00:25:17,642   acc = 0.8453387671930719
2021-07-27 00:25:17,642   att_loss = 0.0
2021-07-27 00:25:17,643   cls_loss = 0.8217328925325413
2021-07-27 00:25:17,643   eval_loss = 0.40078058440832826
2021-07-27 00:25:17,643   global_step = 99
2021-07-27 00:25:17,643   loss = 0.8217328925325413
2021-07-27 00:25:17,643   rep_loss = 0.0
2021-07-27 00:25:17,643 ***** Save model *****
2021-07-27 00:25:18,344 Writing example 0 of 9832
2021-07-27 00:25:18,345 *** Example ***
2021-07-27 00:25:18,345 guid: dev_matched-0
2021-07-27 00:25:18,345 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-27 00:25:18,345 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:25:18,345 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:25:18,345 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 00:25:18,345 label: contradiction
2021-07-27 00:25:18,345 label_id: 0
2021-07-27 00:25:22,902 ***** Running mm evaluation *****
2021-07-27 00:25:22,902   Num examples = 9832
2021-07-27 00:25:22,902   Batch size = 32
2021-07-27 00:25:33,746 ***** Eval results *****
2021-07-27 00:25:33,746   acc = 0.8441822620016274
2021-07-27 00:25:33,746   eval_loss = 0.4057799448537362
2021-07-27 00:25:33,746   global_step = 99
2021-07-27 00:25:52,516 ***** Running evaluation *****
2021-07-27 00:25:52,516   Epoch = 0 iter 199 step
2021-07-27 00:25:52,516   Num examples = 9832
2021-07-27 00:25:52,516   Batch size = 32
2021-07-27 00:26:03,347 ***** Eval results *****
2021-07-27 00:26:03,347   acc = 0.8062449145646867
2021-07-27 00:26:03,347   att_loss = 0.0
2021-07-27 00:26:03,347   cls_loss = 0.6604068051630528
2021-07-27 00:26:03,347   eval_loss = 0.6245384616898252
2021-07-27 00:26:03,347   global_step = 199
2021-07-27 00:26:03,347   loss = 0.6604068051630528
2021-07-27 00:26:03,347   rep_loss = 0.0
2021-07-27 00:26:20,250 ***** Running evaluation *****
2021-07-27 00:26:20,251   Epoch = 0 iter 299 step
2021-07-27 00:26:20,251   Num examples = 9832
2021-07-27 00:26:20,251   Batch size = 32
2021-07-27 00:26:33,116 ***** Eval results *****
2021-07-27 00:26:33,117   acc = 0.7331163547599675
2021-07-27 00:26:33,117   att_loss = 0.0
2021-07-27 00:26:33,117   cls_loss = 0.5681848202062689
2021-07-27 00:26:33,117   eval_loss = 0.8670781428937788
2021-07-27 00:26:33,117   global_step = 299
2021-07-27 00:26:33,117   loss = 0.5681848202062689
2021-07-27 00:26:33,117   rep_loss = 0.0
2021-07-27 00:26:50,038 ***** Running evaluation *****
2021-07-27 00:26:50,038   Epoch = 0 iter 399 step
2021-07-27 00:26:50,038   Num examples = 9832
2021-07-27 00:26:50,038   Batch size = 32
2021-07-27 00:27:02,811 ***** Eval results *****
2021-07-27 00:27:02,811   acc = 0.652766476810415
2021-07-27 00:27:02,811   att_loss = 0.0
2021-07-27 00:27:02,811   cls_loss = 0.5183559582944501
2021-07-27 00:27:02,811   eval_loss = 0.9645102074393979
2021-07-27 00:27:02,811   global_step = 399
2021-07-27 00:27:02,811   loss = 0.5183559582944501
2021-07-27 00:27:02,812   rep_loss = 0.0
2021-07-27 00:27:19,699 ***** Running evaluation *****
2021-07-27 00:27:19,699   Epoch = 0 iter 499 step
2021-07-27 00:27:19,700   Num examples = 9832
2021-07-27 00:27:19,700   Batch size = 32
2021-07-27 00:27:32,204 ***** Eval results *****
2021-07-27 00:27:32,204   acc = 0.5635679414157851
2021-07-27 00:27:32,204   att_loss = 0.0
2021-07-27 00:27:32,204   cls_loss = 0.48792229333000336
2021-07-27 00:27:32,204   eval_loss = 1.0100889407195055
2021-07-27 00:27:32,204   global_step = 499
2021-07-27 00:27:32,204   loss = 0.48792229333000336
2021-07-27 00:27:32,204   rep_loss = 0.0
2021-07-27 00:27:49,073 ***** Running evaluation *****
2021-07-27 00:27:49,073   Epoch = 0 iter 599 step
2021-07-27 00:27:49,073   Num examples = 9832
2021-07-27 00:27:49,073   Batch size = 32
2021-07-27 00:27:59,920 ***** Eval results *****
2021-07-27 00:27:59,920   acc = 0.48291293734743695
2021-07-27 00:27:59,920   att_loss = 0.0
2021-07-27 00:27:59,920   cls_loss = 0.4674916177838792
2021-07-27 00:27:59,920   eval_loss = 1.0363211569847999
2021-07-27 00:27:59,920   global_step = 599
2021-07-27 00:27:59,920   loss = 0.4674916177838792
2021-07-27 00:27:59,920   rep_loss = 0.0
2021-07-27 00:28:18,826 ***** Running evaluation *****
2021-07-27 00:28:18,826   Epoch = 0 iter 699 step
2021-07-27 00:28:18,827   Num examples = 9832
2021-07-27 00:28:18,827   Batch size = 32
2021-07-27 00:28:29,692 ***** Eval results *****
2021-07-27 00:28:29,692   acc = 0.41985353946297804
2021-07-27 00:28:29,692   att_loss = 0.0
2021-07-27 00:28:29,692   cls_loss = 0.4528411517242164
2021-07-27 00:28:29,693   eval_loss = 1.0530202164278402
2021-07-27 00:28:29,693   global_step = 699
2021-07-27 00:28:29,693   loss = 0.4528411517242164
2021-07-27 00:28:29,693   rep_loss = 0.0
2021-07-27 00:28:48,179 ***** Running evaluation *****
2021-07-27 00:28:48,180   Epoch = 0 iter 799 step
2021-07-27 00:28:48,180   Num examples = 9832
2021-07-27 00:28:48,180   Batch size = 32
2021-07-27 00:28:58,999 ***** Eval results *****
2021-07-27 00:28:58,999   acc = 0.37876322213181446
2021-07-27 00:28:58,999   att_loss = 0.0
2021-07-27 00:28:58,999   cls_loss = 0.44183054913268965
2021-07-27 00:28:58,999   eval_loss = 1.0638625079935247
2021-07-27 00:28:58,999   global_step = 799
2021-07-27 00:28:59,000   loss = 0.44183054913268965
2021-07-27 00:28:59,000   rep_loss = 0.0
2021-07-27 00:29:17,478 ***** Running evaluation *****
2021-07-27 00:29:17,478   Epoch = 0 iter 899 step
2021-07-27 00:29:17,478   Num examples = 9832
2021-07-27 00:29:17,478   Batch size = 32
2021-07-27 00:29:28,279 ***** Eval results *****
2021-07-27 00:29:28,279   acc = 0.35648901545972334
2021-07-27 00:29:28,279   att_loss = 0.0
2021-07-27 00:29:28,280   cls_loss = 0.43326128533605207
2021-07-27 00:29:28,280   eval_loss = 1.0714727183441064
2021-07-27 00:29:28,280   global_step = 899
2021-07-27 00:29:28,280   loss = 0.43326128533605207
2021-07-27 00:29:28,280   rep_loss = 0.0
2021-07-27 00:29:45,071 ***** Running evaluation *****
2021-07-27 00:29:45,072   Epoch = 0 iter 999 step
2021-07-27 00:29:45,072   Num examples = 9832
2021-07-27 00:29:45,072   Batch size = 32
2021-07-27 00:29:57,672 ***** Eval results *****
2021-07-27 00:29:57,672   acc = 0.3399104963384866
2021-07-27 00:29:57,672   att_loss = 0.0
2021-07-27 00:29:57,672   cls_loss = 0.4263932945969346
2021-07-27 00:29:57,672   eval_loss = 1.0770316998679916
2021-07-27 00:29:57,672   global_step = 999
2021-07-27 00:29:57,672   loss = 0.4263932945969346
2021-07-27 00:29:57,672   rep_loss = 0.0
2021-07-27 00:30:14,489 ***** Running evaluation *****
2021-07-27 00:30:14,489   Epoch = 0 iter 1099 step
2021-07-27 00:30:14,489   Num examples = 9832
2021-07-27 00:30:14,489   Batch size = 32
2021-07-27 00:30:26,816 ***** Eval results *****
2021-07-27 00:30:26,816   acc = 0.330553295362083
2021-07-27 00:30:26,817   att_loss = 0.0
2021-07-27 00:30:26,817   cls_loss = 0.4207721835542962
2021-07-27 00:30:26,817   eval_loss = 1.0816115717609207
2021-07-27 00:30:26,817   global_step = 1099
2021-07-27 00:30:26,817   loss = 0.4207721835542962
2021-07-27 00:30:26,817   rep_loss = 0.0
2021-07-27 00:30:43,602 ***** Running evaluation *****
2021-07-27 00:30:43,603   Epoch = 0 iter 1199 step
2021-07-27 00:30:43,603   Num examples = 9832
2021-07-27 00:30:43,603   Batch size = 32
2021-07-27 00:30:54,377 ***** Eval results *****
2021-07-27 00:30:54,377   acc = 0.326586655817738
2021-07-27 00:30:54,378   att_loss = 0.0
2021-07-27 00:30:54,378   cls_loss = 0.4160868218036569
2021-07-27 00:30:54,378   eval_loss = 1.084579450743539
2021-07-27 00:30:54,378   global_step = 1199
2021-07-27 00:30:54,378   loss = 0.4160868218036569
2021-07-27 00:30:54,378   rep_loss = 0.0
2021-07-27 00:31:12,914 ***** Running evaluation *****
2021-07-27 00:31:12,914   Epoch = 0 iter 1299 step
2021-07-27 00:31:12,914   Num examples = 9832
2021-07-27 00:31:12,914   Batch size = 32
2021-07-27 00:31:23,699 ***** Eval results *****
2021-07-27 00:31:23,699   acc = 0.32465419039869814
2021-07-27 00:31:23,699   att_loss = 0.0
2021-07-27 00:31:23,699   cls_loss = 0.41211927095406603
2021-07-27 00:31:23,699   eval_loss = 1.0871974187237876
2021-07-27 00:31:23,699   global_step = 1299
2021-07-27 00:31:23,699   loss = 0.41211927095406603
2021-07-27 00:31:23,699   rep_loss = 0.0
2021-07-27 00:31:42,236 ***** Running evaluation *****
2021-07-27 00:31:42,236   Epoch = 0 iter 1399 step
2021-07-27 00:31:42,236   Num examples = 9832
2021-07-27 00:31:42,236   Batch size = 32
2021-07-27 00:31:53,056 ***** Eval results *****
2021-07-27 00:31:53,056   acc = 0.32343368592351507
2021-07-27 00:31:53,057   att_loss = 0.0
2021-07-27 00:31:53,057   cls_loss = 0.4087188513130695
2021-07-27 00:31:53,057   eval_loss = 1.0888869205078522
2021-07-27 00:31:53,057   global_step = 1399
2021-07-27 00:31:53,057   loss = 0.4087188513130695
2021-07-27 00:31:53,057   rep_loss = 0.0
2021-07-27 00:32:11,488 ***** Running evaluation *****
2021-07-27 00:32:11,489   Epoch = 0 iter 1499 step
2021-07-27 00:32:11,489   Num examples = 9832
2021-07-27 00:32:11,489   Batch size = 32
2021-07-27 00:32:22,240 ***** Eval results *****
2021-07-27 00:32:22,240   acc = 0.3213995117982099
2021-07-27 00:32:22,240   att_loss = 0.0
2021-07-27 00:32:22,240   cls_loss = 0.40576956964398003
2021-07-27 00:32:22,240   eval_loss = 1.0910626936268497
2021-07-27 00:32:22,240   global_step = 1499
2021-07-27 00:32:22,240   loss = 0.40576956964398003
2021-07-27 00:32:22,240   rep_loss = 0.0
2021-07-27 00:32:38,972 ***** Running evaluation *****
2021-07-27 00:32:38,972   Epoch = 0 iter 1599 step
2021-07-27 00:32:38,972   Num examples = 9832
2021-07-27 00:32:38,972   Batch size = 32
2021-07-27 00:32:51,404 ***** Eval results *****
2021-07-27 00:32:51,404   acc = 0.32068755085435313
2021-07-27 00:32:51,404   att_loss = 0.0
2021-07-27 00:32:51,404   cls_loss = 0.4031868812961829
2021-07-27 00:32:51,404   eval_loss = 1.0926553595375705
2021-07-27 00:32:51,405   global_step = 1599
2021-07-27 00:32:51,405   loss = 0.4031868812961829
2021-07-27 00:32:51,405   rep_loss = 0.0
2021-07-27 00:33:08,170 ***** Running evaluation *****
2021-07-27 00:33:08,170   Epoch = 0 iter 1699 step
2021-07-27 00:33:08,170   Num examples = 9832
2021-07-27 00:33:08,170   Batch size = 32
2021-07-27 00:33:20,589 ***** Eval results *****
2021-07-27 00:33:20,589   acc = 0.3200772986167616
2021-07-27 00:33:20,589   att_loss = 0.0
2021-07-27 00:33:20,589   cls_loss = 0.4009081786383314
2021-07-27 00:33:20,589   eval_loss = 1.0939290589326387
2021-07-27 00:33:20,589   global_step = 1699
2021-07-27 00:33:20,589   loss = 0.4009081786383314
2021-07-27 00:33:20,589   rep_loss = 0.0
2021-07-27 00:33:37,344 ***** Running evaluation *****
2021-07-27 00:33:37,344   Epoch = 0 iter 1799 step
2021-07-27 00:33:37,344   Num examples = 9832
2021-07-27 00:33:37,344   Batch size = 32
2021-07-27 00:33:49,636 ***** Eval results *****
2021-07-27 00:33:49,636   acc = 0.3198738812042311
2021-07-27 00:33:49,636   att_loss = 0.0
2021-07-27 00:33:49,636   cls_loss = 0.3988808143662373
2021-07-27 00:33:49,636   eval_loss = 1.0953662890892524
2021-07-27 00:33:49,636   global_step = 1799
2021-07-27 00:33:49,636   loss = 0.3988808143662373
2021-07-27 00:33:49,636   rep_loss = 0.0
2021-07-27 00:34:06,462 ***** Running evaluation *****
2021-07-27 00:34:06,463   Epoch = 0 iter 1899 step
2021-07-27 00:34:06,463   Num examples = 9832
2021-07-27 00:34:06,463   Batch size = 32
2021-07-27 00:34:17,247 ***** Eval results *****
2021-07-27 00:34:17,247   acc = 0.31997558991049635
2021-07-27 00:34:17,247   att_loss = 0.0
2021-07-27 00:34:17,247   cls_loss = 0.3970688992327549
2021-07-27 00:34:17,247   eval_loss = 1.0960330030360779
2021-07-27 00:34:17,247   global_step = 1899
2021-07-27 00:34:17,247   loss = 0.3970688992327549
2021-07-27 00:34:17,247   rep_loss = 0.0
2021-07-27 00:34:35,758 ***** Running evaluation *****
2021-07-27 00:34:35,759   Epoch = 0 iter 1999 step
2021-07-27 00:34:35,759   Num examples = 9832
2021-07-27 00:34:35,759   Batch size = 32
2021-07-27 00:34:46,571 ***** Eval results *****
2021-07-27 00:34:46,571   acc = 0.3202807160292921
2021-07-27 00:34:46,571   att_loss = 0.0
2021-07-27 00:34:46,571   cls_loss = 0.39543532223448624
2021-07-27 00:34:46,571   eval_loss = 1.0965179204940796
2021-07-27 00:34:46,571   global_step = 1999
2021-07-27 00:34:46,571   loss = 0.39543532223448624
2021-07-27 00:34:46,571   rep_loss = 0.0
2021-07-27 00:35:05,182 ***** Running evaluation *****
2021-07-27 00:35:05,183   Epoch = 0 iter 2099 step
2021-07-27 00:35:05,183   Num examples = 9832
2021-07-27 00:35:05,183   Batch size = 32
2021-07-27 00:35:15,964 ***** Eval results *****
2021-07-27 00:35:15,965   acc = 0.31967046379170055
2021-07-27 00:35:15,965   att_loss = 0.0
2021-07-27 00:35:15,965   cls_loss = 0.39395920436060844
2021-07-27 00:35:15,965   eval_loss = 1.0979163306100028
2021-07-27 00:35:15,965   global_step = 2099
2021-07-27 00:35:15,965   loss = 0.39395920436060844
2021-07-27 00:35:15,965   rep_loss = 0.0
2021-07-27 00:35:32,776 ***** Running evaluation *****
2021-07-27 00:35:32,776   Epoch = 0 iter 2199 step
2021-07-27 00:35:32,776   Num examples = 9832
2021-07-27 00:35:32,776   Batch size = 32
2021-07-27 00:35:45,068 ***** Eval results *****
2021-07-27 00:35:45,068   acc = 0.3197721724979658
2021-07-27 00:35:45,068   att_loss = 0.0
2021-07-27 00:35:45,068   cls_loss = 0.39261608551501576
2021-07-27 00:35:45,069   eval_loss = 1.0982941656143634
2021-07-27 00:35:45,069   global_step = 2199
2021-07-27 00:35:45,069   loss = 0.39261608551501576
2021-07-27 00:35:45,069   rep_loss = 0.0
2021-07-27 00:36:01,899 ***** Running evaluation *****
2021-07-27 00:36:01,900   Epoch = 0 iter 2299 step
2021-07-27 00:36:01,900   Num examples = 9832
2021-07-27 00:36:01,900   Batch size = 32
2021-07-27 00:36:14,337 ***** Eval results *****
2021-07-27 00:36:14,337   acc = 0.31946704637917006
2021-07-27 00:36:14,338   att_loss = 0.0
2021-07-27 00:36:14,338   cls_loss = 0.39138845896503105
2021-07-27 00:36:14,338   eval_loss = 1.0999527813552261
2021-07-27 00:36:14,338   global_step = 2299
2021-07-27 00:36:14,338   loss = 0.39138845896503105
2021-07-27 00:36:14,338   rep_loss = 0.0
2021-07-27 00:36:31,174 ***** Running evaluation *****
2021-07-27 00:36:31,174   Epoch = 0 iter 2399 step
2021-07-27 00:36:31,174   Num examples = 9832
2021-07-27 00:36:31,174   Batch size = 32
2021-07-27 00:36:43,878 ***** Eval results *****
2021-07-27 00:36:43,878   acc = 0.31997558991049635
2021-07-27 00:36:43,878   att_loss = 0.0
2021-07-27 00:36:43,878   cls_loss = 0.3902623127802753
2021-07-27 00:36:43,878   eval_loss = 1.1001638297910814
2021-07-27 00:36:43,879   global_step = 2399
2021-07-27 00:36:43,879   loss = 0.3902623127802753
2021-07-27 00:36:43,879   rep_loss = 0.0
2021-07-27 00:37:00,691 ***** Running evaluation *****
2021-07-27 00:37:00,691   Epoch = 0 iter 2499 step
2021-07-27 00:37:00,691   Num examples = 9832
2021-07-27 00:37:00,691   Batch size = 32
2021-07-27 00:37:11,476 ***** Eval results *****
2021-07-27 00:37:11,477   acc = 0.31997558991049635
2021-07-27 00:37:11,477   att_loss = 0.0
2021-07-27 00:37:11,477   cls_loss = 0.38922826072224237
2021-07-27 00:37:11,477   eval_loss = 1.1018169464228988
2021-07-27 00:37:11,477   global_step = 2499
2021-07-27 00:37:11,477   loss = 0.38922826072224237
2021-07-27 00:37:11,477   rep_loss = 0.0
2021-07-27 00:37:29,900 ***** Running evaluation *****
2021-07-27 00:37:29,900   Epoch = 0 iter 2599 step
2021-07-27 00:37:29,901   Num examples = 9832
2021-07-27 00:37:29,901   Batch size = 32
2021-07-27 00:37:40,689 ***** Eval results *****
2021-07-27 00:37:40,689   acc = 0.32017900732302684
2021-07-27 00:37:40,689   att_loss = 0.0
2021-07-27 00:37:40,689   cls_loss = 0.38827007716552436
2021-07-27 00:37:40,689   eval_loss = 1.102170554074374
2021-07-27 00:37:40,689   global_step = 2599
2021-07-27 00:37:40,689   loss = 0.38827007716552436
2021-07-27 00:37:40,689   rep_loss = 0.0
2021-07-27 00:37:59,205 ***** Running evaluation *****
2021-07-27 00:37:59,205   Epoch = 0 iter 2699 step
2021-07-27 00:37:59,205   Num examples = 9832
2021-07-27 00:37:59,205   Batch size = 32
2021-07-27 00:38:09,984 ***** Eval results *****
2021-07-27 00:38:09,984   acc = 0.3195687550854353
2021-07-27 00:38:09,984   att_loss = 0.0
2021-07-27 00:38:09,984   cls_loss = 0.387382325506864
2021-07-27 00:38:09,984   eval_loss = 1.1043817346746272
2021-07-27 00:38:09,984   global_step = 2699
2021-07-27 00:38:09,984   loss = 0.387382325506864
2021-07-27 00:38:09,984   rep_loss = 0.0
2021-07-27 00:38:28,613 ***** Running evaluation *****
2021-07-27 00:38:28,614   Epoch = 0 iter 2799 step
2021-07-27 00:38:28,614   Num examples = 9832
2021-07-27 00:38:28,614   Batch size = 32
2021-07-27 00:38:39,382 ***** Eval results *****
2021-07-27 00:38:39,382   acc = 0.31967046379170055
2021-07-27 00:38:39,382   att_loss = 0.0
2021-07-27 00:38:39,382   cls_loss = 0.3865586130501501
2021-07-27 00:38:39,382   eval_loss = 1.1048935969154556
2021-07-27 00:38:39,382   global_step = 2799
2021-07-27 00:38:39,383   loss = 0.3865586130501501
2021-07-27 00:38:39,383   rep_loss = 0.0
2021-07-27 00:38:57,731 ***** Running evaluation *****
2021-07-27 00:38:57,731   Epoch = 0 iter 2899 step
2021-07-27 00:38:57,731   Num examples = 9832
2021-07-27 00:38:57,731   Batch size = 32
2021-07-27 00:39:08,533 ***** Eval results *****
2021-07-27 00:39:08,533   acc = 0.31997558991049635
2021-07-27 00:39:08,534   att_loss = 0.0
2021-07-27 00:39:08,534   cls_loss = 0.38579224577933685
2021-07-27 00:39:08,534   eval_loss = 1.1048032351128467
2021-07-27 00:39:08,534   global_step = 2899
2021-07-27 00:39:08,534   loss = 0.38579224577933685
2021-07-27 00:39:08,534   rep_loss = 0.0
2021-07-27 00:39:25,373 ***** Running evaluation *****
2021-07-27 00:39:25,374   Epoch = 0 iter 2999 step
2021-07-27 00:39:25,374   Num examples = 9832
2021-07-27 00:39:25,374   Batch size = 32
2021-07-27 00:39:37,820 ***** Eval results *****
2021-07-27 00:39:37,820   acc = 0.3197721724979658
2021-07-27 00:39:37,820   att_loss = 0.0
2021-07-27 00:39:37,820   cls_loss = 0.3850763888707277
2021-07-27 00:39:37,820   eval_loss = 1.1055241002664937
2021-07-27 00:39:37,820   global_step = 2999
2021-07-27 00:39:37,820   loss = 0.3850763888707277
2021-07-27 00:39:37,820   rep_loss = 0.0
2021-07-27 00:39:54,650 ***** Running evaluation *****
2021-07-27 00:39:54,651   Epoch = 0 iter 3099 step
2021-07-27 00:39:54,651   Num examples = 9832
2021-07-27 00:39:54,651   Batch size = 32
2021-07-27 00:40:05,439 ***** Eval results *****
2021-07-27 00:40:05,439   acc = 0.3205858421480879
2021-07-27 00:40:05,439   att_loss = 0.0
2021-07-27 00:40:05,439   cls_loss = 0.3844061352795807
2021-07-27 00:40:05,439   eval_loss = 1.105502630596037
2021-07-27 00:40:05,440   global_step = 3099
2021-07-27 00:40:05,440   loss = 0.3844061352795807
2021-07-27 00:40:05,440   rep_loss = 0.0
2021-07-27 00:40:23,990 ***** Running evaluation *****
2021-07-27 00:40:23,991   Epoch = 0 iter 3199 step
2021-07-27 00:40:23,991   Num examples = 9832
2021-07-27 00:40:23,991   Batch size = 32
2021-07-27 00:40:34,780 ***** Eval results *****
2021-07-27 00:40:34,780   acc = 0.3202807160292921
2021-07-27 00:40:34,780   att_loss = 0.0
2021-07-27 00:40:34,780   cls_loss = 0.38377767684833075
2021-07-27 00:40:34,780   eval_loss = 1.1058738889632287
2021-07-27 00:40:34,780   global_step = 3199
2021-07-27 00:40:34,780   loss = 0.38377767684833075
2021-07-27 00:40:34,780   rep_loss = 0.0
2021-07-27 09:34:48,709 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 09:34:49,015 device: cuda n_gpu: 4
2021-07-27 09:54:06,359 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 09:54:06,489 device: cuda n_gpu: 4
2021-07-27 09:54:15,298 Writing example 0 of 505555
2021-07-27 09:54:15,300 *** Example ***
2021-07-27 09:54:15,300 guid: aug-0
2021-07-27 09:54:15,300 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-27 09:54:15,300 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:54:15,300 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:54:15,300 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:54:15,300 label: neutral
2021-07-27 09:54:15,300 label_id: 2
2021-07-27 09:54:20,021 Writing example 10000 of 505555
2021-07-27 09:54:24,600 Writing example 20000 of 505555
2021-07-27 09:54:29,073 Writing example 30000 of 505555
2021-07-27 09:54:33,734 Writing example 40000 of 505555
2021-07-27 09:54:38,762 Writing example 50000 of 505555
2021-07-27 09:54:43,223 Writing example 60000 of 505555
2021-07-27 09:54:47,811 Writing example 70000 of 505555
2021-07-27 09:54:52,331 Writing example 80000 of 505555
2021-07-27 09:54:57,166 Writing example 90000 of 505555
2021-07-27 09:55:01,701 Writing example 100000 of 505555
2021-07-27 09:55:06,254 Writing example 110000 of 505555
2021-07-27 09:55:10,573 Writing example 120000 of 505555
2021-07-27 09:55:15,191 Writing example 130000 of 505555
2021-07-27 09:55:20,457 Writing example 140000 of 505555
2021-07-27 09:55:24,835 Writing example 150000 of 505555
2021-07-27 09:55:29,560 Writing example 160000 of 505555
2021-07-27 09:55:34,188 Writing example 170000 of 505555
2021-07-27 09:55:38,783 Writing example 180000 of 505555
2021-07-27 09:55:43,347 Writing example 190000 of 505555
2021-07-27 09:55:47,883 Writing example 200000 of 505555
2021-07-27 09:55:53,292 Writing example 210000 of 505555
2021-07-27 09:55:57,826 Writing example 220000 of 505555
2021-07-27 09:56:02,418 Writing example 230000 of 505555
2021-07-27 09:56:06,930 Writing example 240000 of 505555
2021-07-27 09:56:11,521 Writing example 250000 of 505555
2021-07-27 09:56:16,053 Writing example 260000 of 505555
2021-07-27 09:56:20,522 Writing example 270000 of 505555
2021-07-27 09:56:24,933 Writing example 280000 of 505555
2021-07-27 09:56:30,446 Writing example 290000 of 505555
2021-07-27 09:56:35,088 Writing example 300000 of 505555
2021-07-27 09:56:39,568 Writing example 310000 of 505555
2021-07-27 09:56:43,963 Writing example 320000 of 505555
2021-07-27 09:56:48,598 Writing example 330000 of 505555
2021-07-27 09:56:52,984 Writing example 340000 of 505555
2021-07-27 09:56:57,451 Writing example 350000 of 505555
2021-07-27 09:57:01,939 Writing example 360000 of 505555
2021-07-27 09:57:06,301 Writing example 370000 of 505555
2021-07-27 09:57:10,745 Writing example 380000 of 505555
2021-07-27 09:57:16,714 Writing example 390000 of 505555
2021-07-27 09:57:21,277 Writing example 400000 of 505555
2021-07-27 09:57:25,788 Writing example 410000 of 505555
2021-07-27 09:57:30,192 Writing example 420000 of 505555
2021-07-27 09:57:34,598 Writing example 430000 of 505555
2021-07-27 09:57:39,027 Writing example 440000 of 505555
2021-07-27 09:57:43,657 Writing example 450000 of 505555
2021-07-27 09:57:48,208 Writing example 460000 of 505555
2021-07-27 09:57:52,727 Writing example 470000 of 505555
2021-07-27 09:57:57,319 Writing example 480000 of 505555
2021-07-27 09:58:01,860 Writing example 490000 of 505555
2021-07-27 09:58:06,492 Writing example 500000 of 505555
2021-07-27 09:58:14,553 Writing example 0 of 9815
2021-07-27 09:58:14,554 *** Example ***
2021-07-27 09:58:14,554 guid: dev_matched-0
2021-07-27 09:58:14,554 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-27 09:58:14,554 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:58:14,554 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:58:14,554 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:58:14,554 label: neutral
2021-07-27 09:58:14,554 label_id: 2
2021-07-27 09:58:19,116 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-27 09:58:21,767 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-27 09:58:24,536 loading model...
2021-07-27 09:58:24,895 done!
2021-07-27 09:58:24,895 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-27 09:58:24,895 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-27 09:58:33,555 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-27 09:58:35,050 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-07-27 09:58:36,985 loading model...
2021-07-27 09:58:36,998 done!
2021-07-27 09:58:36,998 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-07-27 09:58:36,998 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-07-27 09:58:37,065 ***** Running training *****
2021-07-27 09:58:37,066   Num examples = 505555
2021-07-27 09:58:37,066   Batch size = 32
2021-07-27 09:58:37,066   Num steps = 47394
2021-07-27 09:58:37,067 n: module.bert.embeddings.word_embeddings.weight
2021-07-27 09:58:37,067 n: module.bert.embeddings.position_embeddings.weight
2021-07-27 09:58:37,067 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-27 09:58:37,067 n: module.bert.embeddings.LayerNorm.weight
2021-07-27 09:58:37,067 n: module.bert.embeddings.LayerNorm.bias
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-27 09:58:37,067 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-27 09:58:37,068 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-27 09:58:37,069 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-27 09:58:37,070 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-27 09:58:37,071 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-27 09:58:37,072 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-27 09:58:37,072 n: module.bert.pooler.dense.weight
2021-07-27 09:58:37,072 n: module.bert.pooler.dense.bias
2021-07-27 09:58:37,072 n: module.classifier.weight
2021-07-27 09:58:37,072 n: module.classifier.bias
2021-07-27 09:58:37,072 n: module.fit_dense.weight
2021-07-27 09:58:37,072 n: module.fit_dense.bias
2021-07-27 09:58:37,072 Total parameters: 67547907
2021-07-27 09:59:00,844 ***** Running evaluation *****
2021-07-27 09:59:00,844   Epoch = 0 iter 99 step
2021-07-27 09:59:00,844   Num examples = 9815
2021-07-27 09:59:00,844   Batch size = 32
2021-07-27 09:59:13,705 ***** Eval results *****
2021-07-27 09:59:13,705   acc = 0.8453387671930719
2021-07-27 09:59:13,705   att_loss = 0.0
2021-07-27 09:59:13,705   cls_loss = 0.8217328925325413
2021-07-27 09:59:13,705   eval_loss = 0.40078058440832826
2021-07-27 09:59:13,705   global_step = 99
2021-07-27 09:59:13,705   loss = 0.8217328925325413
2021-07-27 09:59:13,705   rep_loss = 0.0
2021-07-27 09:59:13,706 ***** Save model *****
2021-07-27 09:59:16,710 Writing example 0 of 9832
2021-07-27 09:59:16,710 *** Example ***
2021-07-27 09:59:16,710 guid: dev_matched-0
2021-07-27 09:59:16,710 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-27 09:59:16,711 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:59:16,711 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:59:16,711 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 09:59:16,711 label: contradiction
2021-07-27 09:59:16,711 label_id: 0
2021-07-27 09:59:21,242 ***** Running mm evaluation *****
2021-07-27 09:59:21,243   Num examples = 9832
2021-07-27 09:59:21,243   Batch size = 32
2021-07-27 09:59:32,150 ***** Eval results *****
2021-07-27 09:59:32,150   acc = 0.8441822620016274
2021-07-27 09:59:32,150   eval_loss = 0.4057799448537362
2021-07-27 09:59:32,150   global_step = 99
2021-07-27 10:13:34,501 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 10:13:34,681 device: cuda n_gpu: 4
2021-07-27 10:13:58,754 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 10:13:58,879 device: cuda n_gpu: 4
2021-07-27 10:14:06,609 Writing example 0 of 505555
2021-07-27 10:14:06,610 *** Example ***
2021-07-27 10:14:06,611 guid: aug-0
2021-07-27 10:14:06,611 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-27 10:14:06,611 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:14:06,611 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:14:06,611 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:14:06,611 label: neutral
2021-07-27 10:14:06,611 label_id: 2
2021-07-27 10:14:11,318 Writing example 10000 of 505555
2021-07-27 10:14:15,936 Writing example 20000 of 505555
2021-07-27 10:14:20,689 Writing example 30000 of 505555
2021-07-27 10:14:25,420 Writing example 40000 of 505555
2021-07-27 10:14:30,253 Writing example 50000 of 505555
2021-07-27 10:14:34,785 Writing example 60000 of 505555
2021-07-27 10:14:39,479 Writing example 70000 of 505555
2021-07-27 10:14:44,388 Writing example 80000 of 505555
2021-07-27 10:14:49,281 Writing example 90000 of 505555
2021-07-27 10:14:53,890 Writing example 100000 of 505555
2021-07-27 10:14:58,504 Writing example 110000 of 505555
2021-07-27 10:15:02,882 Writing example 120000 of 505555
2021-07-27 10:15:07,623 Writing example 130000 of 505555
2021-07-27 10:15:12,963 Writing example 140000 of 505555
2021-07-27 10:15:17,407 Writing example 150000 of 505555
2021-07-27 10:15:22,201 Writing example 160000 of 505555
2021-07-27 10:15:26,932 Writing example 170000 of 505555
2021-07-27 10:15:31,568 Writing example 180000 of 505555
2021-07-27 10:15:36,243 Writing example 190000 of 505555
2021-07-27 10:15:40,907 Writing example 200000 of 505555
2021-07-27 10:15:46,344 Writing example 210000 of 505555
2021-07-27 10:15:50,851 Writing example 220000 of 505555
2021-07-27 10:15:55,509 Writing example 230000 of 505555
2021-07-27 10:16:00,069 Writing example 240000 of 505555
2021-07-27 10:16:04,803 Writing example 250000 of 505555
2021-07-27 10:16:09,541 Writing example 260000 of 505555
2021-07-27 10:16:14,336 Writing example 270000 of 505555
2021-07-27 10:16:18,821 Writing example 280000 of 505555
2021-07-27 10:16:24,493 Writing example 290000 of 505555
2021-07-27 10:16:29,198 Writing example 300000 of 505555
2021-07-27 10:16:33,764 Writing example 310000 of 505555
2021-07-27 10:16:38,367 Writing example 320000 of 505555
2021-07-27 10:16:43,063 Writing example 330000 of 505555
2021-07-27 10:16:47,616 Writing example 340000 of 505555
2021-07-27 10:16:52,145 Writing example 350000 of 505555
2021-07-27 10:16:56,665 Writing example 360000 of 505555
2021-07-27 10:17:01,180 Writing example 370000 of 505555
2021-07-27 10:17:05,766 Writing example 380000 of 505555
2021-07-27 10:17:11,853 Writing example 390000 of 505555
2021-07-27 10:17:16,480 Writing example 400000 of 505555
2021-07-27 10:17:21,047 Writing example 410000 of 505555
2021-07-27 10:17:25,522 Writing example 420000 of 505555
2021-07-27 10:17:29,979 Writing example 430000 of 505555
2021-07-27 10:17:34,517 Writing example 440000 of 505555
2021-07-27 10:17:39,210 Writing example 450000 of 505555
2021-07-27 10:17:43,851 Writing example 460000 of 505555
2021-07-27 10:17:48,433 Writing example 470000 of 505555
2021-07-27 10:17:53,095 Writing example 480000 of 505555
2021-07-27 10:17:57,702 Writing example 490000 of 505555
2021-07-27 10:18:02,273 Writing example 500000 of 505555
2021-07-27 10:18:09,481 Writing example 0 of 9815
2021-07-27 10:18:09,482 *** Example ***
2021-07-27 10:18:09,482 guid: dev_matched-0
2021-07-27 10:18:09,482 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-27 10:18:09,482 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:18:09,482 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:18:09,482 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:18:09,482 label: neutral
2021-07-27 10:18:09,482 label_id: 2
2021-07-27 10:18:13,832 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-27 10:18:16,236 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-27 10:18:16,425 loading model...
2021-07-27 10:18:16,763 done!
2021-07-27 10:18:16,763 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-27 10:18:16,763 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-27 10:18:19,356 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-27 10:18:20,851 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-07-27 10:18:21,446 loading model...
2021-07-27 10:18:21,459 done!
2021-07-27 10:18:21,459 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-07-27 10:18:21,459 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-07-27 10:18:21,526 ***** Running training *****
2021-07-27 10:18:21,527   Num examples = 505555
2021-07-27 10:18:21,527   Batch size = 32
2021-07-27 10:18:21,527   Num steps = 47394
2021-07-27 10:18:21,528 n: module.bert.embeddings.word_embeddings.weight
2021-07-27 10:18:21,528 n: module.bert.embeddings.position_embeddings.weight
2021-07-27 10:18:21,528 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-27 10:18:21,528 n: module.bert.embeddings.LayerNorm.weight
2021-07-27 10:18:21,528 n: module.bert.embeddings.LayerNorm.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-27 10:18:21,528 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-27 10:18:21,529 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-27 10:18:21,530 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-27 10:18:21,531 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-27 10:18:21,532 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-27 10:18:21,533 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-27 10:18:21,533 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-27 10:18:21,533 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-27 10:18:21,533 n: module.bert.pooler.dense.weight
2021-07-27 10:18:21,533 n: module.bert.pooler.dense.bias
2021-07-27 10:18:21,533 n: module.classifier.weight
2021-07-27 10:18:21,533 n: module.classifier.bias
2021-07-27 10:18:21,533 n: module.fit_dense.weight
2021-07-27 10:18:21,533 n: module.fit_dense.bias
2021-07-27 10:18:21,533 Total parameters: 67547907
2021-07-27 10:24:26,789 ***** Running evaluation *****
2021-07-27 10:24:26,798   Epoch = 0 iter 1999 step
2021-07-27 10:24:26,798   Num examples = 9815
2021-07-27 10:24:26,798   Batch size = 32
2021-07-27 10:24:37,490 ***** Eval results *****
2021-07-27 10:24:37,490   acc = 0.3217524197656648
2021-07-27 10:24:37,490   att_loss = 0.0
2021-07-27 10:24:37,490   cls_loss = 0.39543532223448624
2021-07-27 10:24:37,490   eval_loss = 1.094856278515794
2021-07-27 10:24:37,490   global_step = 1999
2021-07-27 10:24:37,490   loss = 0.39543532223448624
2021-07-27 10:24:37,490   rep_loss = 0.0
2021-07-27 10:24:37,491 ***** Save model *****
2021-07-27 10:24:38,222 Writing example 0 of 9832
2021-07-27 10:24:38,223 *** Example ***
2021-07-27 10:24:38,223 guid: dev_matched-0
2021-07-27 10:24:38,223 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-27 10:24:38,223 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:24:38,223 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:24:38,223 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:24:38,223 label: contradiction
2021-07-27 10:24:38,223 label_id: 0
2021-07-27 10:24:42,844 ***** Running mm evaluation *****
2021-07-27 10:24:42,844   Num examples = 9832
2021-07-27 10:24:42,844   Batch size = 32
2021-07-27 10:24:55,168 ***** Eval results *****
2021-07-27 10:24:55,168   acc = 0.3202807160292921
2021-07-27 10:24:55,168   eval_loss = 1.0965179204940796
2021-07-27 10:24:55,168   global_step = 1999
2021-07-27 10:25:11,638 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 10:25:11,766 device: cuda n_gpu: 4
2021-07-27 10:25:33,745 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-07-27 10:25:33,888 device: cuda n_gpu: 4
2021-07-27 10:25:41,444 Writing example 0 of 505555
2021-07-27 10:25:41,446 *** Example ***
2021-07-27 10:25:41,446 guid: aug-0
2021-07-27 10:25:41,446 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-07-27 10:25:41,446 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:25:41,446 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:25:41,446 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:25:41,446 label: neutral
2021-07-27 10:25:41,446 label_id: 2
2021-07-27 10:25:46,089 Writing example 10000 of 505555
2021-07-27 10:25:50,648 Writing example 20000 of 505555
2021-07-27 10:25:55,189 Writing example 30000 of 505555
2021-07-27 10:25:59,934 Writing example 40000 of 505555
2021-07-27 10:26:04,882 Writing example 50000 of 505555
2021-07-27 10:26:09,340 Writing example 60000 of 505555
2021-07-27 10:26:13,928 Writing example 70000 of 505555
2021-07-27 10:26:18,452 Writing example 80000 of 505555
2021-07-27 10:26:23,319 Writing example 90000 of 505555
2021-07-27 10:26:27,857 Writing example 100000 of 505555
2021-07-27 10:26:32,405 Writing example 110000 of 505555
2021-07-27 10:26:36,808 Writing example 120000 of 505555
2021-07-27 10:26:41,428 Writing example 130000 of 505555
2021-07-27 10:26:46,683 Writing example 140000 of 505555
2021-07-27 10:26:51,069 Writing example 150000 of 505555
2021-07-27 10:26:55,797 Writing example 160000 of 505555
2021-07-27 10:27:00,403 Writing example 170000 of 505555
2021-07-27 10:27:04,948 Writing example 180000 of 505555
2021-07-27 10:27:09,511 Writing example 190000 of 505555
2021-07-27 10:27:14,202 Writing example 200000 of 505555
2021-07-27 10:27:19,552 Writing example 210000 of 505555
2021-07-27 10:27:23,971 Writing example 220000 of 505555
2021-07-27 10:27:28,560 Writing example 230000 of 505555
2021-07-27 10:27:33,053 Writing example 240000 of 505555
2021-07-27 10:27:37,716 Writing example 250000 of 505555
2021-07-27 10:27:42,243 Writing example 260000 of 505555
2021-07-27 10:27:46,692 Writing example 270000 of 505555
2021-07-27 10:27:51,118 Writing example 280000 of 505555
2021-07-27 10:27:56,765 Writing example 290000 of 505555
2021-07-27 10:28:01,396 Writing example 300000 of 505555
2021-07-27 10:28:05,983 Writing example 310000 of 505555
2021-07-27 10:28:10,357 Writing example 320000 of 505555
2021-07-27 10:28:14,987 Writing example 330000 of 505555
2021-07-27 10:28:19,378 Writing example 340000 of 505555
2021-07-27 10:28:23,885 Writing example 350000 of 505555
2021-07-27 10:28:28,331 Writing example 360000 of 505555
2021-07-27 10:28:32,686 Writing example 370000 of 505555
2021-07-27 10:28:37,139 Writing example 380000 of 505555
2021-07-27 10:28:43,125 Writing example 390000 of 505555
2021-07-27 10:28:47,817 Writing example 400000 of 505555
2021-07-27 10:28:52,326 Writing example 410000 of 505555
2021-07-27 10:28:56,736 Writing example 420000 of 505555
2021-07-27 10:29:01,086 Writing example 430000 of 505555
2021-07-27 10:29:05,494 Writing example 440000 of 505555
2021-07-27 10:29:10,079 Writing example 450000 of 505555
2021-07-27 10:29:14,628 Writing example 460000 of 505555
2021-07-27 10:29:19,216 Writing example 470000 of 505555
2021-07-27 10:29:23,965 Writing example 480000 of 505555
2021-07-27 10:29:28,513 Writing example 490000 of 505555
2021-07-27 10:29:33,056 Writing example 500000 of 505555
2021-07-27 10:29:40,205 Writing example 0 of 9815
2021-07-27 10:29:40,205 *** Example ***
2021-07-27 10:29:40,205 guid: dev_matched-0
2021-07-27 10:29:40,205 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-07-27 10:29:40,205 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:29:40,205 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:29:40,206 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:29:40,206 label: neutral
2021-07-27 10:29:40,206 label_id: 2
2021-07-27 10:29:44,680 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-07-27 10:29:47,156 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-07-27 10:29:47,335 loading model...
2021-07-27 10:29:47,674 done!
2021-07-27 10:29:47,674 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-07-27 10:29:47,675 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-07-27 10:29:50,257 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-07-27 10:29:51,747 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-07-27 10:29:51,869 loading model...
2021-07-27 10:29:51,882 done!
2021-07-27 10:29:51,882 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-07-27 10:29:51,882 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-07-27 10:29:51,949 ***** Running training *****
2021-07-27 10:29:51,950   Num examples = 505555
2021-07-27 10:29:51,950   Batch size = 32
2021-07-27 10:29:51,950   Num steps = 47394
2021-07-27 10:29:51,951 n: module.bert.embeddings.word_embeddings.weight
2021-07-27 10:29:51,951 n: module.bert.embeddings.position_embeddings.weight
2021-07-27 10:29:51,951 n: module.bert.embeddings.token_type_embeddings.weight
2021-07-27 10:29:51,951 n: module.bert.embeddings.LayerNorm.weight
2021-07-27 10:29:51,951 n: module.bert.embeddings.LayerNorm.bias
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-07-27 10:29:51,951 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.output.dense.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.output.dense.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.output.dense.weight
2021-07-27 10:29:51,952 n: module.bert.encoder.layer.1.output.dense.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.output.dense.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.output.dense.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-07-27 10:29:51,953 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.output.dense.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.output.dense.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-07-27 10:29:51,954 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.output.dense.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.output.dense.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-07-27 10:29:51,955 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-07-27 10:29:51,956 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-07-27 10:29:51,956 n: module.bert.encoder.layer.5.output.dense.weight
2021-07-27 10:29:51,956 n: module.bert.encoder.layer.5.output.dense.bias
2021-07-27 10:29:51,956 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-07-27 10:29:51,956 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-07-27 10:29:51,956 n: module.bert.pooler.dense.weight
2021-07-27 10:29:51,956 n: module.bert.pooler.dense.bias
2021-07-27 10:29:51,956 n: module.classifier.weight
2021-07-27 10:29:51,956 n: module.classifier.bias
2021-07-27 10:29:51,956 n: module.fit_dense.weight
2021-07-27 10:29:51,956 n: module.fit_dense.bias
2021-07-27 10:29:51,956 Total parameters: 67547907
2021-07-27 10:30:15,523 ***** Running evaluation *****
2021-07-27 10:30:15,524   Epoch = 0 iter 99 step
2021-07-27 10:30:15,524   Num examples = 9815
2021-07-27 10:30:15,524   Batch size = 32
2021-07-27 10:30:28,351 ***** Eval results *****
2021-07-27 10:30:28,351   acc = 0.8453387671930719
2021-07-27 10:30:28,351   att_loss = 0.0
2021-07-27 10:30:28,351   cls_loss = 0.8217328925325413
2021-07-27 10:30:28,351   eval_loss = 0.40078058440832826
2021-07-27 10:30:28,351   global_step = 99
2021-07-27 10:30:28,351   loss = 0.8217328925325413
2021-07-27 10:30:28,351   rep_loss = 0.0
2021-07-27 10:30:28,352 ***** Save model *****
2021-07-27 10:30:29,044 Writing example 0 of 9832
2021-07-27 10:30:29,045 *** Example ***
2021-07-27 10:30:29,045 guid: dev_matched-0
2021-07-27 10:30:29,045 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-07-27 10:30:29,045 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:30:29,045 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:30:29,045 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-07-27 10:30:29,045 label: contradiction
2021-07-27 10:30:29,045 label_id: 0
2021-07-27 10:30:33,583 ***** Running mm evaluation *****
2021-07-27 10:30:33,583   Num examples = 9832
2021-07-27 10:30:33,583   Batch size = 32
2021-08-01 22:48:49,626 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-01 22:48:49,795 device: cuda n_gpu: 4
2021-08-01 22:49:47,226 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-01 22:49:47,349 device: cuda n_gpu: 4
2021-08-01 22:49:55,898 Writing example 0 of 505555
2021-08-01 22:49:55,899 *** Example ***
2021-08-01 22:49:55,899 guid: aug-0
2021-08-01 22:49:55,899 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-01 22:49:55,899 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:49:55,899 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:49:55,899 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:49:55,899 label: neutral
2021-08-01 22:49:55,899 label_id: 2
2021-08-01 22:50:00,636 Writing example 10000 of 505555
2021-08-01 22:50:05,254 Writing example 20000 of 505555
2021-08-01 22:50:09,915 Writing example 30000 of 505555
2021-08-01 22:50:14,618 Writing example 40000 of 505555
2021-08-01 22:50:19,410 Writing example 50000 of 505555
2021-08-01 22:50:23,908 Writing example 60000 of 505555
2021-08-01 22:50:28,570 Writing example 70000 of 505555
2021-08-01 22:50:33,334 Writing example 80000 of 505555
2021-08-01 22:50:38,200 Writing example 90000 of 505555
2021-08-01 22:50:42,800 Writing example 100000 of 505555
2021-08-01 22:50:47,391 Writing example 110000 of 505555
2021-08-01 22:50:51,756 Writing example 120000 of 505555
2021-08-01 22:50:56,476 Writing example 130000 of 505555
2021-08-01 22:51:01,760 Writing example 140000 of 505555
2021-08-01 22:51:06,183 Writing example 150000 of 505555
2021-08-01 22:51:10,966 Writing example 160000 of 505555
2021-08-01 22:51:15,690 Writing example 170000 of 505555
2021-08-01 22:51:20,402 Writing example 180000 of 505555
2021-08-01 22:51:25,161 Writing example 190000 of 505555
2021-08-01 22:51:29,743 Writing example 200000 of 505555
2021-08-01 22:51:35,208 Writing example 210000 of 505555
2021-08-01 22:51:39,678 Writing example 220000 of 505555
2021-08-01 22:51:44,317 Writing example 230000 of 505555
2021-08-01 22:51:48,853 Writing example 240000 of 505555
2021-08-01 22:51:53,569 Writing example 250000 of 505555
2021-08-01 22:51:58,186 Writing example 260000 of 505555
2021-08-01 22:52:02,685 Writing example 270000 of 505555
2021-08-01 22:52:07,138 Writing example 280000 of 505555
2021-08-01 22:52:12,796 Writing example 290000 of 505555
2021-08-01 22:52:17,471 Writing example 300000 of 505555
2021-08-01 22:52:21,998 Writing example 310000 of 505555
2021-08-01 22:52:26,482 Writing example 320000 of 505555
2021-08-01 22:52:31,183 Writing example 330000 of 505555
2021-08-01 22:52:35,611 Writing example 340000 of 505555
2021-08-01 22:52:40,162 Writing example 350000 of 505555
2021-08-01 22:52:44,700 Writing example 360000 of 505555
2021-08-01 22:52:49,099 Writing example 370000 of 505555
2021-08-01 22:52:53,621 Writing example 380000 of 505555
2021-08-01 22:52:59,629 Writing example 390000 of 505555
2021-08-01 22:53:04,236 Writing example 400000 of 505555
2021-08-01 22:53:08,778 Writing example 410000 of 505555
2021-08-01 22:53:13,221 Writing example 420000 of 505555
2021-08-01 22:53:17,705 Writing example 430000 of 505555
2021-08-01 22:53:22,114 Writing example 440000 of 505555
2021-08-01 22:53:26,770 Writing example 450000 of 505555
2021-08-01 22:53:31,363 Writing example 460000 of 505555
2021-08-01 22:53:35,949 Writing example 470000 of 505555
2021-08-01 22:53:40,651 Writing example 480000 of 505555
2021-08-01 22:53:45,227 Writing example 490000 of 505555
2021-08-01 22:53:49,777 Writing example 500000 of 505555
2021-08-01 22:53:57,292 Writing example 0 of 9815
2021-08-01 22:53:57,293 *** Example ***
2021-08-01 22:53:57,293 guid: dev_matched-0
2021-08-01 22:53:57,293 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-01 22:53:57,293 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:53:57,293 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:53:57,293 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:53:57,293 label: neutral
2021-08-01 22:53:57,293 label_id: 2
2021-08-01 22:54:01,605 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-01 22:54:04,011 Loading model /home/mcao610/scratch/huggingface/MNLI/pytorch_model.bin
2021-08-01 22:54:08,462 loading model...
2021-08-01 22:54:08,496 done!
2021-08-01 22:54:08,496 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-01 22:54:08,496 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-01 22:54:15,835 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-01 22:54:17,324 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-01 22:54:19,751 loading model...
2021-08-01 22:54:19,764 done!
2021-08-01 22:54:19,764 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-01 22:54:19,764 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-01 22:54:19,831 ***** Running training *****
2021-08-01 22:54:19,831   Num examples = 505555
2021-08-01 22:54:19,831   Batch size = 32
2021-08-01 22:54:19,831   Num steps = 47394
2021-08-01 22:54:19,832 n: module.bert.embeddings.word_embeddings.weight
2021-08-01 22:54:19,832 n: module.bert.embeddings.position_embeddings.weight
2021-08-01 22:54:19,832 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-01 22:54:19,832 n: module.bert.embeddings.LayerNorm.weight
2021-08-01 22:54:19,833 n: module.bert.embeddings.LayerNorm.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-01 22:54:19,833 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-01 22:54:19,834 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-01 22:54:19,835 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-01 22:54:19,836 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-01 22:54:19,837 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-01 22:54:19,837 n: module.bert.pooler.dense.weight
2021-08-01 22:54:19,837 n: module.bert.pooler.dense.bias
2021-08-01 22:54:19,837 n: module.classifier.weight
2021-08-01 22:54:19,838 n: module.classifier.bias
2021-08-01 22:54:19,838 n: module.fit_dense.weight
2021-08-01 22:54:19,838 n: module.fit_dense.bias
2021-08-01 22:54:19,838 Total parameters: 67547907
2021-08-01 22:57:35,503 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-01 22:57:35,627 device: cuda n_gpu: 4
2021-08-01 22:57:43,107 Writing example 0 of 505555
2021-08-01 22:57:43,109 *** Example ***
2021-08-01 22:57:43,109 guid: aug-0
2021-08-01 22:57:43,109 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-01 22:57:43,109 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:57:43,109 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:57:43,109 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 22:57:43,109 label: neutral
2021-08-01 22:57:43,109 label_id: 2
2021-08-01 22:57:47,916 Writing example 10000 of 505555
2021-08-01 22:57:52,561 Writing example 20000 of 505555
2021-08-01 22:57:57,129 Writing example 30000 of 505555
2021-08-01 22:58:01,791 Writing example 40000 of 505555
2021-08-01 22:58:06,534 Writing example 50000 of 505555
2021-08-01 22:58:10,981 Writing example 60000 of 505555
2021-08-01 22:58:15,559 Writing example 70000 of 505555
2021-08-01 22:58:20,074 Writing example 80000 of 505555
2021-08-01 22:58:24,887 Writing example 90000 of 505555
2021-08-01 22:58:29,462 Writing example 100000 of 505555
2021-08-01 22:58:34,011 Writing example 110000 of 505555
2021-08-01 22:58:38,328 Writing example 120000 of 505555
2021-08-01 22:58:42,988 Writing example 130000 of 505555
2021-08-01 22:58:48,276 Writing example 140000 of 505555
2021-08-01 22:58:52,676 Writing example 150000 of 505555
2021-08-01 22:58:57,438 Writing example 160000 of 505555
2021-08-01 22:59:02,035 Writing example 170000 of 505555
2021-08-01 22:59:06,572 Writing example 180000 of 505555
2021-08-01 22:59:11,219 Writing example 190000 of 505555
2021-08-01 22:59:15,776 Writing example 200000 of 505555
2021-08-01 22:59:21,136 Writing example 210000 of 505555
2021-08-01 22:59:25,546 Writing example 220000 of 505555
2021-08-01 22:59:30,174 Writing example 230000 of 505555
2021-08-01 22:59:34,819 Writing example 240000 of 505555
2021-08-01 22:59:39,483 Writing example 250000 of 505555
2021-08-01 22:59:44,054 Writing example 260000 of 505555
2021-08-01 22:59:48,492 Writing example 270000 of 505555
2021-08-01 22:59:52,900 Writing example 280000 of 505555
2021-08-01 22:59:58,564 Writing example 290000 of 505555
2021-08-01 23:00:03,201 Writing example 300000 of 505555
2021-08-01 23:00:07,731 Writing example 310000 of 505555
2021-08-01 23:00:12,176 Writing example 320000 of 505555
2021-08-01 23:00:16,799 Writing example 330000 of 505555
2021-08-01 23:00:21,221 Writing example 340000 of 505555
2021-08-01 23:00:25,750 Writing example 350000 of 505555
2021-08-01 23:00:30,402 Writing example 360000 of 505555
2021-08-01 23:00:34,747 Writing example 370000 of 505555
2021-08-01 23:00:39,288 Writing example 380000 of 505555
2021-08-01 23:00:45,113 Writing example 390000 of 505555
2021-08-01 23:00:49,672 Writing example 400000 of 505555
2021-08-01 23:00:54,184 Writing example 410000 of 505555
2021-08-01 23:00:58,643 Writing example 420000 of 505555
2021-08-01 23:01:02,989 Writing example 430000 of 505555
2021-08-01 23:01:07,412 Writing example 440000 of 505555
2021-08-01 23:01:12,037 Writing example 450000 of 505555
2021-08-01 23:01:16,590 Writing example 460000 of 505555
2021-08-01 23:01:21,100 Writing example 470000 of 505555
2021-08-01 23:01:25,693 Writing example 480000 of 505555
2021-08-01 23:01:30,234 Writing example 490000 of 505555
2021-08-01 23:01:34,731 Writing example 500000 of 505555
2021-08-01 23:01:41,797 Writing example 0 of 9815
2021-08-01 23:01:41,797 *** Example ***
2021-08-01 23:01:41,797 guid: dev_matched-0
2021-08-01 23:01:41,797 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-01 23:01:41,797 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:01:41,798 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:01:41,798 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:01:41,798 label: neutral
2021-08-01 23:01:41,798 label_id: 2
2021-08-01 23:01:46,267 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-01 23:01:48,660 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-08-01 23:01:50,687 loading model...
2021-08-01 23:01:51,031 done!
2021-08-01 23:01:51,032 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-01 23:01:51,032 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-01 23:01:53,595 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-01 23:01:55,081 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-01 23:01:55,206 loading model...
2021-08-01 23:01:55,219 done!
2021-08-01 23:01:55,219 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-01 23:01:55,219 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-01 23:01:55,286 ***** Running training *****
2021-08-01 23:01:55,286   Num examples = 505555
2021-08-01 23:01:55,286   Batch size = 32
2021-08-01 23:01:55,286   Num steps = 47394
2021-08-01 23:01:55,287 n: module.bert.embeddings.word_embeddings.weight
2021-08-01 23:01:55,287 n: module.bert.embeddings.position_embeddings.weight
2021-08-01 23:01:55,287 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-01 23:01:55,287 n: module.bert.embeddings.LayerNorm.weight
2021-08-01 23:01:55,287 n: module.bert.embeddings.LayerNorm.bias
2021-08-01 23:01:55,287 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-01 23:01:55,287 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-01 23:01:55,287 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-01 23:01:55,287 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-01 23:01:55,287 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-01 23:01:55,288 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-01 23:01:55,289 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-01 23:01:55,290 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-01 23:01:55,291 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-01 23:01:55,292 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-01 23:01:55,292 n: module.bert.pooler.dense.weight
2021-08-01 23:01:55,292 n: module.bert.pooler.dense.bias
2021-08-01 23:01:55,292 n: module.classifier.weight
2021-08-01 23:01:55,292 n: module.classifier.bias
2021-08-01 23:01:55,292 n: module.fit_dense.weight
2021-08-01 23:01:55,292 n: module.fit_dense.bias
2021-08-01 23:01:55,292 Total parameters: 67547907
2021-08-01 23:02:03,703 ***** Running evaluation *****
2021-08-01 23:02:03,704   Epoch = 0 iter 9 step
2021-08-01 23:02:03,704   Num examples = 9815
2021-08-01 23:02:03,704   Batch size = 32
2021-08-01 23:02:14,566 ***** Eval results *****
2021-08-01 23:02:14,566   acc = 0.8456444218033622
2021-08-01 23:02:14,567   att_loss = 0.0
2021-08-01 23:02:14,567   cls_loss = 0.957632429069943
2021-08-01 23:02:14,567   eval_loss = 0.44271945346644337
2021-08-01 23:02:14,567   global_step = 9
2021-08-01 23:02:14,567   loss = 0.957632429069943
2021-08-01 23:02:14,567   rep_loss = 0.0
2021-08-01 23:02:14,568 ***** Save model *****
2021-08-01 23:02:15,739 Writing example 0 of 9832
2021-08-01 23:02:15,740 *** Example ***
2021-08-01 23:02:15,740 guid: dev_matched-0
2021-08-01 23:02:15,740 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-01 23:02:15,740 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:02:15,740 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:02:15,740 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:02:15,741 label: contradiction
2021-08-01 23:02:15,741 label_id: 0
2021-08-01 23:02:20,268 ***** Running mm evaluation *****
2021-08-01 23:02:20,269   Num examples = 9832
2021-08-01 23:02:20,269   Batch size = 32
2021-08-01 23:02:31,167 ***** Eval results *****
2021-08-01 23:02:31,167   acc = 0.845606183889341
2021-08-01 23:02:31,167   eval_loss = 0.4399795558593877
2021-08-01 23:02:31,167   global_step = 9
2021-08-01 23:02:32,918 ***** Running evaluation *****
2021-08-01 23:02:32,918   Epoch = 0 iter 19 step
2021-08-01 23:02:32,918   Num examples = 9832
2021-08-01 23:02:32,918   Batch size = 32
2021-08-01 23:43:43,289 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-01 23:43:43,388 device: cuda n_gpu: 3
2021-08-01 23:43:53,015 Writing example 0 of 505555
2021-08-01 23:43:53,016 *** Example ***
2021-08-01 23:43:53,016 guid: aug-0
2021-08-01 23:43:53,016 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-01 23:43:53,016 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:43:53,016 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:43:53,017 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:43:53,017 label: neutral
2021-08-01 23:43:53,017 label_id: 2
2021-08-01 23:43:57,701 Writing example 10000 of 505555
2021-08-01 23:44:02,345 Writing example 20000 of 505555
2021-08-01 23:44:07,118 Writing example 30000 of 505555
2021-08-01 23:44:11,918 Writing example 40000 of 505555
2021-08-01 23:44:16,709 Writing example 50000 of 505555
2021-08-01 23:44:21,265 Writing example 60000 of 505555
2021-08-01 23:44:25,875 Writing example 70000 of 505555
2021-08-01 23:44:30,429 Writing example 80000 of 505555
2021-08-01 23:44:35,275 Writing example 90000 of 505555
2021-08-01 23:44:39,842 Writing example 100000 of 505555
2021-08-01 23:44:44,475 Writing example 110000 of 505555
2021-08-01 23:44:48,989 Writing example 120000 of 505555
2021-08-01 23:44:53,913 Writing example 130000 of 505555
2021-08-01 23:44:59,254 Writing example 140000 of 505555
2021-08-01 23:45:03,655 Writing example 150000 of 505555
2021-08-01 23:45:08,404 Writing example 160000 of 505555
2021-08-01 23:45:13,125 Writing example 170000 of 505555
2021-08-01 23:45:17,728 Writing example 180000 of 505555
2021-08-01 23:45:22,329 Writing example 190000 of 505555
2021-08-01 23:45:26,887 Writing example 200000 of 505555
2021-08-01 23:45:32,279 Writing example 210000 of 505555
2021-08-01 23:45:36,715 Writing example 220000 of 505555
2021-08-01 23:45:41,337 Writing example 230000 of 505555
2021-08-01 23:45:45,887 Writing example 240000 of 505555
2021-08-01 23:45:50,523 Writing example 250000 of 505555
2021-08-01 23:45:55,066 Writing example 260000 of 505555
2021-08-01 23:45:59,547 Writing example 270000 of 505555
2021-08-01 23:46:03,984 Writing example 280000 of 505555
2021-08-01 23:46:09,720 Writing example 290000 of 505555
2021-08-01 23:46:14,472 Writing example 300000 of 505555
2021-08-01 23:46:18,978 Writing example 310000 of 505555
2021-08-01 23:46:23,373 Writing example 320000 of 505555
2021-08-01 23:46:28,088 Writing example 330000 of 505555
2021-08-01 23:46:32,556 Writing example 340000 of 505555
2021-08-01 23:46:37,051 Writing example 350000 of 505555
2021-08-01 23:46:41,524 Writing example 360000 of 505555
2021-08-01 23:46:46,085 Writing example 370000 of 505555
2021-08-01 23:46:50,577 Writing example 380000 of 505555
2021-08-01 23:46:56,533 Writing example 390000 of 505555
2021-08-01 23:47:01,178 Writing example 400000 of 505555
2021-08-01 23:47:05,700 Writing example 410000 of 505555
2021-08-01 23:47:10,132 Writing example 420000 of 505555
2021-08-01 23:47:14,561 Writing example 430000 of 505555
2021-08-01 23:47:18,944 Writing example 440000 of 505555
2021-08-01 23:47:23,633 Writing example 450000 of 505555
2021-08-01 23:47:28,296 Writing example 460000 of 505555
2021-08-01 23:47:32,843 Writing example 470000 of 505555
2021-08-01 23:47:37,458 Writing example 480000 of 505555
2021-08-01 23:47:42,021 Writing example 490000 of 505555
2021-08-01 23:47:46,619 Writing example 500000 of 505555
2021-08-01 23:47:54,021 Writing example 0 of 9815
2021-08-01 23:47:54,022 *** Example ***
2021-08-01 23:47:54,022 guid: dev_matched-0
2021-08-01 23:47:54,022 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-01 23:47:54,022 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:47:54,022 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:47:54,022 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-01 23:47:54,022 label: neutral
2021-08-01 23:47:54,022 label_id: 2
2021-08-01 23:47:58,450 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-01 23:48:00,952 Loading model /home/mcao610/scratch/huggingface/MNLI/pytorch_model.bin
2021-08-01 23:48:07,481 loading model...
2021-08-01 23:48:07,514 done!
2021-08-01 23:48:07,514 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-01 23:48:07,514 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-01 23:48:13,482 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-01 23:48:14,985 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-01 23:48:17,216 loading model...
2021-08-01 23:48:17,230 done!
2021-08-01 23:48:17,230 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-01 23:48:17,230 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-01 23:48:17,297 ***** Running training *****
2021-08-01 23:48:17,297   Num examples = 505555
2021-08-01 23:48:17,297   Batch size = 32
2021-08-01 23:48:17,297   Num steps = 47394
2021-08-01 23:48:17,298 n: module.bert.embeddings.word_embeddings.weight
2021-08-01 23:48:17,298 n: module.bert.embeddings.position_embeddings.weight
2021-08-01 23:48:17,298 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-01 23:48:17,298 n: module.bert.embeddings.LayerNorm.weight
2021-08-01 23:48:17,298 n: module.bert.embeddings.LayerNorm.bias
2021-08-01 23:48:17,298 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-01 23:48:17,298 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-01 23:48:17,299 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-01 23:48:17,300 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-01 23:48:17,301 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-01 23:48:17,302 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-01 23:48:17,303 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-01 23:48:17,303 n: module.bert.pooler.dense.weight
2021-08-01 23:48:17,303 n: module.bert.pooler.dense.bias
2021-08-01 23:48:17,303 n: module.classifier.weight
2021-08-01 23:48:17,303 n: module.classifier.bias
2021-08-01 23:48:17,303 n: module.fit_dense.weight
2021-08-01 23:48:17,303 n: module.fit_dense.bias
2021-08-01 23:48:17,303 Total parameters: 67547907
2021-08-02 10:17:48,710 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-02 10:17:48,806 device: cuda n_gpu: 2
2021-08-02 10:17:57,460 Writing example 0 of 505555
2021-08-02 10:17:57,462 *** Example ***
2021-08-02 10:17:57,462 guid: aug-0
2021-08-02 10:17:57,462 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-02 10:17:57,462 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:17:57,462 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:17:57,462 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:17:57,462 label: neutral
2021-08-02 10:17:57,462 label_id: 2
2021-08-02 10:18:02,121 Writing example 10000 of 505555
2021-08-02 10:18:06,698 Writing example 20000 of 505555
2021-08-02 10:18:11,165 Writing example 30000 of 505555
2021-08-02 10:18:15,814 Writing example 40000 of 505555
2021-08-02 10:18:20,558 Writing example 50000 of 505555
2021-08-02 10:18:25,014 Writing example 60000 of 505555
2021-08-02 10:18:29,630 Writing example 70000 of 505555
2021-08-02 10:18:34,151 Writing example 80000 of 505555
2021-08-02 10:18:39,059 Writing example 90000 of 505555
2021-08-02 10:18:43,587 Writing example 100000 of 505555
2021-08-02 10:18:48,122 Writing example 110000 of 505555
2021-08-02 10:18:52,435 Writing example 120000 of 505555
2021-08-02 10:18:57,099 Writing example 130000 of 505555
2021-08-02 10:19:02,358 Writing example 140000 of 505555
2021-08-02 10:19:06,801 Writing example 150000 of 505555
2021-08-02 10:19:11,527 Writing example 160000 of 505555
2021-08-02 10:19:16,119 Writing example 170000 of 505555
2021-08-02 10:19:20,660 Writing example 180000 of 505555
2021-08-02 10:19:25,229 Writing example 190000 of 505555
2021-08-02 10:19:29,745 Writing example 200000 of 505555
2021-08-02 10:19:35,170 Writing example 210000 of 505555
2021-08-02 10:19:39,938 Writing example 220000 of 505555
2021-08-02 10:19:44,771 Writing example 230000 of 505555
2021-08-02 10:19:49,325 Writing example 240000 of 505555
2021-08-02 10:19:53,995 Writing example 250000 of 505555
2021-08-02 10:19:58,514 Writing example 260000 of 505555
2021-08-02 10:20:02,980 Writing example 270000 of 505555
2021-08-02 10:20:07,449 Writing example 280000 of 505555
2021-08-02 10:20:13,130 Writing example 290000 of 505555
2021-08-02 10:20:17,755 Writing example 300000 of 505555
2021-08-02 10:20:22,289 Writing example 310000 of 505555
2021-08-02 10:20:26,645 Writing example 320000 of 505555
2021-08-02 10:20:31,260 Writing example 330000 of 505555
2021-08-02 10:20:35,730 Writing example 340000 of 505555
2021-08-02 10:20:40,188 Writing example 350000 of 505555
2021-08-02 10:20:44,623 Writing example 360000 of 505555
2021-08-02 10:20:48,966 Writing example 370000 of 505555
2021-08-02 10:20:53,405 Writing example 380000 of 505555
2021-08-02 10:20:59,349 Writing example 390000 of 505555
2021-08-02 10:21:03,900 Writing example 400000 of 505555
2021-08-02 10:21:08,397 Writing example 410000 of 505555
2021-08-02 10:21:12,788 Writing example 420000 of 505555
2021-08-02 10:21:17,125 Writing example 430000 of 505555
2021-08-02 10:21:21,497 Writing example 440000 of 505555
2021-08-02 10:21:26,074 Writing example 450000 of 505555
2021-08-02 10:21:30,617 Writing example 460000 of 505555
2021-08-02 10:21:35,229 Writing example 470000 of 505555
2021-08-02 10:21:39,989 Writing example 480000 of 505555
2021-08-02 10:21:44,522 Writing example 490000 of 505555
2021-08-02 10:21:49,012 Writing example 500000 of 505555
2021-08-02 10:21:56,574 Writing example 0 of 9815
2021-08-02 10:21:56,575 *** Example ***
2021-08-02 10:21:56,575 guid: dev_matched-0
2021-08-02 10:21:56,575 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-02 10:21:56,575 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:21:56,575 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:21:56,575 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 10:21:56,575 label: neutral
2021-08-02 10:21:56,575 label_id: 2
2021-08-02 10:22:01,462 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-02 10:22:03,955 Loading model /home/mcao610/scratch/huggingface/MNLI/pytorch_model.bin
2021-08-02 10:22:10,079 loading model...
2021-08-02 10:22:10,113 done!
2021-08-02 10:22:10,114 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 10:22:10,114 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-02 10:22:17,863 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-02 10:22:19,344 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-02 10:22:21,890 loading model...
2021-08-02 10:22:21,903 done!
2021-08-02 10:22:21,904 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 10:22:21,904 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-02 10:22:21,970 ***** Running training *****
2021-08-02 10:22:21,970   Num examples = 505555
2021-08-02 10:22:21,970   Batch size = 32
2021-08-02 10:22:21,970   Num steps = 47394
2021-08-02 10:22:21,971 n: module.bert.embeddings.word_embeddings.weight
2021-08-02 10:22:21,971 n: module.bert.embeddings.position_embeddings.weight
2021-08-02 10:22:21,971 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-02 10:22:21,971 n: module.bert.embeddings.LayerNorm.weight
2021-08-02 10:22:21,971 n: module.bert.embeddings.LayerNorm.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-02 10:22:21,972 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-02 10:22:21,973 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-02 10:22:21,974 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-02 10:22:21,975 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-02 10:22:21,976 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-02 10:22:21,976 n: module.bert.pooler.dense.weight
2021-08-02 10:22:21,976 n: module.bert.pooler.dense.bias
2021-08-02 10:22:21,977 n: module.classifier.weight
2021-08-02 10:22:21,977 n: module.classifier.bias
2021-08-02 10:22:21,977 n: module.fit_dense.weight
2021-08-02 10:22:21,977 n: module.fit_dense.bias
2021-08-02 10:22:21,977 Total parameters: 67547907
2021-08-02 12:04:49,762 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-02 12:04:49,849 device: cuda n_gpu: 2
2021-08-02 12:04:58,893 Writing example 0 of 505555
2021-08-02 12:04:58,894 *** Example ***
2021-08-02 12:04:58,894 guid: aug-0
2021-08-02 12:04:58,894 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-02 12:04:58,894 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:04:58,894 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:04:58,895 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:04:58,895 label: neutral
2021-08-02 12:04:58,895 label_id: 2
2021-08-02 12:05:03,620 Writing example 10000 of 505555
2021-08-02 12:05:08,240 Writing example 20000 of 505555
2021-08-02 12:05:12,714 Writing example 30000 of 505555
2021-08-02 12:05:17,363 Writing example 40000 of 505555
2021-08-02 12:05:22,123 Writing example 50000 of 505555
2021-08-02 12:05:26,643 Writing example 60000 of 505555
2021-08-02 12:05:31,230 Writing example 70000 of 505555
2021-08-02 12:05:35,860 Writing example 80000 of 505555
2021-08-02 12:05:40,898 Writing example 90000 of 505555
2021-08-02 12:05:45,437 Writing example 100000 of 505555
2021-08-02 12:05:50,041 Writing example 110000 of 505555
2021-08-02 12:05:54,360 Writing example 120000 of 505555
2021-08-02 12:05:58,970 Writing example 130000 of 505555
2021-08-02 12:06:04,226 Writing example 140000 of 505555
2021-08-02 12:06:08,606 Writing example 150000 of 505555
2021-08-02 12:06:13,386 Writing example 160000 of 505555
2021-08-02 12:06:17,996 Writing example 170000 of 505555
2021-08-02 12:06:22,533 Writing example 180000 of 505555
2021-08-02 12:06:27,119 Writing example 190000 of 505555
2021-08-02 12:06:31,677 Writing example 200000 of 505555
2021-08-02 12:06:37,072 Writing example 210000 of 505555
2021-08-02 12:06:41,514 Writing example 220000 of 505555
2021-08-02 12:06:46,099 Writing example 230000 of 505555
2021-08-02 12:06:50,588 Writing example 240000 of 505555
2021-08-02 12:06:55,175 Writing example 250000 of 505555
2021-08-02 12:06:59,696 Writing example 260000 of 505555
2021-08-02 12:07:04,279 Writing example 270000 of 505555
2021-08-02 12:07:08,692 Writing example 280000 of 505555
2021-08-02 12:07:14,384 Writing example 290000 of 505555
2021-08-02 12:07:19,010 Writing example 300000 of 505555
2021-08-02 12:07:23,496 Writing example 310000 of 505555
2021-08-02 12:07:27,866 Writing example 320000 of 505555
2021-08-02 12:07:32,528 Writing example 330000 of 505555
2021-08-02 12:07:37,071 Writing example 340000 of 505555
2021-08-02 12:07:41,531 Writing example 350000 of 505555
2021-08-02 12:07:46,034 Writing example 360000 of 505555
2021-08-02 12:07:50,381 Writing example 370000 of 505555
2021-08-02 12:07:54,947 Writing example 380000 of 505555
2021-08-02 12:08:00,798 Writing example 390000 of 505555
2021-08-02 12:08:05,480 Writing example 400000 of 505555
2021-08-02 12:08:09,970 Writing example 410000 of 505555
2021-08-02 12:08:14,408 Writing example 420000 of 505555
2021-08-02 12:08:18,798 Writing example 430000 of 505555
2021-08-02 12:08:23,153 Writing example 440000 of 505555
2021-08-02 12:08:27,746 Writing example 450000 of 505555
2021-08-02 12:08:32,315 Writing example 460000 of 505555
2021-08-02 12:08:36,931 Writing example 470000 of 505555
2021-08-02 12:08:41,523 Writing example 480000 of 505555
2021-08-02 12:08:46,203 Writing example 490000 of 505555
2021-08-02 12:08:50,712 Writing example 500000 of 505555
2021-08-02 12:08:58,504 Writing example 0 of 9815
2021-08-02 12:08:58,504 *** Example ***
2021-08-02 12:08:58,504 guid: dev_matched-0
2021-08-02 12:08:58,504 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-02 12:08:58,504 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:08:58,504 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:08:58,504 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:08:58,504 label: neutral
2021-08-02 12:08:58,505 label_id: 2
2021-08-02 12:09:02,828 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-02 12:09:05,311 Loading model /home/mcao610/scratch/huggingface/MNLI/pytorch_model.bin
2021-08-02 12:09:11,958 loading model...
2021-08-02 12:09:11,993 done!
2021-08-02 12:09:11,993 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 12:09:11,993 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-02 12:09:19,595 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-02 12:09:21,079 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-02 12:09:23,551 loading model...
2021-08-02 12:09:23,564 done!
2021-08-02 12:09:23,564 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 12:09:23,564 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-02 12:09:23,631 ***** Running training *****
2021-08-02 12:09:23,631   Num examples = 505555
2021-08-02 12:09:23,632   Batch size = 32
2021-08-02 12:09:23,632   Num steps = 47394
2021-08-02 12:09:23,633 n: module.bert.embeddings.word_embeddings.weight
2021-08-02 12:09:23,633 n: module.bert.embeddings.position_embeddings.weight
2021-08-02 12:09:23,633 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-02 12:09:23,633 n: module.bert.embeddings.LayerNorm.weight
2021-08-02 12:09:23,633 n: module.bert.embeddings.LayerNorm.bias
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-02 12:09:23,633 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-02 12:09:23,634 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-02 12:09:23,635 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-02 12:09:23,636 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-02 12:09:23,637 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-02 12:09:23,638 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-02 12:09:23,638 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-02 12:09:23,638 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-02 12:09:23,638 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-02 12:09:23,638 n: module.bert.pooler.dense.weight
2021-08-02 12:09:23,638 n: module.bert.pooler.dense.bias
2021-08-02 12:09:23,638 n: module.classifier.weight
2021-08-02 12:09:23,638 n: module.classifier.bias
2021-08-02 12:09:23,638 n: module.fit_dense.weight
2021-08-02 12:09:23,638 n: module.fit_dense.bias
2021-08-02 12:09:23,638 Total parameters: 67547907
2021-08-02 12:17:40,118 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/KD_outputs', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-02 12:17:40,178 device: cuda n_gpu: 2
2021-08-02 12:17:47,794 Writing example 0 of 505555
2021-08-02 12:17:47,795 *** Example ***
2021-08-02 12:17:47,795 guid: aug-0
2021-08-02 12:17:47,795 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-02 12:17:47,795 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:17:47,795 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:17:47,795 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:17:47,796 label: neutral
2021-08-02 12:17:47,796 label_id: 2
2021-08-02 12:17:52,486 Writing example 10000 of 505555
2021-08-02 12:17:57,217 Writing example 20000 of 505555
2021-08-02 12:18:01,723 Writing example 30000 of 505555
2021-08-02 12:18:06,531 Writing example 40000 of 505555
2021-08-02 12:18:11,314 Writing example 50000 of 505555
2021-08-02 12:18:15,832 Writing example 60000 of 505555
2021-08-02 12:18:20,472 Writing example 70000 of 505555
2021-08-02 12:18:25,029 Writing example 80000 of 505555
2021-08-02 12:18:29,971 Writing example 90000 of 505555
2021-08-02 12:18:34,628 Writing example 100000 of 505555
2021-08-02 12:18:39,207 Writing example 110000 of 505555
2021-08-02 12:18:43,563 Writing example 120000 of 505555
2021-08-02 12:18:48,415 Writing example 130000 of 505555
2021-08-02 12:18:53,749 Writing example 140000 of 505555
2021-08-02 12:18:58,189 Writing example 150000 of 505555
2021-08-02 12:19:02,960 Writing example 160000 of 505555
2021-08-02 12:19:07,640 Writing example 170000 of 505555
2021-08-02 12:19:12,251 Writing example 180000 of 505555
2021-08-02 12:19:16,881 Writing example 190000 of 505555
2021-08-02 12:19:21,444 Writing example 200000 of 505555
2021-08-02 12:19:26,846 Writing example 210000 of 505555
2021-08-02 12:19:31,304 Writing example 220000 of 505555
2021-08-02 12:19:36,031 Writing example 230000 of 505555
2021-08-02 12:19:40,671 Writing example 240000 of 505555
2021-08-02 12:19:45,293 Writing example 250000 of 505555
2021-08-02 12:19:49,879 Writing example 260000 of 505555
2021-08-02 12:19:54,366 Writing example 270000 of 505555
2021-08-02 12:19:58,819 Writing example 280000 of 505555
2021-08-02 12:20:04,681 Writing example 290000 of 505555
2021-08-02 12:20:09,362 Writing example 300000 of 505555
2021-08-02 12:20:13,888 Writing example 310000 of 505555
2021-08-02 12:20:18,312 Writing example 320000 of 505555
2021-08-02 12:20:22,985 Writing example 330000 of 505555
2021-08-02 12:20:27,522 Writing example 340000 of 505555
2021-08-02 12:20:32,282 Writing example 350000 of 505555
2021-08-02 12:20:36,891 Writing example 360000 of 505555
2021-08-02 12:20:41,274 Writing example 370000 of 505555
2021-08-02 12:20:45,838 Writing example 380000 of 505555
2021-08-02 12:20:51,800 Writing example 390000 of 505555
2021-08-02 12:20:56,440 Writing example 400000 of 505555
2021-08-02 12:21:00,973 Writing example 410000 of 505555
2021-08-02 12:21:05,514 Writing example 420000 of 505555
2021-08-02 12:21:09,906 Writing example 430000 of 505555
2021-08-02 12:21:14,320 Writing example 440000 of 505555
2021-08-02 12:21:19,052 Writing example 450000 of 505555
2021-08-02 12:21:23,650 Writing example 460000 of 505555
2021-08-02 12:21:28,222 Writing example 470000 of 505555
2021-08-02 12:21:32,860 Writing example 480000 of 505555
2021-08-02 12:21:37,547 Writing example 490000 of 505555
2021-08-02 12:21:42,105 Writing example 500000 of 505555
2021-08-02 12:21:49,178 Writing example 0 of 9815
2021-08-02 12:21:49,178 *** Example ***
2021-08-02 12:21:49,179 guid: dev_matched-0
2021-08-02 12:21:49,179 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-02 12:21:49,179 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:21:49,179 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:21:49,179 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-02 12:21:49,179 label: neutral
2021-08-02 12:21:49,179 label_id: 2
2021-08-02 12:21:53,452 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-02 12:21:55,815 Loading model /home/mcao610/scratch/huggingface/MNLI/pytorch_model.bin
2021-08-02 12:21:56,037 loading model...
2021-08-02 12:21:56,065 done!
2021-08-02 12:21:56,065 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 12:21:56,065 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-02 12:21:58,611 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-02 12:22:00,099 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-02 12:22:00,223 loading model...
2021-08-02 12:22:00,236 done!
2021-08-02 12:22:00,236 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-02 12:22:00,236 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-02 12:22:00,302 ***** Running training *****
2021-08-02 12:22:00,302   Num examples = 505555
2021-08-02 12:22:00,302   Batch size = 32
2021-08-02 12:22:00,302   Num steps = 47394
2021-08-02 12:22:00,303 n: module.bert.embeddings.word_embeddings.weight
2021-08-02 12:22:00,303 n: module.bert.embeddings.position_embeddings.weight
2021-08-02 12:22:00,303 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-02 12:22:00,304 n: module.bert.embeddings.LayerNorm.weight
2021-08-02 12:22:00,304 n: module.bert.embeddings.LayerNorm.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-02 12:22:00,304 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-02 12:22:00,305 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-02 12:22:00,306 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-02 12:22:00,307 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-02 12:22:00,308 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-02 12:22:00,308 n: module.bert.pooler.dense.weight
2021-08-02 12:22:00,308 n: module.bert.pooler.dense.bias
2021-08-02 12:22:00,309 n: module.classifier.weight
2021-08-02 12:22:00,309 n: module.classifier.bias
2021-08-02 12:22:00,309 n: module.fit_dense.weight
2021-08-02 12:22:00,309 n: module.fit_dense.bias
2021-08-02 12:22:00,309 Total parameters: 67547907
2021-08-02 16:25:17,558 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-02 16:25:17,634 device: cuda n_gpu: 2
2021-08-08 13:07:53,994 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 13:07:54,131 device: cuda n_gpu: 4
2021-08-08 13:07:58,637 Writing example 0 of 1119178
2021-08-08 13:07:58,638 *** Example ***
2021-08-08 13:07:58,638 guid: aug-1
2021-08-08 13:07:58,638 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 13:07:58,638 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:07:58,638 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:07:58,638 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:07:58,638 label: 0
2021-08-08 13:07:58,638 label_id: 0
2021-08-08 13:08:00,226 Writing example 10000 of 1119178
2021-08-08 13:08:01,919 Writing example 20000 of 1119178
2021-08-08 13:08:03,522 Writing example 30000 of 1119178
2021-08-08 13:08:05,143 Writing example 40000 of 1119178
2021-08-08 13:08:07,100 Writing example 50000 of 1119178
2021-08-08 13:08:08,733 Writing example 60000 of 1119178
2021-08-08 13:08:10,326 Writing example 70000 of 1119178
2021-08-08 13:08:12,006 Writing example 80000 of 1119178
2021-08-08 13:08:13,700 Writing example 90000 of 1119178
2021-08-08 13:08:15,304 Writing example 100000 of 1119178
2021-08-08 13:08:16,891 Writing example 110000 of 1119178
2021-08-08 13:08:18,466 Writing example 120000 of 1119178
2021-08-08 13:08:20,706 Writing example 130000 of 1119178
2021-08-08 13:08:22,385 Writing example 140000 of 1119178
2021-08-08 13:08:24,058 Writing example 150000 of 1119178
2021-08-08 13:08:25,743 Writing example 160000 of 1119178
2021-08-08 13:08:27,532 Writing example 170000 of 1119178
2021-08-08 13:08:29,089 Writing example 180000 of 1119178
2021-08-08 13:08:30,688 Writing example 190000 of 1119178
2021-08-08 13:08:32,328 Writing example 200000 of 1119178
2021-08-08 13:08:34,013 Writing example 210000 of 1119178
2021-08-08 13:08:36,594 Writing example 220000 of 1119178
2021-08-08 13:08:38,334 Writing example 230000 of 1119178
2021-08-08 13:08:39,996 Writing example 240000 of 1119178
2021-08-08 13:08:41,692 Writing example 250000 of 1119178
2021-08-08 13:08:43,483 Writing example 260000 of 1119178
2021-08-08 13:08:45,142 Writing example 270000 of 1119178
2021-08-08 13:08:46,892 Writing example 280000 of 1119178
2021-08-08 13:08:48,520 Writing example 290000 of 1119178
2021-08-08 13:08:50,070 Writing example 300000 of 1119178
2021-08-08 13:08:51,711 Writing example 310000 of 1119178
2021-08-08 13:08:53,348 Writing example 320000 of 1119178
2021-08-08 13:08:56,165 Writing example 330000 of 1119178
2021-08-08 13:08:57,685 Writing example 340000 of 1119178
2021-08-08 13:08:59,314 Writing example 350000 of 1119178
2021-08-08 13:09:00,925 Writing example 360000 of 1119178
2021-08-08 13:09:02,599 Writing example 370000 of 1119178
2021-08-08 13:09:04,268 Writing example 380000 of 1119178
2021-08-08 13:09:05,841 Writing example 390000 of 1119178
2021-08-08 13:09:07,412 Writing example 400000 of 1119178
2021-08-08 13:09:08,992 Writing example 410000 of 1119178
2021-08-08 13:09:10,564 Writing example 420000 of 1119178
2021-08-08 13:09:12,221 Writing example 430000 of 1119178
2021-08-08 13:09:13,971 Writing example 440000 of 1119178
2021-08-08 13:09:15,798 Writing example 450000 of 1119178
2021-08-08 13:09:17,566 Writing example 460000 of 1119178
2021-08-08 13:09:19,265 Writing example 470000 of 1119178
2021-08-08 13:09:22,500 Writing example 480000 of 1119178
2021-08-08 13:09:24,052 Writing example 490000 of 1119178
2021-08-08 13:09:25,669 Writing example 500000 of 1119178
2021-08-08 13:09:27,296 Writing example 510000 of 1119178
2021-08-08 13:09:28,996 Writing example 520000 of 1119178
2021-08-08 13:09:30,682 Writing example 530000 of 1119178
2021-08-08 13:09:32,322 Writing example 540000 of 1119178
2021-08-08 13:09:34,055 Writing example 550000 of 1119178
2021-08-08 13:09:35,740 Writing example 560000 of 1119178
2021-08-08 13:09:37,429 Writing example 570000 of 1119178
2021-08-08 13:09:39,170 Writing example 580000 of 1119178
2021-08-08 13:09:40,760 Writing example 590000 of 1119178
2021-08-08 13:09:42,458 Writing example 600000 of 1119178
2021-08-08 13:09:44,157 Writing example 610000 of 1119178
2021-08-08 13:09:45,820 Writing example 620000 of 1119178
2021-08-08 13:09:47,396 Writing example 630000 of 1119178
2021-08-08 13:09:49,110 Writing example 640000 of 1119178
2021-08-08 13:09:50,748 Writing example 650000 of 1119178
2021-08-08 13:09:54,410 Writing example 660000 of 1119178
2021-08-08 13:09:56,071 Writing example 670000 of 1119178
2021-08-08 13:09:57,742 Writing example 680000 of 1119178
2021-08-08 13:09:59,265 Writing example 690000 of 1119178
2021-08-08 13:10:00,926 Writing example 700000 of 1119178
2021-08-08 13:10:02,597 Writing example 710000 of 1119178
2021-08-08 13:10:04,223 Writing example 720000 of 1119178
2021-08-08 13:10:05,876 Writing example 730000 of 1119178
2021-08-08 13:10:07,529 Writing example 740000 of 1119178
2021-08-08 13:10:09,197 Writing example 750000 of 1119178
2021-08-08 13:10:10,839 Writing example 760000 of 1119178
2021-08-08 13:10:12,401 Writing example 770000 of 1119178
2021-08-08 13:10:14,086 Writing example 780000 of 1119178
2021-08-08 13:10:15,708 Writing example 790000 of 1119178
2021-08-08 13:10:17,320 Writing example 800000 of 1119178
2021-08-08 13:10:19,038 Writing example 810000 of 1119178
2021-08-08 13:10:20,688 Writing example 820000 of 1119178
2021-08-08 13:10:22,435 Writing example 830000 of 1119178
2021-08-08 13:10:24,026 Writing example 840000 of 1119178
2021-08-08 13:10:25,676 Writing example 850000 of 1119178
2021-08-08 13:10:27,402 Writing example 860000 of 1119178
2021-08-08 13:10:29,073 Writing example 870000 of 1119178
2021-08-08 13:10:33,540 Writing example 880000 of 1119178
2021-08-08 13:10:35,128 Writing example 890000 of 1119178
2021-08-08 13:10:36,743 Writing example 900000 of 1119178
2021-08-08 13:10:38,386 Writing example 910000 of 1119178
2021-08-08 13:10:40,070 Writing example 920000 of 1119178
2021-08-08 13:10:41,691 Writing example 930000 of 1119178
2021-08-08 13:10:43,412 Writing example 940000 of 1119178
2021-08-08 13:10:45,093 Writing example 950000 of 1119178
2021-08-08 13:10:46,779 Writing example 960000 of 1119178
2021-08-08 13:10:48,441 Writing example 970000 of 1119178
2021-08-08 13:10:50,041 Writing example 980000 of 1119178
2021-08-08 13:10:51,656 Writing example 990000 of 1119178
2021-08-08 13:10:53,251 Writing example 1000000 of 1119178
2021-08-08 13:10:54,904 Writing example 1010000 of 1119178
2021-08-08 13:10:56,510 Writing example 1020000 of 1119178
2021-08-08 13:10:58,127 Writing example 1030000 of 1119178
2021-08-08 13:10:59,794 Writing example 1040000 of 1119178
2021-08-08 13:11:01,427 Writing example 1050000 of 1119178
2021-08-08 13:11:03,052 Writing example 1060000 of 1119178
2021-08-08 13:11:04,685 Writing example 1070000 of 1119178
2021-08-08 13:11:06,322 Writing example 1080000 of 1119178
2021-08-08 13:11:07,940 Writing example 1090000 of 1119178
2021-08-08 13:11:09,513 Writing example 1100000 of 1119178
2021-08-08 13:11:11,137 Writing example 1110000 of 1119178
2021-08-08 13:11:19,130 Writing example 0 of 872
2021-08-08 13:11:19,130 *** Example ***
2021-08-08 13:11:19,130 guid: dev-1
2021-08-08 13:11:19,130 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 13:11:19,130 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:11:19,130 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:11:19,130 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 13:11:19,130 label: 1
2021-08-08 13:11:19,130 label_id: 1
2021-08-08 13:11:19,514 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 13:11:21,968 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 13:11:43,302 loading model...
2021-08-08 13:11:43,376 done!
2021-08-08 13:11:43,376 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 13:11:43,376 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 13:11:51,283 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 13:11:52,754 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 13:11:56,559 loading model...
2021-08-08 13:11:56,572 done!
2021-08-08 13:11:56,638 ***** Running training *****
2021-08-08 13:11:56,638   Num examples = 1119178
2021-08-08 13:11:56,638   Batch size = 32
2021-08-08 13:11:56,638   Num steps = 104922
2021-08-08 13:11:56,639 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 13:11:56,639 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 13:11:56,639 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 13:11:56,640 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 13:11:56,640 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 13:11:56,640 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 13:11:56,641 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 13:11:56,642 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 13:11:56,643 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 13:11:56,644 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 13:11:56,645 n: module.bert.pooler.dense.weight
2021-08-08 13:11:56,645 n: module.bert.pooler.dense.bias
2021-08-08 13:11:56,645 n: module.classifier.weight
2021-08-08 13:11:56,645 n: module.classifier.bias
2021-08-08 13:11:56,645 n: module.fit_dense.weight
2021-08-08 13:11:56,645 n: module.fit_dense.bias
2021-08-08 13:11:56,645 Total parameters: 67547138
2021-08-08 14:14:57,873 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:14:58,004 device: cuda n_gpu: 4
2021-08-08 14:15:02,285 Writing example 0 of 1119178
2021-08-08 14:15:02,286 *** Example ***
2021-08-08 14:15:02,286 guid: aug-1
2021-08-08 14:15:02,286 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:15:02,286 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:15:02,286 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:15:02,286 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:15:02,286 label: 0
2021-08-08 14:15:02,286 label_id: 0
2021-08-08 14:15:03,853 Writing example 10000 of 1119178
2021-08-08 14:15:05,551 Writing example 20000 of 1119178
2021-08-08 14:15:07,179 Writing example 30000 of 1119178
2021-08-08 14:15:08,783 Writing example 40000 of 1119178
2021-08-08 14:15:10,730 Writing example 50000 of 1119178
2021-08-08 14:15:12,356 Writing example 60000 of 1119178
2021-08-08 14:15:13,982 Writing example 70000 of 1119178
2021-08-08 14:15:15,641 Writing example 80000 of 1119178
2021-08-08 14:15:17,320 Writing example 90000 of 1119178
2021-08-08 14:15:18,906 Writing example 100000 of 1119178
2021-08-08 14:15:20,507 Writing example 110000 of 1119178
2021-08-08 14:15:22,066 Writing example 120000 of 1119178
2021-08-08 14:15:24,282 Writing example 130000 of 1119178
2021-08-08 14:15:25,940 Writing example 140000 of 1119178
2021-08-08 14:15:27,590 Writing example 150000 of 1119178
2021-08-08 14:15:29,254 Writing example 160000 of 1119178
2021-08-08 14:15:31,078 Writing example 170000 of 1119178
2021-08-08 14:15:32,700 Writing example 180000 of 1119178
2021-08-08 14:15:34,309 Writing example 190000 of 1119178
2021-08-08 14:15:35,932 Writing example 200000 of 1119178
2021-08-08 14:15:37,583 Writing example 210000 of 1119178
2021-08-08 14:15:40,020 Writing example 220000 of 1119178
2021-08-08 14:15:41,744 Writing example 230000 of 1119178
2021-08-08 14:15:43,424 Writing example 240000 of 1119178
2021-08-08 14:15:45,098 Writing example 250000 of 1119178
2021-08-08 14:15:46,895 Writing example 260000 of 1119178
2021-08-08 14:15:48,605 Writing example 270000 of 1119178
2021-08-08 14:15:50,339 Writing example 280000 of 1119178
2021-08-08 14:15:51,895 Writing example 290000 of 1119178
2021-08-08 14:15:53,394 Writing example 300000 of 1119178
2021-08-08 14:15:55,013 Writing example 310000 of 1119178
2021-08-08 14:15:56,728 Writing example 320000 of 1119178
2021-08-08 14:15:59,611 Writing example 330000 of 1119178
2021-08-08 14:16:01,149 Writing example 340000 of 1119178
2021-08-08 14:16:02,752 Writing example 350000 of 1119178
2021-08-08 14:16:04,342 Writing example 360000 of 1119178
2021-08-08 14:16:06,005 Writing example 370000 of 1119178
2021-08-08 14:16:07,693 Writing example 380000 of 1119178
2021-08-08 14:16:09,254 Writing example 390000 of 1119178
2021-08-08 14:16:10,807 Writing example 400000 of 1119178
2021-08-08 14:16:12,383 Writing example 410000 of 1119178
2021-08-08 14:16:13,961 Writing example 420000 of 1119178
2021-08-08 14:16:15,598 Writing example 430000 of 1119178
2021-08-08 14:16:17,147 Writing example 440000 of 1119178
2021-08-08 14:16:18,802 Writing example 450000 of 1119178
2021-08-08 14:16:20,453 Writing example 460000 of 1119178
2021-08-08 14:16:22,152 Writing example 470000 of 1119178
2021-08-08 14:16:25,331 Writing example 480000 of 1119178
2021-08-08 14:16:26,868 Writing example 490000 of 1119178
2021-08-08 14:16:28,479 Writing example 500000 of 1119178
2021-08-08 14:16:30,085 Writing example 510000 of 1119178
2021-08-08 14:16:31,759 Writing example 520000 of 1119178
2021-08-08 14:16:33,429 Writing example 530000 of 1119178
2021-08-08 14:16:35,048 Writing example 540000 of 1119178
2021-08-08 14:16:36,747 Writing example 550000 of 1119178
2021-08-08 14:16:38,411 Writing example 560000 of 1119178
2021-08-08 14:16:40,082 Writing example 570000 of 1119178
2021-08-08 14:16:41,780 Writing example 580000 of 1119178
2021-08-08 14:16:43,406 Writing example 590000 of 1119178
2021-08-08 14:16:45,078 Writing example 600000 of 1119178
2021-08-08 14:16:46,776 Writing example 610000 of 1119178
2021-08-08 14:16:48,436 Writing example 620000 of 1119178
2021-08-08 14:16:49,990 Writing example 630000 of 1119178
2021-08-08 14:16:51,679 Writing example 640000 of 1119178
2021-08-08 14:16:53,295 Writing example 650000 of 1119178
2021-08-08 14:16:56,888 Writing example 660000 of 1119178
2021-08-08 14:16:58,554 Writing example 670000 of 1119178
2021-08-08 14:17:00,204 Writing example 680000 of 1119178
2021-08-08 14:17:01,696 Writing example 690000 of 1119178
2021-08-08 14:17:03,338 Writing example 700000 of 1119178
2021-08-08 14:17:04,983 Writing example 710000 of 1119178
2021-08-08 14:17:06,589 Writing example 720000 of 1119178
2021-08-08 14:17:08,209 Writing example 730000 of 1119178
2021-08-08 14:17:09,812 Writing example 740000 of 1119178
2021-08-08 14:17:11,532 Writing example 750000 of 1119178
2021-08-08 14:17:13,255 Writing example 760000 of 1119178
2021-08-08 14:17:14,798 Writing example 770000 of 1119178
2021-08-08 14:17:16,498 Writing example 780000 of 1119178
2021-08-08 14:17:18,135 Writing example 790000 of 1119178
2021-08-08 14:17:19,726 Writing example 800000 of 1119178
2021-08-08 14:17:21,467 Writing example 810000 of 1119178
2021-08-08 14:17:23,178 Writing example 820000 of 1119178
2021-08-08 14:17:25,002 Writing example 830000 of 1119178
2021-08-08 14:17:26,573 Writing example 840000 of 1119178
2021-08-08 14:17:28,202 Writing example 850000 of 1119178
2021-08-08 14:17:29,924 Writing example 860000 of 1119178
2021-08-08 14:17:31,556 Writing example 870000 of 1119178
2021-08-08 14:17:36,189 Writing example 880000 of 1119178
2021-08-08 14:17:37,772 Writing example 890000 of 1119178
2021-08-08 14:17:39,364 Writing example 900000 of 1119178
2021-08-08 14:17:40,987 Writing example 910000 of 1119178
2021-08-08 14:17:42,652 Writing example 920000 of 1119178
2021-08-08 14:17:44,268 Writing example 930000 of 1119178
2021-08-08 14:17:45,857 Writing example 940000 of 1119178
2021-08-08 14:17:47,521 Writing example 950000 of 1119178
2021-08-08 14:17:49,303 Writing example 960000 of 1119178
2021-08-08 14:17:50,947 Writing example 970000 of 1119178
2021-08-08 14:17:52,526 Writing example 980000 of 1119178
2021-08-08 14:17:54,122 Writing example 990000 of 1119178
2021-08-08 14:17:55,697 Writing example 1000000 of 1119178
2021-08-08 14:17:57,321 Writing example 1010000 of 1119178
2021-08-08 14:17:58,931 Writing example 1020000 of 1119178
2021-08-08 14:18:00,528 Writing example 1030000 of 1119178
2021-08-08 14:18:02,155 Writing example 1040000 of 1119178
2021-08-08 14:18:03,768 Writing example 1050000 of 1119178
2021-08-08 14:18:05,374 Writing example 1060000 of 1119178
2021-08-08 14:18:07,088 Writing example 1070000 of 1119178
2021-08-08 14:18:08,714 Writing example 1080000 of 1119178
2021-08-08 14:18:10,312 Writing example 1090000 of 1119178
2021-08-08 14:18:11,960 Writing example 1100000 of 1119178
2021-08-08 14:18:13,666 Writing example 1110000 of 1119178
2021-08-08 14:18:21,515 Writing example 0 of 872
2021-08-08 14:18:21,515 *** Example ***
2021-08-08 14:18:21,516 guid: dev-1
2021-08-08 14:18:21,516 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:18:21,516 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:18:21,516 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:18:21,516 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:18:21,516 label: 1
2021-08-08 14:18:21,516 label_id: 1
2021-08-08 14:18:23,498 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:18:25,884 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 14:18:39,031 loading model...
2021-08-08 14:18:39,067 done!
2021-08-08 14:18:39,067 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:18:39,067 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 14:18:43,315 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:18:44,807 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:18:47,956 loading model...
2021-08-08 14:18:47,969 done!
2021-08-08 14:18:48,035 ***** Running training *****
2021-08-08 14:18:48,035   Num examples = 1119178
2021-08-08 14:18:48,035   Batch size = 32
2021-08-08 14:18:48,035   Num steps = 104922
2021-08-08 14:18:48,036 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:18:48,036 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:18:48,036 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:18:48,036 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:18:48,036 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:18:48,036 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:18:48,036 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:18:48,036 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:18:48,037 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:18:48,038 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:18:48,039 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:18:48,040 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:18:48,041 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:18:48,041 n: module.bert.pooler.dense.weight
2021-08-08 14:18:48,041 n: module.bert.pooler.dense.bias
2021-08-08 14:18:48,041 n: module.classifier.weight
2021-08-08 14:18:48,041 n: module.classifier.bias
2021-08-08 14:18:48,041 n: module.fit_dense.weight
2021-08-08 14:18:48,041 n: module.fit_dense.bias
2021-08-08 14:18:48,041 Total parameters: 67547138
2021-08-08 14:26:48,329 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:26:48,459 device: cuda n_gpu: 4
2021-08-08 14:26:48,490 Writing example 0 of 99
2021-08-08 14:26:48,490 *** Example ***
2021-08-08 14:26:48,490 guid: aug-1
2021-08-08 14:26:48,491 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:26:48,491 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,491 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,491 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,491 label: 0
2021-08-08 14:26:48,491 label_id: 0
2021-08-08 14:26:48,511 Writing example 0 of 872
2021-08-08 14:26:48,511 *** Example ***
2021-08-08 14:26:48,511 guid: dev-1
2021-08-08 14:26:48,511 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:26:48,512 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,512 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,512 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:26:48,512 label: 1
2021-08-08 14:26:48,512 label_id: 1
2021-08-08 14:26:48,753 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:26:51,127 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 14:26:51,359 loading model...
2021-08-08 14:26:51,383 done!
2021-08-08 14:26:51,383 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:26:51,383 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 14:26:54,229 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:26:55,719 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:26:55,838 loading model...
2021-08-08 14:26:55,852 done!
2021-08-08 14:26:55,919 ***** Running training *****
2021-08-08 14:26:55,919   Num examples = 99
2021-08-08 14:26:55,919   Batch size = 32
2021-08-08 14:26:55,919   Num steps = 9
2021-08-08 14:26:55,920 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:26:55,920 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:26:55,920 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:26:55,920 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:26:55,920 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:26:55,920 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:26:55,920 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:26:55,920 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:26:55,921 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:26:55,922 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:26:55,923 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:26:55,924 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:26:55,925 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:26:55,925 n: module.bert.pooler.dense.weight
2021-08-08 14:26:55,925 n: module.bert.pooler.dense.bias
2021-08-08 14:26:55,925 n: module.classifier.weight
2021-08-08 14:26:55,925 n: module.classifier.bias
2021-08-08 14:26:55,925 n: module.fit_dense.weight
2021-08-08 14:26:55,925 n: module.fit_dense.bias
2021-08-08 14:26:55,925 Total parameters: 67547138
2021-08-08 14:28:05,889 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:28:06,019 device: cuda n_gpu: 4
2021-08-08 14:28:06,050 Writing example 0 of 99
2021-08-08 14:28:06,050 *** Example ***
2021-08-08 14:28:06,050 guid: aug-1
2021-08-08 14:28:06,051 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:28:06,051 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,051 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,051 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,051 label: 0
2021-08-08 14:28:06,051 label_id: 0
2021-08-08 14:28:06,072 Writing example 0 of 872
2021-08-08 14:28:06,072 *** Example ***
2021-08-08 14:28:06,072 guid: dev-1
2021-08-08 14:28:06,072 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:28:06,072 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,072 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,073 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:28:06,073 label: 1
2021-08-08 14:28:06,073 label_id: 1
2021-08-08 14:28:06,317 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:28:08,710 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 14:28:08,937 loading model...
2021-08-08 14:28:08,961 done!
2021-08-08 14:28:08,961 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:28:08,961 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 14:28:11,813 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:28:13,324 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:28:13,451 loading model...
2021-08-08 14:28:13,464 done!
2021-08-08 14:28:13,530 ***** Running training *****
2021-08-08 14:28:13,531   Num examples = 99
2021-08-08 14:28:13,531   Batch size = 32
2021-08-08 14:28:13,531   Num steps = 9
2021-08-08 14:28:13,532 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:28:13,532 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:28:13,532 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:28:13,532 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:28:13,532 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:28:13,532 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:28:13,533 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:28:13,534 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:28:13,535 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:28:13,536 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:28:13,537 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:28:13,537 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:28:13,537 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:28:13,537 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:28:13,537 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:28:13,537 n: module.bert.pooler.dense.weight
2021-08-08 14:28:13,537 n: module.bert.pooler.dense.bias
2021-08-08 14:28:13,537 n: module.classifier.weight
2021-08-08 14:28:13,537 n: module.classifier.bias
2021-08-08 14:28:13,537 n: module.fit_dense.weight
2021-08-08 14:28:13,537 n: module.fit_dense.bias
2021-08-08 14:28:13,537 Total parameters: 67547138
2021-08-08 14:29:32,180 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:29:32,308 device: cuda n_gpu: 4
2021-08-08 14:29:32,340 Writing example 0 of 99
2021-08-08 14:29:32,340 *** Example ***
2021-08-08 14:29:32,340 guid: aug-1
2021-08-08 14:29:32,340 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:29:32,340 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,341 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,341 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,341 label: 0
2021-08-08 14:29:32,341 label_id: 0
2021-08-08 14:29:32,361 Writing example 0 of 872
2021-08-08 14:29:32,362 *** Example ***
2021-08-08 14:29:32,362 guid: dev-1
2021-08-08 14:29:32,362 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:29:32,362 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,362 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,362 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:29:32,362 label: 1
2021-08-08 14:29:32,362 label_id: 1
2021-08-08 14:29:32,604 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:29:34,972 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 14:29:35,196 loading model...
2021-08-08 14:29:35,229 done!
2021-08-08 14:29:35,229 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:29:35,229 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 14:29:38,147 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:29:39,636 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:29:39,750 loading model...
2021-08-08 14:29:39,762 done!
2021-08-08 14:29:39,829 ***** Running training *****
2021-08-08 14:29:39,830   Num examples = 99
2021-08-08 14:29:39,830   Batch size = 32
2021-08-08 14:29:39,830   Num steps = 9
2021-08-08 14:29:39,831 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:29:39,831 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:29:39,831 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:29:39,831 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:29:39,831 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:29:39,831 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:29:39,832 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:29:39,833 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:29:39,834 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:29:39,835 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:29:39,836 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:29:39,836 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:29:39,836 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:29:39,836 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:29:39,836 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:29:39,836 n: module.bert.pooler.dense.weight
2021-08-08 14:29:39,836 n: module.bert.pooler.dense.bias
2021-08-08 14:29:39,836 n: module.classifier.weight
2021-08-08 14:29:39,836 n: module.classifier.bias
2021-08-08 14:29:39,836 n: module.fit_dense.weight
2021-08-08 14:29:39,836 n: module.fit_dense.bias
2021-08-08 14:29:39,836 Total parameters: 67547138
2021-08-08 14:30:12,496 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/SST-2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:30:12,624 device: cuda n_gpu: 4
2021-08-08 14:30:12,655 Writing example 0 of 99
2021-08-08 14:30:12,656 *** Example ***
2021-08-08 14:30:12,656 guid: aug-1
2021-08-08 14:30:12,656 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:30:12,656 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,656 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,656 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,656 label: 0
2021-08-08 14:30:12,656 label_id: 0
2021-08-08 14:30:12,677 Writing example 0 of 872
2021-08-08 14:30:12,677 *** Example ***
2021-08-08 14:30:12,677 guid: dev-1
2021-08-08 14:30:12,677 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:30:12,677 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,677 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,678 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:30:12,678 label: 1
2021-08-08 14:30:12,678 label_id: 1
2021-08-08 14:30:12,918 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:30:15,289 Loading model /home/mcao610/scratch/huggingface/SST-2/pytorch_model.bin
2021-08-08 14:30:15,511 loading model...
2021-08-08 14:30:15,535 done!
2021-08-08 14:30:15,535 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:30:15,535 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 14:30:18,392 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:30:19,882 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:30:19,995 loading model...
2021-08-08 14:30:20,008 done!
2021-08-08 14:30:20,075 ***** Running training *****
2021-08-08 14:30:20,075   Num examples = 99
2021-08-08 14:30:20,075   Batch size = 32
2021-08-08 14:30:20,076   Num steps = 9
2021-08-08 14:30:20,077 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:30:20,077 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:30:20,077 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:30:20,077 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:30:20,077 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:30:20,077 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:30:20,078 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:30:20,079 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:30:20,080 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:30:20,081 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:30:20,082 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:30:20,082 n: module.bert.pooler.dense.weight
2021-08-08 14:30:20,082 n: module.bert.pooler.dense.bias
2021-08-08 14:30:20,082 n: module.classifier.weight
2021-08-08 14:30:20,082 n: module.classifier.bias
2021-08-08 14:30:20,082 n: module.fit_dense.weight
2021-08-08 14:30:20,082 n: module.fit_dense.bias
2021-08-08 14:30:20,082 Total parameters: 67547138
2021-08-08 14:37:48,848 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-cased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:37:48,976 device: cuda n_gpu: 4
2021-08-08 14:37:49,006 Writing example 0 of 99
2021-08-08 14:37:49,006 *** Example ***
2021-08-08 14:37:49,006 guid: aug-1
2021-08-08 14:37:49,006 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:37:49,006 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,006 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,007 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,007 label: 0
2021-08-08 14:37:49,007 label_id: 0
2021-08-08 14:37:49,028 Writing example 0 of 872
2021-08-08 14:37:49,028 *** Example ***
2021-08-08 14:37:49,028 guid: dev-1
2021-08-08 14:37:49,028 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:37:49,028 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,028 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,028 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:37:49,028 label: 1
2021-08-08 14:37:49,028 label_id: 1
2021-08-08 14:37:49,514 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:37:51,945 Loading model /home/mcao610/scratch/huggingface/bert-base-cased/pytorch_model.bin
2021-08-08 14:37:56,255 loading model...
2021-08-08 14:37:56,645 done!
2021-08-08 14:37:56,645 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:37:56,645 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:37:59,478 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:38:00,964 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:38:01,077 loading model...
2021-08-08 14:38:01,090 done!
2021-08-08 14:38:01,156 ***** Running training *****
2021-08-08 14:38:01,157   Num examples = 99
2021-08-08 14:38:01,157   Batch size = 32
2021-08-08 14:38:01,157   Num steps = 9
2021-08-08 14:38:01,158 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:38:01,158 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:38:01,158 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:38:01,158 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:38:01,158 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:38:01,158 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:38:01,159 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:38:01,160 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:38:01,161 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:38:01,162 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:38:01,163 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:38:01,163 n: module.bert.pooler.dense.weight
2021-08-08 14:38:01,163 n: module.bert.pooler.dense.bias
2021-08-08 14:38:01,163 n: module.classifier.weight
2021-08-08 14:38:01,163 n: module.classifier.bias
2021-08-08 14:38:01,163 n: module.fit_dense.weight
2021-08-08 14:38:01,163 n: module.fit_dense.bias
2021-08-08 14:38:01,163 Total parameters: 67547138
2021-08-08 14:43:23,730 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-cased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:43:23,857 device: cuda n_gpu: 4
2021-08-08 14:43:23,888 Writing example 0 of 99
2021-08-08 14:43:23,888 *** Example ***
2021-08-08 14:43:23,888 guid: aug-1
2021-08-08 14:43:23,888 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:43:23,889 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,889 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,889 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,889 label: 0
2021-08-08 14:43:23,889 label_id: 0
2021-08-08 14:43:23,910 Writing example 0 of 872
2021-08-08 14:43:23,910 *** Example ***
2021-08-08 14:43:23,910 guid: dev-1
2021-08-08 14:43:23,910 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:43:23,910 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,910 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,910 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:43:23,910 label: 1
2021-08-08 14:43:23,910 label_id: 1
2021-08-08 14:43:24,151 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:43:26,527 Loading model /home/mcao610/scratch/huggingface/bert-base-cased/pytorch_model.bin
2021-08-08 14:43:26,720 loading model...
2021-08-08 14:43:27,084 done!
2021-08-08 14:43:27,085 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:43:27,085 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:43:29,905 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:43:31,397 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:43:31,511 loading model...
2021-08-08 14:43:31,524 done!
2021-08-08 14:43:31,590 ***** Running training *****
2021-08-08 14:43:31,591   Num examples = 99
2021-08-08 14:43:31,591   Batch size = 32
2021-08-08 14:43:31,591   Num steps = 9
2021-08-08 14:43:31,592 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:43:31,592 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:43:31,592 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:43:31,592 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:43:31,592 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:43:31,592 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:43:31,593 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:43:31,594 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:43:31,595 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:43:31,596 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:43:31,597 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:43:31,597 n: module.bert.pooler.dense.weight
2021-08-08 14:43:31,597 n: module.bert.pooler.dense.bias
2021-08-08 14:43:31,597 n: module.classifier.weight
2021-08-08 14:43:31,597 n: module.classifier.bias
2021-08-08 14:43:31,597 n: module.fit_dense.weight
2021-08-08 14:43:31,597 n: module.fit_dense.bias
2021-08-08 14:43:31,597 Total parameters: 67547138
2021-08-08 14:45:44,254 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-cased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:45:44,383 device: cuda n_gpu: 4
2021-08-08 14:45:44,413 Writing example 0 of 99
2021-08-08 14:45:44,414 *** Example ***
2021-08-08 14:45:44,414 guid: aug-1
2021-08-08 14:45:44,414 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:45:44,414 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,414 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,414 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,414 label: 0
2021-08-08 14:45:44,414 label_id: 0
2021-08-08 14:45:44,436 Writing example 0 of 872
2021-08-08 14:45:44,436 *** Example ***
2021-08-08 14:45:44,436 guid: dev-1
2021-08-08 14:45:44,436 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:45:44,436 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,436 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,436 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:45:44,436 label: 1
2021-08-08 14:45:44,436 label_id: 1
2021-08-08 14:45:44,681 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:45:47,086 Loading model /home/mcao610/scratch/huggingface/bert-base-cased/pytorch_model.bin
2021-08-08 14:45:47,270 loading model...
2021-08-08 14:45:47,634 done!
2021-08-08 14:45:47,634 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:45:47,634 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:45:50,491 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:45:51,982 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:45:52,094 loading model...
2021-08-08 14:45:52,107 done!
2021-08-08 14:45:52,173 ***** Running training *****
2021-08-08 14:45:52,173   Num examples = 99
2021-08-08 14:45:52,173   Batch size = 32
2021-08-08 14:45:52,173   Num steps = 9
2021-08-08 14:45:52,174 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:45:52,175 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:45:52,175 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:45:52,175 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:45:52,175 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:45:52,175 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:45:52,176 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:45:52,177 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:45:52,178 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:45:52,179 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:45:52,180 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:45:52,180 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:45:52,180 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:45:52,180 n: module.bert.pooler.dense.weight
2021-08-08 14:45:52,180 n: module.bert.pooler.dense.bias
2021-08-08 14:45:52,180 n: module.classifier.weight
2021-08-08 14:45:52,180 n: module.classifier.bias
2021-08-08 14:45:52,180 n: module.fit_dense.weight
2021-08-08 14:45:52,180 n: module.fit_dense.bias
2021-08-08 14:45:52,180 Total parameters: 67547138
2021-08-08 14:46:18,231 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-cased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:46:18,364 device: cuda n_gpu: 4
2021-08-08 14:46:18,394 Writing example 0 of 99
2021-08-08 14:46:18,394 *** Example ***
2021-08-08 14:46:18,394 guid: aug-1
2021-08-08 14:46:18,395 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:46:18,395 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,395 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,395 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,395 label: 0
2021-08-08 14:46:18,395 label_id: 0
2021-08-08 14:46:18,416 Writing example 0 of 872
2021-08-08 14:46:18,416 *** Example ***
2021-08-08 14:46:18,416 guid: dev-1
2021-08-08 14:46:18,416 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:46:18,417 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,417 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,417 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:46:18,417 label: 1
2021-08-08 14:46:18,417 label_id: 1
2021-08-08 14:46:18,659 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:46:21,031 Loading model /home/mcao610/scratch/huggingface/bert-base-cased/pytorch_model.bin
2021-08-08 14:46:21,214 loading model...
2021-08-08 14:46:21,574 done!
2021-08-08 14:46:21,574 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:46:21,574 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:46:24,394 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:46:25,883 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:46:25,995 loading model...
2021-08-08 14:46:26,008 done!
2021-08-08 14:46:26,074 ***** Running training *****
2021-08-08 14:46:26,075   Num examples = 99
2021-08-08 14:46:26,075   Batch size = 32
2021-08-08 14:46:26,075   Num steps = 9
2021-08-08 14:46:26,076 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:46:26,076 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:46:26,076 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:46:26,076 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:46:26,076 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:46:26,076 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:46:26,077 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:46:26,078 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:46:26,079 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:46:26,080 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:46:26,081 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:46:26,081 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:46:26,081 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:46:26,081 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:46:26,081 n: module.bert.pooler.dense.weight
2021-08-08 14:46:26,081 n: module.bert.pooler.dense.bias
2021-08-08 14:46:26,081 n: module.classifier.weight
2021-08-08 14:46:26,081 n: module.classifier.bias
2021-08-08 14:46:26,081 n: module.fit_dense.weight
2021-08-08 14:46:26,081 n: module.fit_dense.bias
2021-08-08 14:46:26,081 Total parameters: 67547138
2021-08-08 14:47:25,433 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-cased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:47:25,564 device: cuda n_gpu: 4
2021-08-08 14:47:25,595 Writing example 0 of 99
2021-08-08 14:47:25,595 *** Example ***
2021-08-08 14:47:25,595 guid: aug-1
2021-08-08 14:47:25,595 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:47:25,595 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,595 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,596 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,596 label: 0
2021-08-08 14:47:25,596 label_id: 0
2021-08-08 14:47:25,616 Writing example 0 of 872
2021-08-08 14:47:25,617 *** Example ***
2021-08-08 14:47:25,617 guid: dev-1
2021-08-08 14:47:25,617 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:47:25,617 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,617 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,617 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:25,617 label: 1
2021-08-08 14:47:25,617 label_id: 1
2021-08-08 14:47:25,856 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-08 14:47:28,376 Loading model /home/mcao610/scratch/huggingface/bert-base-cased/pytorch_model.bin
2021-08-08 14:47:28,565 loading model...
2021-08-08 14:47:28,947 done!
2021-08-08 14:47:28,947 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:47:28,947 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:47:31,777 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:47:33,267 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:47:33,381 loading model...
2021-08-08 14:47:33,393 done!
2021-08-08 14:47:33,461 ***** Running training *****
2021-08-08 14:47:33,461   Num examples = 99
2021-08-08 14:47:33,461   Batch size = 32
2021-08-08 14:47:33,461   Num steps = 9
2021-08-08 14:47:33,462 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:47:33,462 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:47:33,462 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:47:33,462 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:47:33,462 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:47:33,462 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:47:33,462 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:47:33,462 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:47:33,463 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:47:33,464 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:47:33,465 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:47:33,466 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:47:33,467 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:47:33,467 n: module.bert.pooler.dense.weight
2021-08-08 14:47:33,467 n: module.bert.pooler.dense.bias
2021-08-08 14:47:33,467 n: module.classifier.weight
2021-08-08 14:47:33,467 n: module.classifier.bias
2021-08-08 14:47:33,467 n: module.fit_dense.weight
2021-08-08 14:47:33,467 n: module.fit_dense.bias
2021-08-08 14:47:33,467 Total parameters: 67547138
2021-08-08 14:47:57,326 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/bert-base-uncased', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 14:47:57,461 device: cuda n_gpu: 4
2021-08-08 14:47:57,490 Writing example 0 of 99
2021-08-08 14:47:57,491 *** Example ***
2021-08-08 14:47:57,491 guid: aug-1
2021-08-08 14:47:57,491 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 14:47:57,491 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,491 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,491 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,491 label: 0
2021-08-08 14:47:57,491 label_id: 0
2021-08-08 14:47:57,512 Writing example 0 of 872
2021-08-08 14:47:57,513 *** Example ***
2021-08-08 14:47:57,513 guid: dev-1
2021-08-08 14:47:57,513 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 14:47:57,513 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,513 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,513 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 14:47:57,513 label: 1
2021-08-08 14:47:57,513 label_id: 1
2021-08-08 14:47:57,807 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 14:48:00,220 Loading model /home/mcao610/scratch/huggingface/bert-base-uncased/pytorch_model.bin
2021-08-08 14:48:03,513 loading model...
2021-08-08 14:48:03,892 done!
2021-08-08 14:48:03,893 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2021-08-08 14:48:03,893 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-08-08 14:48:06,713 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 14:48:08,206 Loading model /home/mcao610/scratch/6L_768D_FinalModel/SST-2/pytorch_model.bin
2021-08-08 14:48:08,320 loading model...
2021-08-08 14:48:08,333 done!
2021-08-08 14:48:08,399 ***** Running training *****
2021-08-08 14:48:08,400   Num examples = 99
2021-08-08 14:48:08,400   Batch size = 32
2021-08-08 14:48:08,400   Num steps = 9
2021-08-08 14:48:08,401 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 14:48:08,401 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 14:48:08,401 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 14:48:08,401 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 14:48:08,401 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 14:48:08,401 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 14:48:08,402 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 14:48:08,403 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 14:48:08,404 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 14:48:08,405 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 14:48:08,406 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 14:48:08,406 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 14:48:08,406 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 14:48:08,406 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 14:48:08,406 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 14:48:08,406 n: module.bert.pooler.dense.weight
2021-08-08 14:48:08,406 n: module.bert.pooler.dense.bias
2021-08-08 14:48:08,406 n: module.classifier.weight
2021-08-08 14:48:08,406 n: module.classifier.bias
2021-08-08 14:48:08,406 n: module.fit_dense.weight
2021-08-08 14:48:08,406 n: module.fit_dense.bias
2021-08-08 14:48:08,406 Total parameters: 67547138
2021-08-08 16:33:57,681 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/SST-2', task_name='SST-2', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 16:33:57,824 device: cuda n_gpu: 3
2021-08-08 16:33:58,923 Writing example 0 of 99
2021-08-08 16:33:58,925 *** Example ***
2021-08-08 16:33:58,925 guid: aug-1
2021-08-08 16:33:58,925 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2021-08-08 16:33:58,925 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:58,925 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:58,925 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:58,926 label: 0
2021-08-08 16:33:58,926 label_id: 0
2021-08-08 16:33:59,208 Writing example 0 of 872
2021-08-08 16:33:59,208 *** Example ***
2021-08-08 16:33:59,208 guid: dev-1
2021-08-08 16:33:59,208 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-08 16:33:59,208 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:59,208 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:59,208 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:33:59,208 label: 1
2021-08-08 16:33:59,208 label_id: 1
2021-08-08 16:34:00,144 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 16:34:02,820 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000/pytorch_model.bin
2021-08-08 16:34:05,809 loading model...
2021-08-08 16:34:05,841 done!
2021-08-08 16:34:05,841 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 16:34:05,841 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 16:36:28,365 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 16:36:28,469 device: cuda n_gpu: 3
2021-08-08 16:36:45,513 Writing example 0 of 505555
2021-08-08 16:36:45,515 *** Example ***
2021-08-08 16:36:45,515 guid: aug-0
2021-08-08 16:36:45,515 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 16:36:45,515 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:36:45,515 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:36:45,515 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:36:45,515 label: neutral
2021-08-08 16:36:45,515 label_id: 2
2021-08-08 16:36:50,204 Writing example 10000 of 505555
2021-08-08 16:36:54,807 Writing example 20000 of 505555
2021-08-08 16:36:59,441 Writing example 30000 of 505555
2021-08-08 16:37:04,147 Writing example 40000 of 505555
2021-08-08 16:37:08,958 Writing example 50000 of 505555
2021-08-08 16:37:13,478 Writing example 60000 of 505555
2021-08-08 16:37:18,115 Writing example 70000 of 505555
2021-08-08 16:37:22,683 Writing example 80000 of 505555
2021-08-08 16:37:27,620 Writing example 90000 of 505555
2021-08-08 16:37:32,211 Writing example 100000 of 505555
2021-08-08 16:37:36,814 Writing example 110000 of 505555
2021-08-08 16:37:41,181 Writing example 120000 of 505555
2021-08-08 16:37:45,853 Writing example 130000 of 505555
2021-08-08 16:37:51,166 Writing example 140000 of 505555
2021-08-08 16:37:55,615 Writing example 150000 of 505555
2021-08-08 16:38:00,429 Writing example 160000 of 505555
2021-08-08 16:38:05,089 Writing example 170000 of 505555
2021-08-08 16:38:09,680 Writing example 180000 of 505555
2021-08-08 16:38:14,298 Writing example 190000 of 505555
2021-08-08 16:38:19,031 Writing example 200000 of 505555
2021-08-08 16:38:24,530 Writing example 210000 of 505555
2021-08-08 16:38:29,034 Writing example 220000 of 505555
2021-08-08 16:38:33,674 Writing example 230000 of 505555
2021-08-08 16:38:38,366 Writing example 240000 of 505555
2021-08-08 16:38:43,490 Writing example 250000 of 505555
2021-08-08 16:38:48,314 Writing example 260000 of 505555
2021-08-08 16:38:52,869 Writing example 270000 of 505555
2021-08-08 16:38:57,490 Writing example 280000 of 505555
2021-08-08 16:39:03,203 Writing example 290000 of 505555
2021-08-08 16:39:07,935 Writing example 300000 of 505555
2021-08-08 16:39:12,496 Writing example 310000 of 505555
2021-08-08 16:39:16,955 Writing example 320000 of 505555
2021-08-08 16:39:21,680 Writing example 330000 of 505555
2021-08-08 16:39:26,123 Writing example 340000 of 505555
2021-08-08 16:39:30,687 Writing example 350000 of 505555
2021-08-08 16:39:35,229 Writing example 360000 of 505555
2021-08-08 16:39:39,635 Writing example 370000 of 505555
2021-08-08 16:39:44,130 Writing example 380000 of 505555
2021-08-08 16:39:50,135 Writing example 390000 of 505555
2021-08-08 16:39:54,817 Writing example 400000 of 505555
2021-08-08 16:39:59,448 Writing example 410000 of 505555
2021-08-08 16:40:03,923 Writing example 420000 of 505555
2021-08-08 16:40:08,351 Writing example 430000 of 505555
2021-08-08 16:40:12,808 Writing example 440000 of 505555
2021-08-08 16:40:17,469 Writing example 450000 of 505555
2021-08-08 16:40:22,182 Writing example 460000 of 505555
2021-08-08 16:40:26,809 Writing example 470000 of 505555
2021-08-08 16:40:31,483 Writing example 480000 of 505555
2021-08-08 16:40:36,070 Writing example 490000 of 505555
2021-08-08 16:40:40,627 Writing example 500000 of 505555
2021-08-08 16:40:48,097 Writing example 0 of 9815
2021-08-08 16:40:48,098 *** Example ***
2021-08-08 16:40:48,098 guid: dev_matched-0
2021-08-08 16:40:48,098 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 16:40:48,098 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:40:48,098 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:40:48,098 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:40:48,098 label: neutral
2021-08-08 16:40:48,098 label_id: 2
2021-08-08 16:40:52,416 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 16:40:54,816 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000/pytorch_model.bin
2021-08-08 16:40:55,044 loading model...
2021-08-08 16:40:55,070 done!
2021-08-08 16:40:55,070 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 16:40:55,070 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 16:41:02,579 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 16:41:04,087 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 16:41:06,513 loading model...
2021-08-08 16:41:06,526 done!
2021-08-08 16:41:06,527 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 16:41:06,527 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 16:41:06,594 ***** Running training *****
2021-08-08 16:41:06,594   Num examples = 505555
2021-08-08 16:41:06,594   Batch size = 32
2021-08-08 16:41:06,594   Num steps = 47394
2021-08-08 16:41:06,595 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 16:41:06,595 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 16:41:06,596 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 16:41:06,596 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 16:41:06,596 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 16:41:06,596 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 16:41:06,597 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 16:41:06,598 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 16:41:06,599 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 16:41:06,600 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 16:41:06,600 n: module.bert.pooler.dense.weight
2021-08-08 16:41:06,601 n: module.bert.pooler.dense.bias
2021-08-08 16:41:06,601 n: module.classifier.weight
2021-08-08 16:41:06,601 n: module.classifier.bias
2021-08-08 16:41:06,601 n: module.fit_dense.weight
2021-08-08 16:41:06,601 n: module.fit_dense.bias
2021-08-08 16:41:06,601 Total parameters: 67547907
2021-08-08 16:41:27,222 ***** Running evaluation *****
2021-08-08 16:41:27,222   Epoch = 0 iter 99 step
2021-08-08 16:41:27,222   Num examples = 9815
2021-08-08 16:41:27,222   Batch size = 32
2021-08-08 16:41:36,629 ***** Eval results *****
2021-08-08 16:41:36,629   acc = 0.8428935303107489
2021-08-08 16:41:36,629   att_loss = 0.0
2021-08-08 16:41:36,629   cls_loss = 1.0090069204869896
2021-08-08 16:41:36,629   eval_loss = 0.4055772553826
2021-08-08 16:41:36,629   global_step = 99
2021-08-08 16:41:36,629   loss = 1.0090069204869896
2021-08-08 16:41:36,629   rep_loss = 0.0
2021-08-08 16:41:36,630 ***** Save model *****
2021-08-08 16:41:40,062 Writing example 0 of 9832
2021-08-08 16:41:40,063 *** Example ***
2021-08-08 16:41:40,063 guid: dev_matched-0
2021-08-08 16:41:40,063 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 16:41:40,063 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:41:40,063 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:41:40,063 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:41:40,063 label: contradiction
2021-08-08 16:41:40,063 label_id: 0
2021-08-08 16:50:51,795 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=1, gradient_accumulation_steps=1, learning_rate=0.0, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 16:50:51,905 device: cuda n_gpu: 3
2021-08-08 16:51:42,592 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=1, gradient_accumulation_steps=1, learning_rate=0.0, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 16:51:42,693 device: cuda n_gpu: 3
2021-08-08 16:51:43,627 Writing example 0 of 99
2021-08-08 16:51:43,627 *** Example ***
2021-08-08 16:51:43,627 guid: aug-0
2021-08-08 16:51:43,627 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 16:51:43,628 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,628 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,628 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,628 label: neutral
2021-08-08 16:51:43,628 label_id: 2
2021-08-08 16:51:43,789 Writing example 0 of 9815
2021-08-08 16:51:43,790 *** Example ***
2021-08-08 16:51:43,790 guid: dev_matched-0
2021-08-08 16:51:43,790 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 16:51:43,790 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,790 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,790 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:51:43,790 label: neutral
2021-08-08 16:51:43,790 label_id: 2
2021-08-08 16:51:48,132 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 16:51:50,535 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-10000/pytorch_model.bin
2021-08-08 16:51:50,749 loading model...
2021-08-08 16:51:50,774 done!
2021-08-08 16:51:50,774 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 16:51:50,774 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 16:51:53,621 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 16:51:55,118 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 16:51:55,500 loading model...
2021-08-08 16:51:55,512 done!
2021-08-08 16:51:55,513 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 16:51:55,513 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 16:51:55,580 ***** Running training *****
2021-08-08 16:51:55,580   Num examples = 99
2021-08-08 16:51:55,580   Batch size = 32
2021-08-08 16:51:55,580   Num steps = 9
2021-08-08 16:51:55,581 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 16:51:55,581 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 16:51:55,581 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 16:51:55,581 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 16:51:55,581 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 16:51:55,581 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 16:51:55,582 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 16:51:55,583 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 16:51:55,584 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 16:51:55,585 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 16:51:55,586 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 16:51:55,586 n: module.bert.pooler.dense.weight
2021-08-08 16:51:55,586 n: module.bert.pooler.dense.bias
2021-08-08 16:51:55,586 n: module.classifier.weight
2021-08-08 16:51:55,586 n: module.classifier.bias
2021-08-08 16:51:55,586 n: module.fit_dense.weight
2021-08-08 16:51:55,586 n: module.fit_dense.bias
2021-08-08 16:51:55,587 Total parameters: 67547907
2021-08-08 16:52:00,630 ***** Running evaluation *****
2021-08-08 16:52:00,630   Epoch = 0 iter 1 step
2021-08-08 16:52:00,630   Num examples = 9815
2021-08-08 16:52:00,630   Batch size = 32
2021-08-08 16:52:09,782 ***** Eval results *****
2021-08-08 16:52:09,783   acc = 0.8453387671930719
2021-08-08 16:52:09,783   att_loss = 0.0
2021-08-08 16:52:09,783   cls_loss = 0.8953660726547241
2021-08-08 16:52:09,783   eval_loss = 0.4437068619974661
2021-08-08 16:52:09,783   global_step = 1
2021-08-08 16:52:09,783   loss = 0.8953660726547241
2021-08-08 16:52:09,783   rep_loss = 0.0
2021-08-08 16:52:09,784 ***** Save model *****
2021-08-08 16:52:10,581 Writing example 0 of 9832
2021-08-08 16:52:10,582 *** Example ***
2021-08-08 16:52:10,582 guid: dev_matched-0
2021-08-08 16:52:10,582 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 16:52:10,582 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:52:10,582 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:52:10,582 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 16:52:10,582 label: contradiction
2021-08-08 16:52:10,582 label_id: 0
2021-08-08 17:24:19,907 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=1000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 17:24:20,018 device: cuda n_gpu: 3
2021-08-08 17:24:20,050 Writing example 0 of 99
2021-08-08 17:24:20,051 *** Example ***
2021-08-08 17:24:20,051 guid: aug-0
2021-08-08 17:24:20,051 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 17:24:20,051 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,051 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,051 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,051 label: neutral
2021-08-08 17:24:20,051 label_id: 2
2021-08-08 17:24:20,211 Writing example 0 of 9815
2021-08-08 17:24:20,211 *** Example ***
2021-08-08 17:24:20,212 guid: dev_matched-0
2021-08-08 17:24:20,212 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 17:24:20,212 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,212 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,212 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:24:20,212 label: neutral
2021-08-08 17:24:20,212 label_id: 2
2021-08-08 17:24:24,591 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 17:24:27,035 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 17:24:35,310 loading model...
2021-08-08 17:24:35,335 done!
2021-08-08 17:24:35,335 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 17:24:35,336 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 17:24:38,192 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 17:24:38,524 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 17:24:38,556 loading model...
2021-08-08 17:24:38,560 done!
2021-08-08 17:24:38,560 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 17:24:38,560 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 17:24:38,579 ***** Running training *****
2021-08-08 17:24:38,579   Num examples = 99
2021-08-08 17:24:38,579   Batch size = 32
2021-08-08 17:24:38,579   Num steps = 15
2021-08-08 17:24:38,580 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 17:24:38,580 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 17:24:38,580 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 17:24:38,580 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 17:24:38,580 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 17:24:38,580 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 17:24:38,580 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 17:24:38,580 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 17:24:38,580 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 17:24:38,580 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 17:24:38,581 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 17:24:38,582 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 17:24:38,583 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 17:24:38,584 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 17:24:38,584 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 17:24:38,584 n: module.bert.pooler.dense.weight
2021-08-08 17:24:38,584 n: module.bert.pooler.dense.bias
2021-08-08 17:24:38,584 n: module.classifier.weight
2021-08-08 17:24:38,584 n: module.classifier.bias
2021-08-08 17:24:38,584 n: module.fit_dense.weight
2021-08-08 17:24:38,584 n: module.fit_dense.bias
2021-08-08 17:24:38,584 Total parameters: 14591571
2021-08-08 17:27:25,318 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=1000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 17:27:25,427 device: cuda n_gpu: 3
2021-08-08 17:27:33,560 Writing example 0 of 505555
2021-08-08 17:27:33,562 *** Example ***
2021-08-08 17:27:33,562 guid: aug-0
2021-08-08 17:27:33,562 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 17:27:33,562 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:27:33,562 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:27:33,562 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:27:33,562 label: neutral
2021-08-08 17:27:33,562 label_id: 2
2021-08-08 17:27:38,380 Writing example 10000 of 505555
2021-08-08 17:27:43,159 Writing example 20000 of 505555
2021-08-08 17:27:47,788 Writing example 30000 of 505555
2021-08-08 17:27:52,513 Writing example 40000 of 505555
2021-08-08 17:27:57,421 Writing example 50000 of 505555
2021-08-08 17:28:01,986 Writing example 60000 of 505555
2021-08-08 17:28:06,644 Writing example 70000 of 505555
2021-08-08 17:28:11,234 Writing example 80000 of 505555
2021-08-08 17:28:16,135 Writing example 90000 of 505555
2021-08-08 17:28:20,742 Writing example 100000 of 505555
2021-08-08 17:28:25,354 Writing example 110000 of 505555
2021-08-08 17:28:29,769 Writing example 120000 of 505555
2021-08-08 17:28:34,454 Writing example 130000 of 505555
2021-08-08 17:28:39,810 Writing example 140000 of 505555
2021-08-08 17:28:44,264 Writing example 150000 of 505555
2021-08-08 17:28:49,058 Writing example 160000 of 505555
2021-08-08 17:28:53,740 Writing example 170000 of 505555
2021-08-08 17:28:58,365 Writing example 180000 of 505555
2021-08-08 17:29:03,184 Writing example 190000 of 505555
2021-08-08 17:29:07,865 Writing example 200000 of 505555
2021-08-08 17:29:13,337 Writing example 210000 of 505555
2021-08-08 17:29:17,838 Writing example 220000 of 505555
2021-08-08 17:29:22,491 Writing example 230000 of 505555
2021-08-08 17:29:27,085 Writing example 240000 of 505555
2021-08-08 17:29:31,747 Writing example 250000 of 505555
2021-08-08 17:29:36,367 Writing example 260000 of 505555
2021-08-08 17:29:40,878 Writing example 270000 of 505555
2021-08-08 17:29:45,516 Writing example 280000 of 505555
2021-08-08 17:29:51,255 Writing example 290000 of 505555
2021-08-08 17:29:55,953 Writing example 300000 of 505555
2021-08-08 17:30:00,530 Writing example 310000 of 505555
2021-08-08 17:30:04,990 Writing example 320000 of 505555
2021-08-08 17:30:09,726 Writing example 330000 of 505555
2021-08-08 17:30:14,181 Writing example 340000 of 505555
2021-08-08 17:30:18,710 Writing example 350000 of 505555
2021-08-08 17:30:23,225 Writing example 360000 of 505555
2021-08-08 17:30:27,690 Writing example 370000 of 505555
2021-08-08 17:30:32,214 Writing example 380000 of 505555
2021-08-08 17:30:38,320 Writing example 390000 of 505555
2021-08-08 17:30:43,005 Writing example 400000 of 505555
2021-08-08 17:30:47,570 Writing example 410000 of 505555
2021-08-08 17:30:52,060 Writing example 420000 of 505555
2021-08-08 17:30:56,498 Writing example 430000 of 505555
2021-08-08 17:31:00,947 Writing example 440000 of 505555
2021-08-08 17:31:05,596 Writing example 450000 of 505555
2021-08-08 17:31:10,220 Writing example 460000 of 505555
2021-08-08 17:31:14,819 Writing example 470000 of 505555
2021-08-08 17:31:19,566 Writing example 480000 of 505555
2021-08-08 17:31:24,179 Writing example 490000 of 505555
2021-08-08 17:31:28,763 Writing example 500000 of 505555
2021-08-08 17:31:35,841 Writing example 0 of 9815
2021-08-08 17:31:35,842 *** Example ***
2021-08-08 17:31:35,842 guid: dev_matched-0
2021-08-08 17:31:35,842 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 17:31:35,842 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:31:35,842 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:31:35,842 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:31:35,842 label: neutral
2021-08-08 17:31:35,842 label_id: 2
2021-08-08 17:31:40,188 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 17:31:42,590 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 17:31:42,827 loading model...
2021-08-08 17:31:42,852 done!
2021-08-08 17:31:42,852 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 17:31:42,852 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 17:31:45,644 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 17:31:45,976 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 17:31:46,004 loading model...
2021-08-08 17:31:46,043 done!
2021-08-08 17:31:46,043 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 17:31:46,043 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 17:31:46,061 ***** Running training *****
2021-08-08 17:31:46,061   Num examples = 505555
2021-08-08 17:31:46,061   Batch size = 32
2021-08-08 17:31:46,061   Num steps = 78990
2021-08-08 17:31:46,062 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 17:31:46,062 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 17:31:46,062 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 17:31:46,062 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 17:31:46,062 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 17:31:46,062 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 17:31:46,062 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 17:31:46,062 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 17:31:46,062 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 17:31:46,063 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 17:31:46,064 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 17:31:46,065 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 17:31:46,066 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 17:31:46,066 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 17:31:46,066 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 17:31:46,066 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 17:31:46,066 n: module.bert.pooler.dense.weight
2021-08-08 17:31:46,066 n: module.bert.pooler.dense.bias
2021-08-08 17:31:46,066 n: module.classifier.weight
2021-08-08 17:31:46,066 n: module.classifier.bias
2021-08-08 17:31:46,066 n: module.fit_dense.weight
2021-08-08 17:31:46,066 n: module.fit_dense.bias
2021-08-08 17:31:46,066 Total parameters: 14591571
2021-08-08 17:33:41,640 ***** Running evaluation *****
2021-08-08 17:33:41,641   Epoch = 0 iter 999 step
2021-08-08 17:33:41,641   Num examples = 9815
2021-08-08 17:33:41,641   Batch size = 32
2021-08-08 17:33:48,074 ***** Eval results *****
2021-08-08 17:33:48,074   acc = 0.3370351502801834
2021-08-08 17:33:48,074   att_loss = 0.0
2021-08-08 17:33:48,074   cls_loss = 0.3542455739087171
2021-08-08 17:33:48,074   eval_loss = 1.1782646221912645
2021-08-08 17:33:48,074   global_step = 999
2021-08-08 17:33:48,074   loss = 0.3542455739087171
2021-08-08 17:33:48,074   rep_loss = 0.0
2021-08-08 17:33:48,076 ***** Save model *****
2021-08-08 17:33:48,619 Writing example 0 of 9832
2021-08-08 17:33:48,619 *** Example ***
2021-08-08 17:33:48,619 guid: dev_matched-0
2021-08-08 17:33:48,619 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 17:33:48,619 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:33:48,620 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:33:48,620 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:33:48,620 label: contradiction
2021-08-08 17:33:48,620 label_id: 0
2021-08-08 17:33:53,284 ***** Running mm evaluation *****
2021-08-08 17:33:53,284   Num examples = 9832
2021-08-08 17:33:53,284   Batch size = 32
2021-08-08 17:34:01,079 ***** Eval results *****
2021-08-08 17:34:01,079   acc = 0.33411310008136696
2021-08-08 17:34:01,079   eval_loss = 1.1786781859088253
2021-08-08 17:34:01,079   global_step = 999
2021-08-08 17:35:54,448 ***** Running evaluation *****
2021-08-08 17:35:54,448   Epoch = 0 iter 1999 step
2021-08-08 17:35:54,448   Num examples = 9832
2021-08-08 17:35:54,448   Batch size = 32
2021-08-08 17:36:00,286 ***** Eval results *****
2021-08-08 17:36:00,286   acc = 0.19985760781122863
2021-08-08 17:36:00,286   att_loss = 0.0
2021-08-08 17:36:00,286   cls_loss = 0.34257959043341557
2021-08-08 17:36:00,286   eval_loss = 1.4196752758769247
2021-08-08 17:36:00,286   global_step = 1999
2021-08-08 17:36:00,286   loss = 0.34257959043341557
2021-08-08 17:36:00,286   rep_loss = 0.0
2021-08-08 17:37:51,176 ***** Running evaluation *****
2021-08-08 17:37:51,178   Epoch = 0 iter 2999 step
2021-08-08 17:37:51,178   Num examples = 9832
2021-08-08 17:37:51,178   Batch size = 32
2021-08-08 17:37:56,924 ***** Eval results *****
2021-08-08 17:37:56,924   acc = 0.2032139951179821
2021-08-08 17:37:56,924   att_loss = 0.0
2021-08-08 17:37:56,924   cls_loss = 0.3347558636850578
2021-08-08 17:37:56,924   eval_loss = 1.5270790279685678
2021-08-08 17:37:56,924   global_step = 2999
2021-08-08 17:37:56,924   loss = 0.3347558636850578
2021-08-08 17:37:56,924   rep_loss = 0.0
2021-08-08 17:39:49,601 ***** Running evaluation *****
2021-08-08 17:39:49,602   Epoch = 0 iter 3999 step
2021-08-08 17:39:49,602   Num examples = 9832
2021-08-08 17:39:49,602   Batch size = 32
2021-08-08 17:39:55,372 ***** Eval results *****
2021-08-08 17:39:55,372   acc = 0.17839707078925957
2021-08-08 17:39:55,372   att_loss = 0.0
2021-08-08 17:39:55,372   cls_loss = 0.3291192640734184
2021-08-08 17:39:55,372   eval_loss = 1.6204329078847712
2021-08-08 17:39:55,372   global_step = 3999
2021-08-08 17:39:55,372   loss = 0.3291192640734184
2021-08-08 17:39:55,372   rep_loss = 0.0
2021-08-08 17:42:48,426 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI$', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate$', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D$', task_name='MNLI$', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000$', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 17:42:48,532 device: cuda n_gpu: 3
2021-08-08 17:43:28,201 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 17:43:28,304 device: cuda n_gpu: 3
2021-08-08 17:43:35,885 Writing example 0 of 505555
2021-08-08 17:43:35,886 *** Example ***
2021-08-08 17:43:35,886 guid: aug-0
2021-08-08 17:43:35,886 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 17:43:35,886 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:43:35,886 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:43:35,886 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:43:35,886 label: neutral
2021-08-08 17:43:35,886 label_id: 2
2021-08-08 17:43:40,523 Writing example 10000 of 505555
2021-08-08 17:43:45,087 Writing example 20000 of 505555
2021-08-08 17:43:49,587 Writing example 30000 of 505555
2021-08-08 17:43:54,251 Writing example 40000 of 505555
2021-08-08 17:43:59,052 Writing example 50000 of 505555
2021-08-08 17:44:03,533 Writing example 60000 of 505555
2021-08-08 17:44:08,143 Writing example 70000 of 505555
2021-08-08 17:44:12,671 Writing example 80000 of 505555
2021-08-08 17:44:17,559 Writing example 90000 of 505555
2021-08-08 17:44:22,149 Writing example 100000 of 505555
2021-08-08 17:44:26,754 Writing example 110000 of 505555
2021-08-08 17:44:31,079 Writing example 120000 of 505555
2021-08-08 17:44:35,701 Writing example 130000 of 505555
2021-08-08 17:44:40,976 Writing example 140000 of 505555
2021-08-08 17:44:45,365 Writing example 150000 of 505555
2021-08-08 17:44:50,121 Writing example 160000 of 505555
2021-08-08 17:44:54,732 Writing example 170000 of 505555
2021-08-08 17:44:59,400 Writing example 180000 of 505555
2021-08-08 17:45:03,977 Writing example 190000 of 505555
2021-08-08 17:45:08,543 Writing example 200000 of 505555
2021-08-08 17:45:13,970 Writing example 210000 of 505555
2021-08-08 17:45:18,401 Writing example 220000 of 505555
2021-08-08 17:45:22,995 Writing example 230000 of 505555
2021-08-08 17:45:27,527 Writing example 240000 of 505555
2021-08-08 17:45:32,126 Writing example 250000 of 505555
2021-08-08 17:45:36,735 Writing example 260000 of 505555
2021-08-08 17:45:41,187 Writing example 270000 of 505555
2021-08-08 17:45:45,607 Writing example 280000 of 505555
2021-08-08 17:45:51,293 Writing example 290000 of 505555
2021-08-08 17:45:56,032 Writing example 300000 of 505555
2021-08-08 17:46:00,546 Writing example 310000 of 505555
2021-08-08 17:46:04,927 Writing example 320000 of 505555
2021-08-08 17:46:09,567 Writing example 330000 of 505555
2021-08-08 17:46:13,963 Writing example 340000 of 505555
2021-08-08 17:46:18,434 Writing example 350000 of 505555
2021-08-08 17:46:22,886 Writing example 360000 of 505555
2021-08-08 17:46:27,321 Writing example 370000 of 505555
2021-08-08 17:46:31,961 Writing example 380000 of 505555
2021-08-08 17:46:37,890 Writing example 390000 of 505555
2021-08-08 17:46:42,470 Writing example 400000 of 505555
2021-08-08 17:46:46,978 Writing example 410000 of 505555
2021-08-08 17:46:51,504 Writing example 420000 of 505555
2021-08-08 17:46:55,899 Writing example 430000 of 505555
2021-08-08 17:47:00,370 Writing example 440000 of 505555
2021-08-08 17:47:04,948 Writing example 450000 of 505555
2021-08-08 17:47:09,514 Writing example 460000 of 505555
2021-08-08 17:47:14,035 Writing example 470000 of 505555
2021-08-08 17:47:18,640 Writing example 480000 of 505555
2021-08-08 17:47:23,187 Writing example 490000 of 505555
2021-08-08 17:47:27,726 Writing example 500000 of 505555
2021-08-08 17:47:35,021 Writing example 0 of 9815
2021-08-08 17:47:35,021 *** Example ***
2021-08-08 17:47:35,021 guid: dev_matched-0
2021-08-08 17:47:35,021 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 17:47:35,021 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:47:35,021 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:47:35,022 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 17:47:35,022 label: neutral
2021-08-08 17:47:35,022 label_id: 2
2021-08-08 17:47:39,294 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 17:47:41,771 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 17:47:47,930 loading model...
2021-08-08 17:47:47,963 done!
2021-08-08 17:47:47,963 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 17:47:47,963 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 17:47:50,806 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 17:47:51,145 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 17:47:51,174 loading model...
2021-08-08 17:47:51,178 done!
2021-08-08 17:47:51,178 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 17:47:51,178 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 17:47:51,197 ***** Running training *****
2021-08-08 17:47:51,197   Num examples = 505555
2021-08-08 17:47:51,197   Batch size = 32
2021-08-08 17:47:51,197   Num steps = 157980
2021-08-08 17:47:51,198 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 17:47:51,198 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 17:47:51,198 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 17:47:51,198 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 17:47:51,198 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 17:47:51,198 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 17:47:51,199 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 17:47:51,200 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 17:47:51,201 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 17:47:51,202 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 17:47:51,202 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 17:47:51,202 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 17:47:51,202 n: module.bert.pooler.dense.weight
2021-08-08 17:47:51,202 n: module.bert.pooler.dense.bias
2021-08-08 17:47:51,202 n: module.classifier.weight
2021-08-08 17:47:51,202 n: module.classifier.bias
2021-08-08 17:47:51,202 n: module.fit_dense.weight
2021-08-08 17:47:51,202 n: module.fit_dense.bias
2021-08-08 17:47:51,202 Total parameters: 14591571
2021-08-08 17:48:01,783 ***** Running evaluation *****
2021-08-08 17:48:01,783   Epoch = 0 iter 49 step
2021-08-08 17:48:01,783   Num examples = 9815
2021-08-08 17:48:01,783   Batch size = 32
2021-08-08 17:48:02,086 ***** Eval results *****
2021-08-08 17:48:02,086   att_loss = 2.8334677997900517
2021-08-08 17:48:02,086   cls_loss = 0.0
2021-08-08 17:48:02,086   global_step = 49
2021-08-08 17:48:02,086   loss = 3.823365406114228
2021-08-08 17:48:02,086   rep_loss = 0.9898976197048109
2021-08-08 17:48:02,087 ***** Save model *****
2021-08-08 17:48:08,923 ***** Running evaluation *****
2021-08-08 17:48:08,924   Epoch = 0 iter 99 step
2021-08-08 17:48:08,924   Num examples = 9815
2021-08-08 17:48:08,924   Batch size = 32
2021-08-08 17:48:08,925 ***** Eval results *****
2021-08-08 17:48:08,925   att_loss = 2.68485771285163
2021-08-08 17:48:08,925   cls_loss = 0.0
2021-08-08 17:48:08,925   global_step = 99
2021-08-08 17:48:08,925   loss = 3.6473348959527834
2021-08-08 17:48:08,925   rep_loss = 0.9624771861114887
2021-08-08 17:48:08,925 ***** Save model *****
2021-08-08 17:48:14,798 ***** Running evaluation *****
2021-08-08 17:48:14,798   Epoch = 0 iter 149 step
2021-08-08 17:48:14,798   Num examples = 9815
2021-08-08 17:48:14,798   Batch size = 32
2021-08-08 17:48:14,799 ***** Eval results *****
2021-08-08 17:48:14,799   att_loss = 2.632826224269483
2021-08-08 17:48:14,799   cls_loss = 0.0
2021-08-08 17:48:14,799   global_step = 149
2021-08-08 17:48:14,799   loss = 3.583615635865487
2021-08-08 17:48:14,799   rep_loss = 0.9507894159963467
2021-08-08 17:48:14,799 ***** Save model *****
2021-08-08 17:48:20,674 ***** Running evaluation *****
2021-08-08 17:48:20,674   Epoch = 0 iter 199 step
2021-08-08 17:48:20,674   Num examples = 9815
2021-08-08 17:48:20,674   Batch size = 32
2021-08-08 17:48:20,675 ***** Eval results *****
2021-08-08 17:48:20,675   att_loss = 2.5856395941882875
2021-08-08 17:48:20,675   cls_loss = 0.0
2021-08-08 17:48:20,675   global_step = 199
2021-08-08 17:48:20,675   loss = 3.5279066646518418
2021-08-08 17:48:20,676   rep_loss = 0.9422670734587626
2021-08-08 17:48:20,676 ***** Save model *****
2021-08-08 17:48:26,577 ***** Running evaluation *****
2021-08-08 17:48:26,578   Epoch = 0 iter 249 step
2021-08-08 17:48:26,578   Num examples = 9815
2021-08-08 17:48:26,578   Batch size = 32
2021-08-08 17:48:26,579 ***** Eval results *****
2021-08-08 17:48:26,579   att_loss = 2.546640126101942
2021-08-08 17:48:26,579   cls_loss = 0.0
2021-08-08 17:48:26,579   global_step = 249
2021-08-08 17:48:26,579   loss = 3.4825207976452317
2021-08-08 17:48:26,579   rep_loss = 0.9358806734582985
2021-08-08 17:48:26,579 ***** Save model *****
2021-08-08 17:48:32,509 ***** Running evaluation *****
2021-08-08 17:48:32,509   Epoch = 0 iter 299 step
2021-08-08 17:48:32,509   Num examples = 9815
2021-08-08 17:48:32,509   Batch size = 32
2021-08-08 17:48:32,510 ***** Eval results *****
2021-08-08 17:48:32,510   att_loss = 2.523643003259614
2021-08-08 17:48:32,510   cls_loss = 0.0
2021-08-08 17:48:32,510   global_step = 299
2021-08-08 17:48:32,510   loss = 3.4550196366963974
2021-08-08 17:48:32,510   rep_loss = 0.931376632639397
2021-08-08 17:48:32,511 ***** Save model *****
2021-08-08 17:48:38,499 ***** Running evaluation *****
2021-08-08 17:48:38,499   Epoch = 0 iter 349 step
2021-08-08 17:48:38,499   Num examples = 9815
2021-08-08 17:48:38,499   Batch size = 32
2021-08-08 17:48:38,500 ***** Eval results *****
2021-08-08 17:48:38,500   att_loss = 2.5014842174797822
2021-08-08 17:48:38,500   cls_loss = 0.0
2021-08-08 17:48:38,500   global_step = 349
2021-08-08 17:48:38,500   loss = 3.4286449358593085
2021-08-08 17:48:38,500   rep_loss = 0.9271607166716568
2021-08-08 17:48:38,501 ***** Save model *****
2021-08-08 17:48:44,435 ***** Running evaluation *****
2021-08-08 17:48:44,435   Epoch = 0 iter 399 step
2021-08-08 17:48:44,435   Num examples = 9815
2021-08-08 17:48:44,436   Batch size = 32
2021-08-08 17:48:44,436 ***** Eval results *****
2021-08-08 17:48:44,436   att_loss = 2.481389964732311
2021-08-08 17:48:44,436   cls_loss = 0.0
2021-08-08 17:48:44,436   global_step = 399
2021-08-08 17:48:44,436   loss = 3.4049785262659977
2021-08-08 17:48:44,437   rep_loss = 0.9235885579484447
2021-08-08 17:48:44,437 ***** Save model *****
2021-08-08 17:48:51,389 ***** Running evaluation *****
2021-08-08 17:48:51,389   Epoch = 0 iter 449 step
2021-08-08 17:48:51,390   Num examples = 9815
2021-08-08 17:48:51,390   Batch size = 32
2021-08-08 17:48:51,390 ***** Eval results *****
2021-08-08 17:48:51,391   att_loss = 2.472100568242487
2021-08-08 17:48:51,391   cls_loss = 0.0
2021-08-08 17:48:51,391   global_step = 449
2021-08-08 17:48:51,391   loss = 3.393055028002087
2021-08-08 17:48:51,391   rep_loss = 0.920954456175356
2021-08-08 17:48:51,391 ***** Save model *****
2021-08-08 17:48:57,408 ***** Running evaluation *****
2021-08-08 17:48:57,409   Epoch = 0 iter 499 step
2021-08-08 17:48:57,409   Num examples = 9815
2021-08-08 17:48:57,409   Batch size = 32
2021-08-08 17:48:57,409 ***** Eval results *****
2021-08-08 17:48:57,410   att_loss = 2.4647763838509995
2021-08-08 17:48:57,410   cls_loss = 0.0
2021-08-08 17:48:57,410   global_step = 499
2021-08-08 17:48:57,410   loss = 3.3836959220603378
2021-08-08 17:48:57,410   rep_loss = 0.9189195364176151
2021-08-08 17:48:57,410 ***** Save model *****
2021-08-08 17:49:03,322 ***** Running evaluation *****
2021-08-08 17:49:03,322   Epoch = 0 iter 549 step
2021-08-08 17:49:03,322   Num examples = 9815
2021-08-08 17:49:03,322   Batch size = 32
2021-08-08 17:49:03,323 ***** Eval results *****
2021-08-08 17:49:03,323   att_loss = 2.460334958491647
2021-08-08 17:49:03,323   cls_loss = 0.0
2021-08-08 17:49:03,323   global_step = 549
2021-08-08 17:49:03,323   loss = 3.377333998028698
2021-08-08 17:49:03,323   rep_loss = 0.9169990397541901
2021-08-08 17:49:03,324 ***** Save model *****
2021-08-08 17:49:09,238 ***** Running evaluation *****
2021-08-08 17:49:09,239   Epoch = 0 iter 599 step
2021-08-08 17:49:09,239   Num examples = 9815
2021-08-08 17:49:09,239   Batch size = 32
2021-08-08 17:49:09,239 ***** Eval results *****
2021-08-08 17:49:09,239   att_loss = 2.4519731463096375
2021-08-08 17:49:09,240   cls_loss = 0.0
2021-08-08 17:49:09,240   global_step = 599
2021-08-08 17:49:09,240   loss = 3.3670604595158853
2021-08-08 17:49:09,240   rep_loss = 0.915087313902796
2021-08-08 17:49:09,240 ***** Save model *****
2021-08-08 17:49:15,506 ***** Running evaluation *****
2021-08-08 17:49:15,507   Epoch = 0 iter 649 step
2021-08-08 17:49:15,507   Num examples = 9815
2021-08-08 17:49:15,507   Batch size = 32
2021-08-08 17:49:15,508 ***** Eval results *****
2021-08-08 17:49:15,508   att_loss = 2.4449911966162214
2021-08-08 17:49:15,508   cls_loss = 0.0
2021-08-08 17:49:15,508   global_step = 649
2021-08-08 17:49:15,508   loss = 3.358502757567654
2021-08-08 17:49:15,508   rep_loss = 0.913511560859592
2021-08-08 17:49:15,508 ***** Save model *****
2021-08-08 17:49:23,574 ***** Running evaluation *****
2021-08-08 17:49:23,574   Epoch = 0 iter 699 step
2021-08-08 17:49:23,574   Num examples = 9815
2021-08-08 17:49:23,574   Batch size = 32
2021-08-08 17:49:23,575 ***** Eval results *****
2021-08-08 17:49:23,575   att_loss = 2.434544782099635
2021-08-08 17:49:23,575   cls_loss = 0.0
2021-08-08 17:49:23,575   global_step = 699
2021-08-08 17:49:23,575   loss = 3.3463147689344543
2021-08-08 17:49:23,575   rep_loss = 0.9117699876875325
2021-08-08 17:49:23,575 ***** Save model *****
2021-08-08 17:49:29,701 ***** Running evaluation *****
2021-08-08 17:49:29,701   Epoch = 0 iter 749 step
2021-08-08 17:49:29,701   Num examples = 9815
2021-08-08 17:49:29,701   Batch size = 32
2021-08-08 17:49:29,702 ***** Eval results *****
2021-08-08 17:49:29,702   att_loss = 2.4266644307226937
2021-08-08 17:49:29,702   cls_loss = 0.0
2021-08-08 17:49:29,702   global_step = 749
2021-08-08 17:49:29,702   loss = 3.3369691935336796
2021-08-08 17:49:29,702   rep_loss = 0.9103047632088808
2021-08-08 17:49:29,702 ***** Save model *****
2021-08-08 17:49:35,635 ***** Running evaluation *****
2021-08-08 17:49:35,635   Epoch = 0 iter 799 step
2021-08-08 17:49:35,636   Num examples = 9815
2021-08-08 17:49:35,636   Batch size = 32
2021-08-08 17:49:35,636 ***** Eval results *****
2021-08-08 17:49:35,636   att_loss = 2.418528738696226
2021-08-08 17:49:35,636   cls_loss = 0.0
2021-08-08 17:49:35,637   global_step = 799
2021-08-08 17:49:35,637   loss = 3.3274676125398717
2021-08-08 17:49:35,637   rep_loss = 0.9089388746642351
2021-08-08 17:49:35,637 ***** Save model *****
2021-08-08 17:49:41,564 ***** Running evaluation *****
2021-08-08 17:49:41,564   Epoch = 0 iter 849 step
2021-08-08 17:49:41,564   Num examples = 9815
2021-08-08 17:49:41,565   Batch size = 32
2021-08-08 17:49:41,565 ***** Eval results *****
2021-08-08 17:49:41,565   att_loss = 2.414351413190997
2021-08-08 17:49:41,566   cls_loss = 0.0
2021-08-08 17:49:41,566   global_step = 849
2021-08-08 17:49:41,566   loss = 3.3221355346684183
2021-08-08 17:49:41,566   rep_loss = 0.9077841229517413
2021-08-08 17:49:41,566 ***** Save model *****
2021-08-08 17:49:47,456 ***** Running evaluation *****
2021-08-08 17:49:47,456   Epoch = 0 iter 899 step
2021-08-08 17:49:47,456   Num examples = 9815
2021-08-08 17:49:47,456   Batch size = 32
2021-08-08 17:49:47,457 ***** Eval results *****
2021-08-08 17:49:47,457   att_loss = 2.4077798951587104
2021-08-08 17:49:47,457   cls_loss = 0.0
2021-08-08 17:49:47,457   global_step = 899
2021-08-08 17:49:47,457   loss = 3.314394138015815
2021-08-08 17:49:47,457   rep_loss = 0.9066142442494266
2021-08-08 17:49:47,457 ***** Save model *****
2021-08-08 17:49:53,345 ***** Running evaluation *****
2021-08-08 17:49:53,346   Epoch = 0 iter 949 step
2021-08-08 17:49:53,346   Num examples = 9815
2021-08-08 17:49:53,346   Batch size = 32
2021-08-08 17:49:53,346 ***** Eval results *****
2021-08-08 17:49:53,347   att_loss = 2.4009528864799234
2021-08-08 17:49:53,347   cls_loss = 0.0
2021-08-08 17:49:53,347   global_step = 949
2021-08-08 17:49:53,347   loss = 3.3064321109944577
2021-08-08 17:49:53,347   rep_loss = 0.9054792249541891
2021-08-08 17:49:53,347 ***** Save model *****
2021-08-08 17:49:59,234 ***** Running evaluation *****
2021-08-08 17:49:59,234   Epoch = 0 iter 999 step
2021-08-08 17:49:59,234   Num examples = 9815
2021-08-08 17:49:59,234   Batch size = 32
2021-08-08 17:49:59,235 ***** Eval results *****
2021-08-08 17:49:59,235   att_loss = 2.3983927378067382
2021-08-08 17:49:59,235   cls_loss = 0.0
2021-08-08 17:49:59,235   global_step = 999
2021-08-08 17:49:59,235   loss = 3.3030743334028454
2021-08-08 17:49:59,235   rep_loss = 0.9046815956557716
2021-08-08 17:49:59,236 ***** Save model *****
2021-08-08 17:50:05,151 ***** Running evaluation *****
2021-08-08 17:50:05,151   Epoch = 0 iter 1049 step
2021-08-08 17:50:05,151   Num examples = 9815
2021-08-08 17:50:05,151   Batch size = 32
2021-08-08 17:50:05,152 ***** Eval results *****
2021-08-08 17:50:05,152   att_loss = 2.395601340312976
2021-08-08 17:50:05,152   cls_loss = 0.0
2021-08-08 17:50:05,152   global_step = 1049
2021-08-08 17:50:05,152   loss = 3.2993804988006503
2021-08-08 17:50:05,152   rep_loss = 0.9037791584876745
2021-08-08 17:50:05,153 ***** Save model *****
2021-08-08 17:50:11,089 ***** Running evaluation *****
2021-08-08 17:50:11,090   Epoch = 0 iter 1099 step
2021-08-08 17:50:11,090   Num examples = 9815
2021-08-08 17:50:11,090   Batch size = 32
2021-08-08 17:50:11,091 ***** Eval results *****
2021-08-08 17:50:11,091   att_loss = 2.3937326803111945
2021-08-08 17:50:11,091   cls_loss = 0.0
2021-08-08 17:50:11,091   global_step = 1099
2021-08-08 17:50:11,091   loss = 3.296790778799638
2021-08-08 17:50:11,092   rep_loss = 0.9030580977833846
2021-08-08 17:50:11,092 ***** Save model *****
2021-08-08 17:50:16,988 ***** Running evaluation *****
2021-08-08 17:50:16,988   Epoch = 0 iter 1149 step
2021-08-08 17:50:16,988   Num examples = 9815
2021-08-08 17:50:16,988   Batch size = 32
2021-08-08 17:50:16,989 ***** Eval results *****
2021-08-08 17:50:16,989   att_loss = 2.3913153740921884
2021-08-08 17:50:16,989   cls_loss = 0.0
2021-08-08 17:50:16,989   global_step = 1149
2021-08-08 17:50:16,989   loss = 3.293806296830804
2021-08-08 17:50:16,989   rep_loss = 0.902490922167988
2021-08-08 17:50:16,990 ***** Save model *****
2021-08-08 17:50:24,920 ***** Running evaluation *****
2021-08-08 17:50:24,920   Epoch = 0 iter 1199 step
2021-08-08 17:50:24,920   Num examples = 9815
2021-08-08 17:50:24,920   Batch size = 32
2021-08-08 17:50:24,921 ***** Eval results *****
2021-08-08 17:50:24,921   att_loss = 2.3880919123610624
2021-08-08 17:50:24,921   cls_loss = 0.0
2021-08-08 17:50:24,921   global_step = 1199
2021-08-08 17:50:24,921   loss = 3.289847033733721
2021-08-08 17:50:24,921   rep_loss = 0.9017551204778435
2021-08-08 17:50:24,922 ***** Save model *****
2021-08-08 17:50:30,820 ***** Running evaluation *****
2021-08-08 17:50:30,820   Epoch = 0 iter 1249 step
2021-08-08 17:50:30,820   Num examples = 9815
2021-08-08 17:50:30,820   Batch size = 32
2021-08-08 17:50:30,821 ***** Eval results *****
2021-08-08 17:50:30,821   att_loss = 2.383635783405472
2021-08-08 17:50:30,821   cls_loss = 0.0
2021-08-08 17:50:30,821   global_step = 1249
2021-08-08 17:50:30,821   loss = 3.284617525754307
2021-08-08 17:50:30,821   rep_loss = 0.9009817407262908
2021-08-08 17:50:30,821 ***** Save model *****
2021-08-08 17:50:36,695 ***** Running evaluation *****
2021-08-08 17:50:36,695   Epoch = 0 iter 1299 step
2021-08-08 17:50:36,695   Num examples = 9815
2021-08-08 17:50:36,695   Batch size = 32
2021-08-08 17:50:36,696 ***** Eval results *****
2021-08-08 17:50:36,696   att_loss = 2.379025946129644
2021-08-08 17:50:36,696   cls_loss = 0.0
2021-08-08 17:50:36,696   global_step = 1299
2021-08-08 17:50:36,696   loss = 3.279213081606541
2021-08-08 17:50:36,696   rep_loss = 0.9001871342838866
2021-08-08 17:50:36,696 ***** Save model *****
2021-08-08 17:50:42,572 ***** Running evaluation *****
2021-08-08 17:50:42,573   Epoch = 0 iter 1349 step
2021-08-08 17:50:42,573   Num examples = 9815
2021-08-08 17:50:42,573   Batch size = 32
2021-08-08 17:50:42,574 ***** Eval results *****
2021-08-08 17:50:42,574   att_loss = 2.3756578678197555
2021-08-08 17:50:42,574   cls_loss = 0.0
2021-08-08 17:50:42,574   global_step = 1349
2021-08-08 17:50:42,574   loss = 3.275142831569075
2021-08-08 17:50:42,574   rep_loss = 0.8994849626888957
2021-08-08 17:50:42,574 ***** Save model *****
2021-08-08 17:50:49,204 ***** Running evaluation *****
2021-08-08 17:50:49,204   Epoch = 0 iter 1399 step
2021-08-08 17:50:49,204   Num examples = 9815
2021-08-08 17:50:49,204   Batch size = 32
2021-08-08 17:50:49,205 ***** Eval results *****
2021-08-08 17:50:49,205   att_loss = 2.373822082529075
2021-08-08 17:50:49,205   cls_loss = 0.0
2021-08-08 17:50:49,205   global_step = 1399
2021-08-08 17:50:49,205   loss = 3.2727446736737265
2021-08-08 17:50:49,205   rep_loss = 0.8989225900795222
2021-08-08 17:50:49,206 ***** Save model *****
2021-08-08 17:50:55,238 ***** Running evaluation *****
2021-08-08 17:50:55,239   Epoch = 0 iter 1449 step
2021-08-08 17:50:55,239   Num examples = 9815
2021-08-08 17:50:55,239   Batch size = 32
2021-08-08 17:50:55,239 ***** Eval results *****
2021-08-08 17:50:55,239   att_loss = 2.3715456274972446
2021-08-08 17:50:55,240   cls_loss = 0.0
2021-08-08 17:50:55,240   global_step = 1449
2021-08-08 17:50:55,240   loss = 3.269868388679457
2021-08-08 17:50:55,240   rep_loss = 0.8983227596190817
2021-08-08 17:50:55,483 ***** Save model *****
2021-08-08 17:51:01,392 ***** Running evaluation *****
2021-08-08 17:51:01,393   Epoch = 0 iter 1499 step
2021-08-08 17:51:01,393   Num examples = 9815
2021-08-08 17:51:01,393   Batch size = 32
2021-08-08 17:51:01,393 ***** Eval results *****
2021-08-08 17:51:01,394   att_loss = 2.3677065205462697
2021-08-08 17:51:01,394   cls_loss = 0.0
2021-08-08 17:51:01,394   global_step = 1499
2021-08-08 17:51:01,394   loss = 3.2653131282989625
2021-08-08 17:51:01,394   rep_loss = 0.8976066053669162
2021-08-08 17:51:01,394 ***** Save model *****
2021-08-08 17:51:07,263 ***** Running evaluation *****
2021-08-08 17:51:07,263   Epoch = 0 iter 1549 step
2021-08-08 17:51:07,264   Num examples = 9815
2021-08-08 17:51:07,264   Batch size = 32
2021-08-08 17:51:07,264 ***** Eval results *****
2021-08-08 17:51:07,264   att_loss = 2.3648172271413292
2021-08-08 17:51:07,264   cls_loss = 0.0
2021-08-08 17:51:07,265   global_step = 1549
2021-08-08 17:51:07,265   loss = 3.261766967040789
2021-08-08 17:51:07,265   rep_loss = 0.8969497366671874
2021-08-08 17:51:07,265 ***** Save model *****
2021-08-08 17:51:14,104 ***** Running evaluation *****
2021-08-08 17:51:14,104   Epoch = 0 iter 1599 step
2021-08-08 17:51:14,104   Num examples = 9815
2021-08-08 17:51:14,105   Batch size = 32
2021-08-08 17:51:14,105 ***** Eval results *****
2021-08-08 17:51:14,105   att_loss = 2.363028234135292
2021-08-08 17:51:14,105   cls_loss = 0.0
2021-08-08 17:51:14,105   global_step = 1599
2021-08-08 17:51:14,105   loss = 3.259473514377959
2021-08-08 17:51:14,106   rep_loss = 0.8964452769250851
2021-08-08 17:51:14,106 ***** Save model *****
2021-08-08 17:51:20,004 ***** Running evaluation *****
2021-08-08 17:51:20,004   Epoch = 0 iter 1649 step
2021-08-08 17:51:20,004   Num examples = 9815
2021-08-08 17:51:20,004   Batch size = 32
2021-08-08 17:51:20,005 ***** Eval results *****
2021-08-08 17:51:20,005   att_loss = 2.3606407133429754
2021-08-08 17:51:20,005   cls_loss = 0.0
2021-08-08 17:51:20,005   global_step = 1649
2021-08-08 17:51:20,006   loss = 3.256513571724738
2021-08-08 17:51:20,006   rep_loss = 0.8958728553455045
2021-08-08 17:51:20,006 ***** Save model *****
2021-08-08 17:51:25,916 ***** Running evaluation *****
2021-08-08 17:51:25,916   Epoch = 0 iter 1699 step
2021-08-08 17:51:25,916   Num examples = 9815
2021-08-08 17:51:25,916   Batch size = 32
2021-08-08 17:51:25,917 ***** Eval results *****
2021-08-08 17:51:25,917   att_loss = 2.3585785522118536
2021-08-08 17:51:25,917   cls_loss = 0.0
2021-08-08 17:51:25,917   global_step = 1699
2021-08-08 17:51:25,917   loss = 3.2539218337904923
2021-08-08 17:51:25,917   rep_loss = 0.8953432784212414
2021-08-08 17:51:25,917 ***** Save model *****
2021-08-08 17:51:31,834 ***** Running evaluation *****
2021-08-08 17:51:31,835   Epoch = 0 iter 1749 step
2021-08-08 17:51:31,835   Num examples = 9815
2021-08-08 17:51:31,835   Batch size = 32
2021-08-08 17:51:31,836 ***** Eval results *****
2021-08-08 17:51:31,836   att_loss = 2.354763789951222
2021-08-08 17:51:31,836   cls_loss = 0.0
2021-08-08 17:51:31,836   global_step = 1749
2021-08-08 17:51:31,836   loss = 3.2495383778730758
2021-08-08 17:51:31,836   rep_loss = 0.8947745846843229
2021-08-08 17:51:31,836 ***** Save model *****
2021-08-08 17:51:37,746 ***** Running evaluation *****
2021-08-08 17:51:37,746   Epoch = 0 iter 1799 step
2021-08-08 17:51:37,746   Num examples = 9815
2021-08-08 17:51:37,746   Batch size = 32
2021-08-08 17:51:37,747 ***** Eval results *****
2021-08-08 17:51:37,747   att_loss = 2.3521192012196317
2021-08-08 17:51:37,747   cls_loss = 0.0
2021-08-08 17:51:37,747   global_step = 1799
2021-08-08 17:51:37,747   loss = 3.2463976349547017
2021-08-08 17:51:37,747   rep_loss = 0.8942784304549921
2021-08-08 17:51:37,748 ***** Save model *****
2021-08-08 17:51:44,928 ***** Running evaluation *****
2021-08-08 17:51:44,928   Epoch = 0 iter 1849 step
2021-08-08 17:51:44,928   Num examples = 9815
2021-08-08 17:51:44,928   Batch size = 32
2021-08-08 17:51:44,929 ***** Eval results *****
2021-08-08 17:51:44,929   att_loss = 2.3500536720839107
2021-08-08 17:51:44,929   cls_loss = 0.0
2021-08-08 17:51:44,929   global_step = 1849
2021-08-08 17:51:44,929   loss = 3.243887297974334
2021-08-08 17:51:44,929   rep_loss = 0.8938336227312805
2021-08-08 17:51:44,929 ***** Save model *****
2021-08-08 17:51:50,968 ***** Running evaluation *****
2021-08-08 17:51:50,968   Epoch = 0 iter 1899 step
2021-08-08 17:51:50,968   Num examples = 9815
2021-08-08 17:51:50,968   Batch size = 32
2021-08-08 17:51:50,969 ***** Eval results *****
2021-08-08 17:51:50,969   att_loss = 2.3484624412073596
2021-08-08 17:51:50,969   cls_loss = 0.0
2021-08-08 17:51:50,969   global_step = 1899
2021-08-08 17:51:50,970   loss = 3.2418653398767905
2021-08-08 17:51:50,970   rep_loss = 0.893402895719017
2021-08-08 17:51:50,970 ***** Save model *****
2021-08-08 17:51:56,899 ***** Running evaluation *****
2021-08-08 17:51:56,899   Epoch = 0 iter 1949 step
2021-08-08 17:51:56,899   Num examples = 9815
2021-08-08 17:51:56,899   Batch size = 32
2021-08-08 17:51:56,900 ***** Eval results *****
2021-08-08 17:51:56,900   att_loss = 2.344849274854528
2021-08-08 17:51:56,900   cls_loss = 0.0
2021-08-08 17:51:56,900   global_step = 1949
2021-08-08 17:51:56,900   loss = 3.2377297977351605
2021-08-08 17:51:56,900   rep_loss = 0.8928805205869699
2021-08-08 17:51:56,900 ***** Save model *****
2021-08-08 17:52:04,908 ***** Running evaluation *****
2021-08-08 17:52:04,908   Epoch = 0 iter 1999 step
2021-08-08 17:52:04,908   Num examples = 9815
2021-08-08 17:52:04,909   Batch size = 32
2021-08-08 17:52:04,909 ***** Eval results *****
2021-08-08 17:52:04,909   att_loss = 2.341614772463632
2021-08-08 17:52:04,910   cls_loss = 0.0
2021-08-08 17:52:04,910   global_step = 1999
2021-08-08 17:52:04,910   loss = 3.2339812279224636
2021-08-08 17:52:04,910   rep_loss = 0.892366453341808
2021-08-08 17:52:04,910 ***** Save model *****
2021-08-08 17:52:10,811 ***** Running evaluation *****
2021-08-08 17:52:10,811   Epoch = 0 iter 2049 step
2021-08-08 17:52:10,811   Num examples = 9815
2021-08-08 17:52:10,812   Batch size = 32
2021-08-08 17:52:10,812 ***** Eval results *****
2021-08-08 17:52:10,812   att_loss = 2.3384887864381407
2021-08-08 17:52:10,812   cls_loss = 0.0
2021-08-08 17:52:10,812   global_step = 2049
2021-08-08 17:52:10,812   loss = 3.2303377212461113
2021-08-08 17:52:10,812   rep_loss = 0.8918489326553382
2021-08-08 17:52:10,813 ***** Save model *****
2021-08-08 17:52:16,711 ***** Running evaluation *****
2021-08-08 17:52:16,711   Epoch = 0 iter 2099 step
2021-08-08 17:52:16,711   Num examples = 9815
2021-08-08 17:52:16,711   Batch size = 32
2021-08-08 17:52:16,712 ***** Eval results *****
2021-08-08 17:52:16,712   att_loss = 2.3375084665969985
2021-08-08 17:52:16,712   cls_loss = 0.0
2021-08-08 17:52:16,712   global_step = 2099
2021-08-08 17:52:16,712   loss = 3.229054537428283
2021-08-08 17:52:16,712   rep_loss = 0.8915460684743594
2021-08-08 17:52:16,713 ***** Save model *****
2021-08-08 17:52:22,592 ***** Running evaluation *****
2021-08-08 17:52:22,592   Epoch = 0 iter 2149 step
2021-08-08 17:52:22,592   Num examples = 9815
2021-08-08 17:52:22,592   Batch size = 32
2021-08-08 17:52:22,593 ***** Eval results *****
2021-08-08 17:52:22,593   att_loss = 2.334136879183404
2021-08-08 17:52:22,593   cls_loss = 0.0
2021-08-08 17:52:22,593   global_step = 2149
2021-08-08 17:52:22,593   loss = 3.2251612450034743
2021-08-08 17:52:22,593   rep_loss = 0.8910243634347751
2021-08-08 17:52:22,593 ***** Save model *****
2021-08-08 17:52:28,491 ***** Running evaluation *****
2021-08-08 17:52:28,492   Epoch = 0 iter 2199 step
2021-08-08 17:52:28,492   Num examples = 9815
2021-08-08 17:52:28,492   Batch size = 32
2021-08-08 17:52:28,492 ***** Eval results *****
2021-08-08 17:52:28,492   att_loss = 2.330740370171457
2021-08-08 17:52:28,493   cls_loss = 0.0
2021-08-08 17:52:28,493   global_step = 2199
2021-08-08 17:52:28,493   loss = 3.2212615611608486
2021-08-08 17:52:28,493   rep_loss = 0.8905211886854375
2021-08-08 17:52:28,493 ***** Save model *****
2021-08-08 17:52:34,390 ***** Running evaluation *****
2021-08-08 17:52:34,390   Epoch = 0 iter 2249 step
2021-08-08 17:52:34,390   Num examples = 9815
2021-08-08 17:52:34,390   Batch size = 32
2021-08-08 17:52:34,391 ***** Eval results *****
2021-08-08 17:52:34,391   att_loss = 2.3284270996091205
2021-08-08 17:52:34,391   cls_loss = 0.0
2021-08-08 17:52:34,391   global_step = 2249
2021-08-08 17:52:34,391   loss = 3.2185614405869907
2021-08-08 17:52:34,391   rep_loss = 0.8901343386986352
2021-08-08 17:52:34,392 ***** Save model *****
2021-08-08 17:52:41,003 ***** Running evaluation *****
2021-08-08 17:52:41,003   Epoch = 0 iter 2299 step
2021-08-08 17:52:41,003   Num examples = 9815
2021-08-08 17:52:41,003   Batch size = 32
2021-08-08 17:52:41,004 ***** Eval results *****
2021-08-08 17:52:41,004   att_loss = 2.3260786430687426
2021-08-08 17:52:41,004   cls_loss = 0.0
2021-08-08 17:52:41,004   global_step = 2299
2021-08-08 17:52:41,004   loss = 3.215779593323769
2021-08-08 17:52:41,004   rep_loss = 0.889700948025362
2021-08-08 17:52:42,720 ***** Save model *****
2021-08-08 17:52:48,626 ***** Running evaluation *****
2021-08-08 17:52:48,626   Epoch = 0 iter 2349 step
2021-08-08 17:52:48,626   Num examples = 9815
2021-08-08 17:52:48,626   Batch size = 32
2021-08-08 17:52:48,627 ***** Eval results *****
2021-08-08 17:52:48,628   att_loss = 2.322989203210281
2021-08-08 17:52:48,628   cls_loss = 0.0
2021-08-08 17:52:48,628   global_step = 2349
2021-08-08 17:52:48,628   loss = 3.2121987497111286
2021-08-08 17:52:48,628   rep_loss = 0.889209544217145
2021-08-08 17:52:48,628 ***** Save model *****
2021-08-08 17:52:54,525 ***** Running evaluation *****
2021-08-08 17:52:54,525   Epoch = 0 iter 2399 step
2021-08-08 17:52:54,525   Num examples = 9815
2021-08-08 17:52:54,525   Batch size = 32
2021-08-08 17:52:54,526 ***** Eval results *****
2021-08-08 17:52:54,526   att_loss = 2.3223489924439193
2021-08-08 17:52:54,526   cls_loss = 0.0
2021-08-08 17:52:54,526   global_step = 2399
2021-08-08 17:52:54,526   loss = 3.211270872678991
2021-08-08 17:52:54,526   rep_loss = 0.8889218779492746
2021-08-08 17:52:54,527 ***** Save model *****
2021-08-08 17:53:00,404 ***** Running evaluation *****
2021-08-08 17:53:00,405   Epoch = 0 iter 2449 step
2021-08-08 17:53:00,405   Num examples = 9815
2021-08-08 17:53:00,405   Batch size = 32
2021-08-08 17:53:00,406 ***** Eval results *****
2021-08-08 17:53:00,406   att_loss = 2.3197561909880817
2021-08-08 17:53:00,406   cls_loss = 0.0
2021-08-08 17:53:00,406   global_step = 2449
2021-08-08 17:53:00,406   loss = 3.208295945587719
2021-08-08 17:53:00,406   rep_loss = 0.8885397526525682
2021-08-08 17:53:00,406 ***** Save model *****
2021-08-08 17:53:06,268 ***** Running evaluation *****
2021-08-08 17:53:06,269   Epoch = 0 iter 2499 step
2021-08-08 17:53:06,269   Num examples = 9815
2021-08-08 17:53:06,269   Batch size = 32
2021-08-08 17:53:06,269 ***** Eval results *****
2021-08-08 17:53:06,270   att_loss = 2.317756249361775
2021-08-08 17:53:06,270   cls_loss = 0.0
2021-08-08 17:53:06,270   global_step = 2499
2021-08-08 17:53:06,270   loss = 3.2059262429489617
2021-08-08 17:53:06,270   rep_loss = 0.8881699914644126
2021-08-08 17:53:06,270 ***** Save model *****
2021-08-08 17:53:14,155 ***** Running evaluation *****
2021-08-08 17:53:14,155   Epoch = 0 iter 2549 step
2021-08-08 17:53:14,155   Num examples = 9815
2021-08-08 17:53:14,155   Batch size = 32
2021-08-08 17:53:14,156 ***** Eval results *****
2021-08-08 17:53:14,156   att_loss = 2.3168273487012496
2021-08-08 17:53:14,156   cls_loss = 0.0
2021-08-08 17:53:14,156   global_step = 2549
2021-08-08 17:53:14,156   loss = 3.204680925539869
2021-08-08 17:53:14,156   rep_loss = 0.8878535749679362
2021-08-08 17:53:14,157 ***** Save model *****
2021-08-08 17:53:21,420 ***** Running evaluation *****
2021-08-08 17:53:21,421   Epoch = 0 iter 2599 step
2021-08-08 17:53:21,421   Num examples = 9815
2021-08-08 17:53:21,421   Batch size = 32
2021-08-08 17:53:21,421 ***** Eval results *****
2021-08-08 17:53:21,421   att_loss = 2.3152371830277922
2021-08-08 17:53:21,422   cls_loss = 0.0
2021-08-08 17:53:21,422   global_step = 2599
2021-08-08 17:53:21,422   loss = 3.2027965219445576
2021-08-08 17:53:21,422   rep_loss = 0.8875593368756675
2021-08-08 17:53:21,422 ***** Save model *****
2021-08-08 17:53:27,287 ***** Running evaluation *****
2021-08-08 17:53:27,287   Epoch = 0 iter 2649 step
2021-08-08 17:53:27,287   Num examples = 9815
2021-08-08 17:53:27,288   Batch size = 32
2021-08-08 17:53:27,288 ***** Eval results *****
2021-08-08 17:53:27,288   att_loss = 2.3140206520581703
2021-08-08 17:53:27,288   cls_loss = 0.0
2021-08-08 17:53:27,288   global_step = 2649
2021-08-08 17:53:27,288   loss = 3.2012920153730544
2021-08-08 17:53:27,289   rep_loss = 0.887271361762328
2021-08-08 17:53:27,289 ***** Save model *****
2021-08-08 17:53:33,254 ***** Running evaluation *****
2021-08-08 17:53:33,255   Epoch = 0 iter 2699 step
2021-08-08 17:53:33,255   Num examples = 9815
2021-08-08 17:53:33,255   Batch size = 32
2021-08-08 17:53:33,256 ***** Eval results *****
2021-08-08 17:53:33,256   att_loss = 2.313990626082503
2021-08-08 17:53:33,256   cls_loss = 0.0
2021-08-08 17:53:33,256   global_step = 2699
2021-08-08 17:53:33,256   loss = 3.2010514396965704
2021-08-08 17:53:33,256   rep_loss = 0.8870608121344407
2021-08-08 17:53:33,256 ***** Save model *****
2021-08-08 17:53:39,196 ***** Running evaluation *****
2021-08-08 17:53:39,196   Epoch = 0 iter 2749 step
2021-08-08 17:53:39,196   Num examples = 9815
2021-08-08 17:53:39,196   Batch size = 32
2021-08-08 17:53:39,197 ***** Eval results *****
2021-08-08 17:53:39,197   att_loss = 2.312359526748005
2021-08-08 17:53:39,197   cls_loss = 0.0
2021-08-08 17:53:39,197   global_step = 2749
2021-08-08 17:53:39,197   loss = 3.199102119256384
2021-08-08 17:53:39,197   rep_loss = 0.8867425912074411
2021-08-08 17:53:39,198 ***** Save model *****
2021-08-08 17:53:45,055 ***** Running evaluation *****
2021-08-08 17:53:45,056   Epoch = 0 iter 2799 step
2021-08-08 17:53:45,056   Num examples = 9815
2021-08-08 17:53:45,056   Batch size = 32
2021-08-08 17:53:45,056 ***** Eval results *****
2021-08-08 17:53:45,057   att_loss = 2.3100839465292236
2021-08-08 17:53:45,057   cls_loss = 0.0
2021-08-08 17:53:45,057   global_step = 2799
2021-08-08 17:53:45,057   loss = 3.1964715833790005
2021-08-08 17:53:45,057   rep_loss = 0.8863876354017186
2021-08-08 17:53:45,057 ***** Save model *****
2021-08-08 17:53:50,940 ***** Running evaluation *****
2021-08-08 17:53:50,940   Epoch = 0 iter 2849 step
2021-08-08 17:53:50,940   Num examples = 9815
2021-08-08 17:53:50,940   Batch size = 32
2021-08-08 17:53:50,941 ***** Eval results *****
2021-08-08 17:53:50,941   att_loss = 2.3079719700366415
2021-08-08 17:53:50,941   cls_loss = 0.0
2021-08-08 17:53:50,941   global_step = 2849
2021-08-08 17:53:50,941   loss = 3.1940351781948864
2021-08-08 17:53:50,941   rep_loss = 0.8860632066309934
2021-08-08 17:53:50,941 ***** Save model *****
2021-08-08 17:53:56,804 ***** Running evaluation *****
2021-08-08 17:53:56,804   Epoch = 0 iter 2899 step
2021-08-08 17:53:56,804   Num examples = 9815
2021-08-08 17:53:56,804   Batch size = 32
2021-08-08 17:53:56,805 ***** Eval results *****
2021-08-08 17:53:56,805   att_loss = 2.3062192427367743
2021-08-08 17:53:56,805   cls_loss = 0.0
2021-08-08 17:53:56,805   global_step = 2899
2021-08-08 17:53:56,805   loss = 3.1920247660212864
2021-08-08 17:53:56,805   rep_loss = 0.8858055216396789
2021-08-08 17:53:56,806 ***** Save model *****
2021-08-08 17:54:02,682 ***** Running evaluation *****
2021-08-08 17:54:02,682   Epoch = 0 iter 2949 step
2021-08-08 17:54:02,682   Num examples = 9815
2021-08-08 17:54:02,682   Batch size = 32
2021-08-08 17:54:02,683 ***** Eval results *****
2021-08-08 17:54:02,683   att_loss = 2.305295274070903
2021-08-08 17:54:02,683   cls_loss = 0.0
2021-08-08 17:54:02,683   global_step = 2949
2021-08-08 17:54:02,683   loss = 3.1908681028048522
2021-08-08 17:54:02,683   rep_loss = 0.8855728272180632
2021-08-08 17:54:02,683 ***** Save model *****
2021-08-08 17:54:08,569 ***** Running evaluation *****
2021-08-08 17:54:08,569   Epoch = 0 iter 2999 step
2021-08-08 17:54:08,569   Num examples = 9815
2021-08-08 17:54:08,569   Batch size = 32
2021-08-08 17:54:08,570 ***** Eval results *****
2021-08-08 17:54:08,570   att_loss = 2.3056896081086835
2021-08-08 17:54:08,570   cls_loss = 0.0
2021-08-08 17:54:08,570   global_step = 2999
2021-08-08 17:54:08,570   loss = 3.1911331432268755
2021-08-08 17:54:08,570   rep_loss = 0.8854435337269533
2021-08-08 17:54:08,571 ***** Save model *****
2021-08-08 17:54:14,439 ***** Running evaluation *****
2021-08-08 17:54:14,439   Epoch = 0 iter 3049 step
2021-08-08 17:54:14,439   Num examples = 9815
2021-08-08 17:54:14,439   Batch size = 32
2021-08-08 17:54:14,440 ***** Eval results *****
2021-08-08 17:54:14,440   att_loss = 2.3037344150052146
2021-08-08 17:54:14,440   cls_loss = 0.0
2021-08-08 17:54:14,440   global_step = 3049
2021-08-08 17:54:14,440   loss = 3.188906370049111
2021-08-08 17:54:14,440   rep_loss = 0.885171953773217
2021-08-08 17:54:14,441 ***** Save model *****
2021-08-08 17:54:20,428 ***** Running evaluation *****
2021-08-08 17:54:20,428   Epoch = 0 iter 3099 step
2021-08-08 17:54:20,428   Num examples = 9815
2021-08-08 17:54:20,429   Batch size = 32
2021-08-08 17:54:20,429 ***** Eval results *****
2021-08-08 17:54:20,429   att_loss = 2.301784050314609
2021-08-08 17:54:20,429   cls_loss = 0.0
2021-08-08 17:54:20,429   global_step = 3099
2021-08-08 17:54:20,430   loss = 3.186658944495842
2021-08-08 17:54:20,430   rep_loss = 0.8848748930079893
2021-08-08 17:54:20,430 ***** Save model *****
2021-08-08 17:54:26,337 ***** Running evaluation *****
2021-08-08 17:54:26,337   Epoch = 0 iter 3149 step
2021-08-08 17:54:26,337   Num examples = 9815
2021-08-08 17:54:26,338   Batch size = 32
2021-08-08 17:54:26,338 ***** Eval results *****
2021-08-08 17:54:26,338   att_loss = 2.300527587320056
2021-08-08 17:54:26,338   cls_loss = 0.0
2021-08-08 17:54:26,338   global_step = 3149
2021-08-08 17:54:26,338   loss = 3.1851751260736245
2021-08-08 17:54:26,339   rep_loss = 0.884647537504313
2021-08-08 17:54:26,339 ***** Save model *****
2021-08-08 17:55:21,341 ***** Running evaluation *****
2021-08-08 17:55:21,341   Epoch = 0 iter 3199 step
2021-08-08 17:55:21,341   Num examples = 9815
2021-08-08 17:55:21,341   Batch size = 32
2021-08-08 17:55:21,342 ***** Eval results *****
2021-08-08 17:55:21,342   att_loss = 2.2993201004337167
2021-08-08 17:55:21,342   cls_loss = 0.0
2021-08-08 17:55:21,342   global_step = 3199
2021-08-08 17:55:21,343   loss = 3.1837150733073676
2021-08-08 17:55:21,343   rep_loss = 0.8843949716998175
2021-08-08 17:55:21,344 ***** Save model *****
2021-08-08 17:55:27,832 ***** Running evaluation *****
2021-08-08 17:55:27,833   Epoch = 0 iter 3249 step
2021-08-08 17:55:27,833   Num examples = 9815
2021-08-08 17:55:27,833   Batch size = 32
2021-08-08 17:55:27,834 ***** Eval results *****
2021-08-08 17:55:27,834   att_loss = 2.2972172857174766
2021-08-08 17:55:27,834   cls_loss = 0.0
2021-08-08 17:55:27,834   global_step = 3249
2021-08-08 17:55:27,834   loss = 3.1813088715278246
2021-08-08 17:55:27,834   rep_loss = 0.884091584471124
2021-08-08 17:55:27,834 ***** Save model *****
2021-08-08 17:55:35,997 ***** Running evaluation *****
2021-08-08 17:55:35,997   Epoch = 0 iter 3299 step
2021-08-08 17:55:35,997   Num examples = 9815
2021-08-08 17:55:35,997   Batch size = 32
2021-08-08 17:55:35,998 ***** Eval results *****
2021-08-08 17:55:35,998   att_loss = 2.2956361342791753
2021-08-08 17:55:35,998   cls_loss = 0.0
2021-08-08 17:55:35,998   global_step = 3299
2021-08-08 17:55:35,998   loss = 3.1794488731388904
2021-08-08 17:55:35,998   rep_loss = 0.8838127375046533
2021-08-08 17:55:35,998 ***** Save model *****
2021-08-08 17:55:41,922 ***** Running evaluation *****
2021-08-08 17:55:41,922   Epoch = 0 iter 3349 step
2021-08-08 17:55:41,922   Num examples = 9815
2021-08-08 17:55:41,922   Batch size = 32
2021-08-08 17:55:41,923 ***** Eval results *****
2021-08-08 17:55:41,923   att_loss = 2.2947171343088364
2021-08-08 17:55:41,923   cls_loss = 0.0
2021-08-08 17:55:41,923   global_step = 3349
2021-08-08 17:55:41,923   loss = 3.1782822425986947
2021-08-08 17:55:41,923   rep_loss = 0.8835651069906229
2021-08-08 17:55:41,923 ***** Save model *****
2021-08-08 17:55:47,822 ***** Running evaluation *****
2021-08-08 17:55:47,822   Epoch = 0 iter 3399 step
2021-08-08 17:55:47,822   Num examples = 9815
2021-08-08 17:55:47,822   Batch size = 32
2021-08-08 17:55:47,823 ***** Eval results *****
2021-08-08 17:55:47,823   att_loss = 2.292886274532207
2021-08-08 17:55:47,823   cls_loss = 0.0
2021-08-08 17:55:47,823   global_step = 3399
2021-08-08 17:55:47,823   loss = 3.176172139890827
2021-08-08 17:55:47,823   rep_loss = 0.8832858641311049
2021-08-08 17:55:47,824 ***** Save model *****
2021-08-08 17:55:53,698 ***** Running evaluation *****
2021-08-08 17:55:53,698   Epoch = 0 iter 3449 step
2021-08-08 17:55:53,698   Num examples = 9815
2021-08-08 17:55:53,698   Batch size = 32
2021-08-08 17:55:53,699 ***** Eval results *****
2021-08-08 17:55:53,699   att_loss = 2.2920582335940236
2021-08-08 17:55:53,699   cls_loss = 0.0
2021-08-08 17:55:53,699   global_step = 3449
2021-08-08 17:55:53,699   loss = 3.1751524954818233
2021-08-08 17:55:53,699   rep_loss = 0.8830942606089529
2021-08-08 17:55:53,700 ***** Save model *****
2021-08-08 17:55:59,587 ***** Running evaluation *****
2021-08-08 17:55:59,587   Epoch = 0 iter 3499 step
2021-08-08 17:55:59,587   Num examples = 9815
2021-08-08 17:55:59,587   Batch size = 32
2021-08-08 17:55:59,588 ***** Eval results *****
2021-08-08 17:55:59,588   att_loss = 2.2904592827068937
2021-08-08 17:55:59,588   cls_loss = 0.0
2021-08-08 17:55:59,588   global_step = 3499
2021-08-08 17:55:59,588   loss = 3.1732769048564875
2021-08-08 17:55:59,588   rep_loss = 0.8828176207868124
2021-08-08 17:55:59,589 ***** Save model *****
2021-08-08 17:56:05,450 ***** Running evaluation *****
2021-08-08 17:56:05,450   Epoch = 0 iter 3549 step
2021-08-08 17:56:05,450   Num examples = 9815
2021-08-08 17:56:05,450   Batch size = 32
2021-08-08 17:56:05,656 ***** Eval results *****
2021-08-08 17:56:05,656   att_loss = 2.288963219285784
2021-08-08 17:56:05,656   cls_loss = 0.0
2021-08-08 17:56:05,656   global_step = 3549
2021-08-08 17:56:05,656   loss = 3.171518371554622
2021-08-08 17:56:05,656   rep_loss = 0.8825551509252566
2021-08-08 17:56:05,656 ***** Save model *****
2021-08-08 17:56:11,517 ***** Running evaluation *****
2021-08-08 17:56:11,518   Epoch = 0 iter 3599 step
2021-08-08 17:56:11,518   Num examples = 9815
2021-08-08 17:56:11,518   Batch size = 32
2021-08-08 17:56:11,518 ***** Eval results *****
2021-08-08 17:56:11,519   att_loss = 2.2870773496346923
2021-08-08 17:56:11,519   cls_loss = 0.0
2021-08-08 17:56:11,519   global_step = 3599
2021-08-08 17:56:11,519   loss = 3.1693474532299883
2021-08-08 17:56:11,519   rep_loss = 0.8822701022206959
2021-08-08 17:56:11,519 ***** Save model *****
2021-08-08 17:56:17,394 ***** Running evaluation *****
2021-08-08 17:56:17,395   Epoch = 0 iter 3649 step
2021-08-08 17:56:17,395   Num examples = 9815
2021-08-08 17:56:17,395   Batch size = 32
2021-08-08 17:56:17,396 ***** Eval results *****
2021-08-08 17:56:17,396   att_loss = 2.286523170064789
2021-08-08 17:56:17,396   cls_loss = 0.0
2021-08-08 17:56:17,396   global_step = 3649
2021-08-08 17:56:17,396   loss = 3.1686095647793593
2021-08-08 17:56:17,396   rep_loss = 0.8820863933751406
2021-08-08 17:56:17,396 ***** Save model *****
2021-08-08 17:56:23,262 ***** Running evaluation *****
2021-08-08 17:56:23,262   Epoch = 0 iter 3699 step
2021-08-08 17:56:23,262   Num examples = 9815
2021-08-08 17:56:23,262   Batch size = 32
2021-08-08 17:56:23,263 ***** Eval results *****
2021-08-08 17:56:23,263   att_loss = 2.2856847072620914
2021-08-08 17:56:23,263   cls_loss = 0.0
2021-08-08 17:56:23,263   global_step = 3699
2021-08-08 17:56:23,263   loss = 3.1675743793725775
2021-08-08 17:56:23,263   rep_loss = 0.8818896708052749
2021-08-08 17:56:23,264 ***** Save model *****
2021-08-08 17:56:29,151 ***** Running evaluation *****
2021-08-08 17:56:29,151   Epoch = 0 iter 3749 step
2021-08-08 17:56:29,151   Num examples = 9815
2021-08-08 17:56:29,151   Batch size = 32
2021-08-08 17:56:29,152 ***** Eval results *****
2021-08-08 17:56:29,152   att_loss = 2.2837176613885264
2021-08-08 17:56:29,152   cls_loss = 0.0
2021-08-08 17:56:29,152   global_step = 3749
2021-08-08 17:56:29,152   loss = 3.1653399644263045
2021-08-08 17:56:29,153   rep_loss = 0.8816223017340757
2021-08-08 17:56:29,153 ***** Save model *****
2021-08-08 17:56:35,035 ***** Running evaluation *****
2021-08-08 17:56:35,035   Epoch = 0 iter 3799 step
2021-08-08 17:56:35,035   Num examples = 9815
2021-08-08 17:56:35,035   Batch size = 32
2021-08-08 17:56:35,036 ***** Eval results *****
2021-08-08 17:56:35,036   att_loss = 2.2815440332428283
2021-08-08 17:56:35,036   cls_loss = 0.0
2021-08-08 17:56:35,036   global_step = 3799
2021-08-08 17:56:35,036   loss = 3.162840118907759
2021-08-08 17:56:35,036   rep_loss = 0.881296084284249
2021-08-08 17:56:35,037 ***** Save model *****
2021-08-08 17:56:42,740 ***** Running evaluation *****
2021-08-08 17:56:42,740   Epoch = 0 iter 3849 step
2021-08-08 17:56:42,740   Num examples = 9815
2021-08-08 17:56:42,740   Batch size = 32
2021-08-08 17:56:42,741 ***** Eval results *****
2021-08-08 17:56:42,741   att_loss = 2.280495226646467
2021-08-08 17:56:42,741   cls_loss = 0.0
2021-08-08 17:56:42,741   global_step = 3849
2021-08-08 17:56:42,741   loss = 3.161594396939554
2021-08-08 17:56:42,741   rep_loss = 0.8810991689767984
2021-08-08 17:56:42,741 ***** Save model *****
2021-08-08 17:56:48,821 ***** Running evaluation *****
2021-08-08 17:56:48,822   Epoch = 0 iter 3899 step
2021-08-08 17:56:48,822   Num examples = 9815
2021-08-08 17:56:48,822   Batch size = 32
2021-08-08 17:56:48,822 ***** Eval results *****
2021-08-08 17:56:48,823   att_loss = 2.280242337939616
2021-08-08 17:56:48,823   cls_loss = 0.0
2021-08-08 17:56:48,823   global_step = 3899
2021-08-08 17:56:48,823   loss = 3.1611631263308295
2021-08-08 17:56:48,823   rep_loss = 0.8809207873363989
2021-08-08 17:56:48,823 ***** Save model *****
2021-08-08 17:56:54,795 ***** Running evaluation *****
2021-08-08 17:56:54,795   Epoch = 0 iter 3949 step
2021-08-08 17:56:54,795   Num examples = 9815
2021-08-08 17:56:54,796   Batch size = 32
2021-08-08 17:56:54,796 ***** Eval results *****
2021-08-08 17:56:54,796   att_loss = 2.2796441514028962
2021-08-08 17:56:54,796   cls_loss = 0.0
2021-08-08 17:56:54,796   global_step = 3949
2021-08-08 17:56:54,796   loss = 3.1603708777979493
2021-08-08 17:56:54,797   rep_loss = 0.8807267249762544
2021-08-08 17:56:54,797 ***** Save model *****
2021-08-08 17:57:00,732 ***** Running evaluation *****
2021-08-08 17:57:00,732   Epoch = 0 iter 3999 step
2021-08-08 17:57:00,732   Num examples = 9815
2021-08-08 17:57:00,732   Batch size = 32
2021-08-08 17:57:00,733 ***** Eval results *****
2021-08-08 17:57:00,733   att_loss = 2.278546109739677
2021-08-08 17:57:00,733   cls_loss = 0.0
2021-08-08 17:57:00,733   global_step = 3999
2021-08-08 17:57:00,733   loss = 3.1590712649370913
2021-08-08 17:57:00,733   rep_loss = 0.8805251538857843
2021-08-08 17:57:00,733 ***** Save model *****
2021-08-08 17:57:06,635 ***** Running evaluation *****
2021-08-08 17:57:06,635   Epoch = 0 iter 4049 step
2021-08-08 17:57:06,636   Num examples = 9815
2021-08-08 17:57:06,636   Batch size = 32
2021-08-08 17:57:06,636 ***** Eval results *****
2021-08-08 17:57:06,636   att_loss = 2.277132552445568
2021-08-08 17:57:06,636   cls_loss = 0.0
2021-08-08 17:57:06,637   global_step = 4049
2021-08-08 17:57:06,637   loss = 3.1574307921257216
2021-08-08 17:57:06,637   rep_loss = 0.8802982386644165
2021-08-08 17:57:06,637 ***** Save model *****
2021-08-08 17:57:12,515 ***** Running evaluation *****
2021-08-08 17:57:12,515   Epoch = 0 iter 4099 step
2021-08-08 17:57:12,515   Num examples = 9815
2021-08-08 17:57:12,515   Batch size = 32
2021-08-08 17:57:12,516 ***** Eval results *****
2021-08-08 17:57:12,516   att_loss = 2.2757806259994244
2021-08-08 17:57:12,516   cls_loss = 0.0
2021-08-08 17:57:12,516   global_step = 4099
2021-08-08 17:57:12,516   loss = 3.1558412225341237
2021-08-08 17:57:12,516   rep_loss = 0.8800605954877282
2021-08-08 17:57:12,517 ***** Save model *****
2021-08-08 17:57:18,414 ***** Running evaluation *****
2021-08-08 17:57:18,414   Epoch = 0 iter 4149 step
2021-08-08 17:57:18,414   Num examples = 9815
2021-08-08 17:57:18,414   Batch size = 32
2021-08-08 17:57:18,415 ***** Eval results *****
2021-08-08 17:57:18,415   att_loss = 2.275274930847326
2021-08-08 17:57:18,415   cls_loss = 0.0
2021-08-08 17:57:18,415   global_step = 4149
2021-08-08 17:57:18,415   loss = 3.1551878224743164
2021-08-08 17:57:18,415   rep_loss = 0.8799128905639042
2021-08-08 17:57:18,415 ***** Save model *****
2021-08-08 17:57:24,290 ***** Running evaluation *****
2021-08-08 17:57:24,290   Epoch = 0 iter 4199 step
2021-08-08 17:57:24,290   Num examples = 9815
2021-08-08 17:57:24,291   Batch size = 32
2021-08-08 17:57:24,291 ***** Eval results *****
2021-08-08 17:57:24,291   att_loss = 2.274199215535125
2021-08-08 17:57:24,291   cls_loss = 0.0
2021-08-08 17:57:24,292   global_step = 4199
2021-08-08 17:57:24,292   loss = 3.153932162539452
2021-08-08 17:57:24,292   rep_loss = 0.8797329459538997
2021-08-08 17:57:24,292 ***** Save model *****
2021-08-08 17:57:30,209 ***** Running evaluation *****
2021-08-08 17:57:30,209   Epoch = 0 iter 4249 step
2021-08-08 17:57:30,209   Num examples = 9815
2021-08-08 17:57:30,209   Batch size = 32
2021-08-08 17:57:30,210 ***** Eval results *****
2021-08-08 17:57:30,210   att_loss = 2.2725750733387
2021-08-08 17:57:30,210   cls_loss = 0.0
2021-08-08 17:57:30,210   global_step = 4249
2021-08-08 17:57:30,210   loss = 3.1520495364794483
2021-08-08 17:57:30,210   rep_loss = 0.8794744620746263
2021-08-08 17:57:30,211 ***** Save model *****
2021-08-08 17:57:36,129 ***** Running evaluation *****
2021-08-08 17:57:36,129   Epoch = 0 iter 4299 step
2021-08-08 17:57:36,130   Num examples = 9815
2021-08-08 17:57:36,130   Batch size = 32
2021-08-08 17:57:36,131 ***** Eval results *****
2021-08-08 17:57:36,131   att_loss = 2.271482154113466
2021-08-08 17:57:36,131   cls_loss = 0.0
2021-08-08 17:57:36,131   global_step = 4299
2021-08-08 17:57:36,131   loss = 3.150760043801527
2021-08-08 17:57:36,131   rep_loss = 0.8792778886343385
2021-08-08 17:57:36,131 ***** Save model *****
2021-08-08 17:57:42,046 ***** Running evaluation *****
2021-08-08 17:57:42,046   Epoch = 0 iter 4349 step
2021-08-08 17:57:42,046   Num examples = 9815
2021-08-08 17:57:42,046   Batch size = 32
2021-08-08 17:57:42,047 ***** Eval results *****
2021-08-08 17:57:42,047   att_loss = 2.2709194716433596
2021-08-08 17:57:42,047   cls_loss = 0.0
2021-08-08 17:57:42,047   global_step = 4349
2021-08-08 17:57:42,047   loss = 3.150036984406605
2021-08-08 17:57:42,047   rep_loss = 0.8791175115982892
2021-08-08 17:57:42,048 ***** Save model *****
2021-08-08 17:57:47,954 ***** Running evaluation *****
2021-08-08 17:57:47,954   Epoch = 0 iter 4399 step
2021-08-08 17:57:47,954   Num examples = 9815
2021-08-08 17:57:47,954   Batch size = 32
2021-08-08 17:57:47,955 ***** Eval results *****
2021-08-08 17:57:47,955   att_loss = 2.269955949502578
2021-08-08 17:57:47,955   cls_loss = 0.0
2021-08-08 17:57:47,955   global_step = 4399
2021-08-08 17:57:47,955   loss = 3.148880026291164
2021-08-08 17:57:47,955   rep_loss = 0.8789240757723672
2021-08-08 17:57:47,955 ***** Save model *****
2021-08-08 17:57:53,993 ***** Running evaluation *****
2021-08-08 17:57:53,993   Epoch = 0 iter 4449 step
2021-08-08 17:57:53,993   Num examples = 9815
2021-08-08 17:57:53,993   Batch size = 32
2021-08-08 17:57:53,994 ***** Eval results *****
2021-08-08 17:57:53,994   att_loss = 2.2683125526670875
2021-08-08 17:57:53,994   cls_loss = 0.0
2021-08-08 17:57:53,994   global_step = 4449
2021-08-08 17:57:53,994   loss = 3.147012371351328
2021-08-08 17:57:53,994   rep_loss = 0.8786998179071966
2021-08-08 17:57:53,995 ***** Save model *****
2021-08-08 17:58:00,005 ***** Running evaluation *****
2021-08-08 17:58:00,005   Epoch = 0 iter 4499 step
2021-08-08 17:58:00,006   Num examples = 9815
2021-08-08 17:58:00,006   Batch size = 32
2021-08-08 17:58:00,006 ***** Eval results *****
2021-08-08 17:58:00,006   att_loss = 2.267441708131694
2021-08-08 17:58:00,006   cls_loss = 0.0
2021-08-08 17:58:00,006   global_step = 4499
2021-08-08 17:58:00,007   loss = 3.1459657471824896
2021-08-08 17:58:00,007   rep_loss = 0.8785240382823872
2021-08-08 17:58:00,007 ***** Save model *****
2021-08-08 17:58:05,974 ***** Running evaluation *****
2021-08-08 17:58:05,974   Epoch = 0 iter 4549 step
2021-08-08 17:58:05,974   Num examples = 9815
2021-08-08 17:58:05,974   Batch size = 32
2021-08-08 17:58:05,975 ***** Eval results *****
2021-08-08 17:58:05,975   att_loss = 2.2666226758829024
2021-08-08 17:58:05,975   cls_loss = 0.0
2021-08-08 17:58:05,975   global_step = 4549
2021-08-08 17:58:05,975   loss = 3.1449844668948748
2021-08-08 17:58:05,975   rep_loss = 0.8783617902651124
2021-08-08 17:58:05,976 ***** Save model *****
2021-08-08 17:58:11,993 ***** Running evaluation *****
2021-08-08 17:58:11,994   Epoch = 0 iter 4599 step
2021-08-08 17:58:11,994   Num examples = 9815
2021-08-08 17:58:11,994   Batch size = 32
2021-08-08 17:58:11,994 ***** Eval results *****
2021-08-08 17:58:11,994   att_loss = 2.265758464792081
2021-08-08 17:58:11,995   cls_loss = 0.0
2021-08-08 17:58:11,995   global_step = 4599
2021-08-08 17:58:11,995   loss = 3.143954742781674
2021-08-08 17:58:11,995   rep_loss = 0.8781962771082888
2021-08-08 17:58:11,995 ***** Save model *****
2021-08-08 17:58:19,895 ***** Running evaluation *****
2021-08-08 17:58:19,895   Epoch = 0 iter 4649 step
2021-08-08 17:58:19,895   Num examples = 9815
2021-08-08 17:58:19,895   Batch size = 32
2021-08-08 17:58:19,896 ***** Eval results *****
2021-08-08 17:58:19,896   att_loss = 2.264189720230734
2021-08-08 17:58:19,896   cls_loss = 0.0
2021-08-08 17:58:19,896   global_step = 4649
2021-08-08 17:58:19,896   loss = 3.142144086412728
2021-08-08 17:58:19,896   rep_loss = 0.8779543651563169
2021-08-08 17:58:19,897 ***** Save model *****
2021-08-08 17:58:25,816 ***** Running evaluation *****
2021-08-08 17:58:25,816   Epoch = 0 iter 4699 step
2021-08-08 17:58:25,816   Num examples = 9815
2021-08-08 17:58:25,816   Batch size = 32
2021-08-08 17:58:25,817 ***** Eval results *****
2021-08-08 17:58:25,817   att_loss = 2.263619950766968
2021-08-08 17:58:25,817   cls_loss = 0.0
2021-08-08 17:58:25,817   global_step = 4699
2021-08-08 17:58:25,817   loss = 3.1414405900281803
2021-08-08 17:58:25,817   rep_loss = 0.877820638246449
2021-08-08 17:58:25,817 ***** Save model *****
2021-08-08 17:58:32,229 ***** Running evaluation *****
2021-08-08 17:58:32,229   Epoch = 0 iter 4749 step
2021-08-08 17:58:32,229   Num examples = 9815
2021-08-08 17:58:32,229   Batch size = 32
2021-08-08 17:58:32,230 ***** Eval results *****
2021-08-08 17:58:32,230   att_loss = 2.263043132172707
2021-08-08 17:58:32,230   cls_loss = 0.0
2021-08-08 17:58:32,230   global_step = 4749
2021-08-08 17:58:32,230   loss = 3.140735998784248
2021-08-08 17:58:32,230   rep_loss = 0.8776928655949109
2021-08-08 17:58:32,231 ***** Save model *****
2021-08-08 17:58:38,338 ***** Running evaluation *****
2021-08-08 17:58:38,339   Epoch = 0 iter 4799 step
2021-08-08 17:58:38,339   Num examples = 9815
2021-08-08 17:58:38,339   Batch size = 32
2021-08-08 17:58:38,340 ***** Eval results *****
2021-08-08 17:58:38,340   att_loss = 2.2618747901906566
2021-08-08 17:58:38,340   cls_loss = 0.0
2021-08-08 17:58:38,340   global_step = 4799
2021-08-08 17:58:38,340   loss = 3.1393655898993797
2021-08-08 17:58:38,340   rep_loss = 0.8774907987026851
2021-08-08 17:58:38,340 ***** Save model *****
2021-08-08 17:58:44,273 ***** Running evaluation *****
2021-08-08 17:58:44,273   Epoch = 0 iter 4849 step
2021-08-08 17:58:44,274   Num examples = 9815
2021-08-08 17:58:44,274   Batch size = 32
2021-08-08 17:58:44,274 ***** Eval results *****
2021-08-08 17:58:44,274   att_loss = 2.2610382796022224
2021-08-08 17:58:44,274   cls_loss = 0.0
2021-08-08 17:58:44,275   global_step = 4849
2021-08-08 17:58:44,275   loss = 3.138355751975735
2021-08-08 17:58:44,275   rep_loss = 0.8773174712426345
2021-08-08 17:58:44,275 ***** Save model *****
2021-08-08 17:58:50,220 ***** Running evaluation *****
2021-08-08 17:58:50,221   Epoch = 0 iter 4899 step
2021-08-08 17:58:50,221   Num examples = 9815
2021-08-08 17:58:50,221   Batch size = 32
2021-08-08 17:58:50,221 ***** Eval results *****
2021-08-08 17:58:50,221   att_loss = 2.2601436169494487
2021-08-08 17:58:50,222   cls_loss = 0.0
2021-08-08 17:58:50,222   global_step = 4899
2021-08-08 17:58:50,222   loss = 3.137283193892327
2021-08-08 17:58:50,222   rep_loss = 0.877139575835709
2021-08-08 17:58:50,222 ***** Save model *****
2021-08-08 17:58:56,588 ***** Running evaluation *****
2021-08-08 17:58:56,588   Epoch = 0 iter 4949 step
2021-08-08 17:58:56,588   Num examples = 9815
2021-08-08 17:58:56,588   Batch size = 32
2021-08-08 17:58:56,589 ***** Eval results *****
2021-08-08 17:58:56,589   att_loss = 2.259739493042273
2021-08-08 17:58:56,589   cls_loss = 0.0
2021-08-08 17:58:56,589   global_step = 4949
2021-08-08 17:58:56,589   loss = 3.136752565115672
2021-08-08 17:58:56,589   rep_loss = 0.8770130709412846
2021-08-08 17:58:56,589 ***** Save model *****
2021-08-08 17:59:02,769 ***** Running evaluation *****
2021-08-08 17:59:02,769   Epoch = 0 iter 4999 step
2021-08-08 17:59:02,769   Num examples = 9815
2021-08-08 17:59:02,769   Batch size = 32
2021-08-08 17:59:02,770 ***** Eval results *****
2021-08-08 17:59:02,770   att_loss = 2.259714280707666
2021-08-08 17:59:02,770   cls_loss = 0.0
2021-08-08 17:59:02,770   global_step = 4999
2021-08-08 17:59:02,770   loss = 3.136641587083591
2021-08-08 17:59:02,770   rep_loss = 0.8769273052909036
2021-08-08 17:59:02,770 ***** Save model *****
2021-08-08 17:59:08,798 ***** Running evaluation *****
2021-08-08 17:59:08,798   Epoch = 0 iter 5049 step
2021-08-08 17:59:08,798   Num examples = 9815
2021-08-08 17:59:08,798   Batch size = 32
2021-08-08 17:59:08,799 ***** Eval results *****
2021-08-08 17:59:08,799   att_loss = 2.2586511827313234
2021-08-08 17:59:08,799   cls_loss = 0.0
2021-08-08 17:59:08,799   global_step = 5049
2021-08-08 17:59:08,799   loss = 3.1354109640758
2021-08-08 17:59:08,800   rep_loss = 0.8767597802938105
2021-08-08 17:59:08,800 ***** Save model *****
2021-08-08 17:59:14,718 ***** Running evaluation *****
2021-08-08 17:59:14,718   Epoch = 0 iter 5099 step
2021-08-08 17:59:14,718   Num examples = 9815
2021-08-08 17:59:14,718   Batch size = 32
2021-08-08 17:59:14,719 ***** Eval results *****
2021-08-08 17:59:14,719   att_loss = 2.257680191482182
2021-08-08 17:59:14,719   cls_loss = 0.0
2021-08-08 17:59:14,719   global_step = 5099
2021-08-08 17:59:14,719   loss = 3.1342669557323406
2021-08-08 17:59:14,719   rep_loss = 0.8765867632097953
2021-08-08 17:59:14,719 ***** Save model *****
2021-08-08 17:59:22,690 ***** Running evaluation *****
2021-08-08 17:59:22,690   Epoch = 0 iter 5149 step
2021-08-08 17:59:22,690   Num examples = 9815
2021-08-08 17:59:22,690   Batch size = 32
2021-08-08 17:59:22,691 ***** Eval results *****
2021-08-08 17:59:22,691   att_loss = 2.2569977753276755
2021-08-08 17:59:22,691   cls_loss = 0.0
2021-08-08 17:59:22,691   global_step = 5149
2021-08-08 17:59:22,691   loss = 3.133444792924749
2021-08-08 17:59:22,691   rep_loss = 0.8764470164857806
2021-08-08 17:59:22,692 ***** Save model *****
2021-08-08 17:59:28,588 ***** Running evaluation *****
2021-08-08 17:59:28,588   Epoch = 0 iter 5199 step
2021-08-08 17:59:28,588   Num examples = 9815
2021-08-08 17:59:28,588   Batch size = 32
2021-08-08 17:59:28,589 ***** Eval results *****
2021-08-08 17:59:28,589   att_loss = 2.256188931637577
2021-08-08 17:59:28,589   cls_loss = 0.0
2021-08-08 17:59:28,589   global_step = 5199
2021-08-08 17:59:28,589   loss = 3.1324661359073427
2021-08-08 17:59:28,589   rep_loss = 0.8762772031003728
2021-08-08 17:59:28,589 ***** Save model *****
2021-08-08 17:59:34,514 ***** Running evaluation *****
2021-08-08 17:59:34,515   Epoch = 0 iter 5249 step
2021-08-08 17:59:34,515   Num examples = 9815
2021-08-08 17:59:34,515   Batch size = 32
2021-08-08 17:59:34,516 ***** Eval results *****
2021-08-08 17:59:34,516   att_loss = 2.2555410782480996
2021-08-08 17:59:34,516   cls_loss = 0.0
2021-08-08 17:59:34,516   global_step = 5249
2021-08-08 17:59:34,516   loss = 3.131665665482448
2021-08-08 17:59:34,516   rep_loss = 0.8761245861896494
2021-08-08 17:59:34,516 ***** Save model *****
2021-08-08 17:59:40,435 ***** Running evaluation *****
2021-08-08 17:59:40,435   Epoch = 0 iter 5299 step
2021-08-08 17:59:40,435   Num examples = 9815
2021-08-08 17:59:40,435   Batch size = 32
2021-08-08 17:59:40,436 ***** Eval results *****
2021-08-08 17:59:40,436   att_loss = 2.2546472267781503
2021-08-08 17:59:40,436   cls_loss = 0.0
2021-08-08 17:59:40,436   global_step = 5299
2021-08-08 17:59:40,436   loss = 3.1306131048053136
2021-08-08 17:59:40,436   rep_loss = 0.8759658770260664
2021-08-08 17:59:40,436 ***** Save model *****
2021-08-08 17:59:46,343 ***** Running evaluation *****
2021-08-08 17:59:46,344   Epoch = 0 iter 5349 step
2021-08-08 17:59:46,344   Num examples = 9815
2021-08-08 17:59:46,344   Batch size = 32
2021-08-08 17:59:46,345 ***** Eval results *****
2021-08-08 17:59:46,345   att_loss = 2.2540372718403465
2021-08-08 17:59:46,345   cls_loss = 0.0
2021-08-08 17:59:46,345   global_step = 5349
2021-08-08 17:59:46,345   loss = 3.1298684403035635
2021-08-08 17:59:46,345   rep_loss = 0.8758311675160505
2021-08-08 17:59:46,345 ***** Save model *****
2021-08-08 17:59:52,255 ***** Running evaluation *****
2021-08-08 17:59:52,255   Epoch = 0 iter 5399 step
2021-08-08 17:59:52,255   Num examples = 9815
2021-08-08 17:59:52,255   Batch size = 32
2021-08-08 17:59:52,256 ***** Eval results *****
2021-08-08 17:59:52,256   att_loss = 2.2530310688515156
2021-08-08 17:59:52,256   cls_loss = 0.0
2021-08-08 17:59:52,256   global_step = 5399
2021-08-08 17:59:52,257   loss = 3.1287259408159818
2021-08-08 17:59:52,257   rep_loss = 0.8756948709487915
2021-08-08 17:59:52,257 ***** Save model *****
2021-08-08 17:59:58,646 ***** Running evaluation *****
2021-08-08 17:59:58,646   Epoch = 0 iter 5449 step
2021-08-08 17:59:58,646   Num examples = 9815
2021-08-08 17:59:58,646   Batch size = 32
2021-08-08 17:59:58,647 ***** Eval results *****
2021-08-08 17:59:58,647   att_loss = 2.2522961960995738
2021-08-08 17:59:58,647   cls_loss = 0.0
2021-08-08 17:59:58,647   global_step = 5449
2021-08-08 17:59:58,647   loss = 3.1278315959534178
2021-08-08 17:59:58,647   rep_loss = 0.8755353988474893
2021-08-08 17:59:58,647 ***** Save model *****
2021-08-08 18:00:04,651 ***** Running evaluation *****
2021-08-08 18:00:04,651   Epoch = 0 iter 5499 step
2021-08-08 18:00:04,652   Num examples = 9815
2021-08-08 18:00:04,652   Batch size = 32
2021-08-08 18:00:04,652 ***** Eval results *****
2021-08-08 18:00:04,652   att_loss = 2.2517086117457077
2021-08-08 18:00:04,652   cls_loss = 0.0
2021-08-08 18:00:04,652   global_step = 5499
2021-08-08 18:00:04,653   loss = 3.127099495667678
2021-08-08 18:00:04,653   rep_loss = 0.8753908829789614
2021-08-08 18:00:04,653 ***** Save model *****
2021-08-08 18:00:10,640 ***** Running evaluation *****
2021-08-08 18:00:10,640   Epoch = 0 iter 5549 step
2021-08-08 18:00:10,641   Num examples = 9815
2021-08-08 18:00:10,641   Batch size = 32
2021-08-08 18:00:10,642 ***** Eval results *****
2021-08-08 18:00:10,642   att_loss = 2.2509432895739114
2021-08-08 18:00:10,642   cls_loss = 0.0
2021-08-08 18:00:10,642   global_step = 5549
2021-08-08 18:00:10,642   loss = 3.1261997386601017
2021-08-08 18:00:10,642   rep_loss = 0.8752564480120394
2021-08-08 18:00:10,643 ***** Save model *****
2021-08-08 18:00:16,594 ***** Running evaluation *****
2021-08-08 18:00:16,595   Epoch = 0 iter 5599 step
2021-08-08 18:00:16,595   Num examples = 9815
2021-08-08 18:00:16,595   Batch size = 32
2021-08-08 18:00:16,595 ***** Eval results *****
2021-08-08 18:00:16,595   att_loss = 2.250281887451311
2021-08-08 18:00:16,595   cls_loss = 0.0
2021-08-08 18:00:16,596   global_step = 5599
2021-08-08 18:00:16,596   loss = 3.125389447785378
2021-08-08 18:00:16,596   rep_loss = 0.8751075590672419
2021-08-08 18:00:16,596 ***** Save model *****
2021-08-08 18:00:22,538 ***** Running evaluation *****
2021-08-08 18:00:22,538   Epoch = 0 iter 5649 step
2021-08-08 18:00:22,538   Num examples = 9815
2021-08-08 18:00:22,539   Batch size = 32
2021-08-08 18:00:22,539 ***** Eval results *****
2021-08-08 18:00:22,539   att_loss = 2.2493986727854156
2021-08-08 18:00:22,539   cls_loss = 0.0
2021-08-08 18:00:22,539   global_step = 5649
2021-08-08 18:00:22,540   loss = 3.124340753678538
2021-08-08 18:00:22,540   rep_loss = 0.8749420796586129
2021-08-08 18:00:22,540 ***** Save model *****
2021-08-08 18:00:28,483 ***** Running evaluation *****
2021-08-08 18:00:28,483   Epoch = 0 iter 5699 step
2021-08-08 18:00:28,483   Num examples = 9815
2021-08-08 18:00:28,483   Batch size = 32
2021-08-08 18:00:28,484 ***** Eval results *****
2021-08-08 18:00:28,484   att_loss = 2.2485631642623582
2021-08-08 18:00:28,484   cls_loss = 0.0
2021-08-08 18:00:28,484   global_step = 5699
2021-08-08 18:00:28,484   loss = 3.1233388647988796
2021-08-08 18:00:28,484   rep_loss = 0.87477569916642
2021-08-08 18:00:28,484 ***** Save model *****
2021-08-08 18:00:34,417 ***** Running evaluation *****
2021-08-08 18:00:34,417   Epoch = 0 iter 5749 step
2021-08-08 18:00:34,417   Num examples = 9815
2021-08-08 18:00:34,417   Batch size = 32
2021-08-08 18:00:34,418 ***** Eval results *****
2021-08-08 18:00:34,418   att_loss = 2.2478835987907884
2021-08-08 18:00:34,418   cls_loss = 0.0
2021-08-08 18:00:34,418   global_step = 5749
2021-08-08 18:00:34,418   loss = 3.1225317066535676
2021-08-08 18:00:34,418   rep_loss = 0.8746481064942261
2021-08-08 18:00:34,419 ***** Save model *****
2021-08-08 18:00:40,342 ***** Running evaluation *****
2021-08-08 18:00:40,342   Epoch = 0 iter 5799 step
2021-08-08 18:00:40,342   Num examples = 9815
2021-08-08 18:00:40,343   Batch size = 32
2021-08-08 18:00:40,343 ***** Eval results *****
2021-08-08 18:00:40,343   att_loss = 2.246786321601203
2021-08-08 18:00:40,343   cls_loss = 0.0
2021-08-08 18:00:40,343   global_step = 5799
2021-08-08 18:00:40,343   loss = 3.121275404318178
2021-08-08 18:00:40,344   rep_loss = 0.8744890814013349
2021-08-08 18:00:40,344 ***** Save model *****
2021-08-08 18:00:46,268 ***** Running evaluation *****
2021-08-08 18:00:46,268   Epoch = 0 iter 5849 step
2021-08-08 18:00:46,269   Num examples = 9815
2021-08-08 18:00:46,269   Batch size = 32
2021-08-08 18:00:46,269 ***** Eval results *****
2021-08-08 18:00:46,269   att_loss = 2.2463735141679146
2021-08-08 18:00:46,269   cls_loss = 0.0
2021-08-08 18:00:46,270   global_step = 5849
2021-08-08 18:00:46,270   loss = 3.1207377825420846
2021-08-08 18:00:46,270   rep_loss = 0.8743642669882524
2021-08-08 18:00:46,270 ***** Save model *****
2021-08-08 18:00:52,189 ***** Running evaluation *****
2021-08-08 18:00:52,189   Epoch = 0 iter 5899 step
2021-08-08 18:00:52,189   Num examples = 9815
2021-08-08 18:00:52,189   Batch size = 32
2021-08-08 18:00:52,190 ***** Eval results *****
2021-08-08 18:00:52,190   att_loss = 2.245662918452955
2021-08-08 18:00:52,190   cls_loss = 0.0
2021-08-08 18:00:52,190   global_step = 5899
2021-08-08 18:00:52,190   loss = 3.1199047775627458
2021-08-08 18:00:52,190   rep_loss = 0.8742418579073917
2021-08-08 18:00:52,190 ***** Save model *****
2021-08-08 18:01:00,208 ***** Running evaluation *****
2021-08-08 18:01:00,208   Epoch = 0 iter 5949 step
2021-08-08 18:01:00,208   Num examples = 9815
2021-08-08 18:01:00,208   Batch size = 32
2021-08-08 18:01:00,209 ***** Eval results *****
2021-08-08 18:01:00,209   att_loss = 2.24488919786013
2021-08-08 18:01:00,209   cls_loss = 0.0
2021-08-08 18:01:00,209   global_step = 5949
2021-08-08 18:01:00,209   loss = 3.1189836584473682
2021-08-08 18:01:00,209   rep_loss = 0.8740944594450409
2021-08-08 18:01:00,209 ***** Save model *****
2021-08-08 18:01:06,408 ***** Running evaluation *****
2021-08-08 18:01:06,408   Epoch = 0 iter 5999 step
2021-08-08 18:01:06,408   Num examples = 9815
2021-08-08 18:01:06,408   Batch size = 32
2021-08-08 18:01:06,409 ***** Eval results *****
2021-08-08 18:01:06,409   att_loss = 2.244479528525051
2021-08-08 18:01:06,409   cls_loss = 0.0
2021-08-08 18:01:06,409   global_step = 5999
2021-08-08 18:01:06,409   loss = 3.1184615512195477
2021-08-08 18:01:06,410   rep_loss = 0.8739820215419484
2021-08-08 18:01:06,410 ***** Save model *****
2021-08-08 18:01:12,327 ***** Running evaluation *****
2021-08-08 18:01:12,327   Epoch = 0 iter 6049 step
2021-08-08 18:01:12,327   Num examples = 9815
2021-08-08 18:01:12,327   Batch size = 32
2021-08-08 18:01:12,328 ***** Eval results *****
2021-08-08 18:01:12,328   att_loss = 2.2431578778061794
2021-08-08 18:01:12,328   cls_loss = 0.0
2021-08-08 18:01:12,328   global_step = 6049
2021-08-08 18:01:12,328   loss = 3.1169668257305454
2021-08-08 18:01:12,328   rep_loss = 0.8738089466828082
2021-08-08 18:01:12,329 ***** Save model *****
2021-08-08 18:01:18,577 ***** Running evaluation *****
2021-08-08 18:01:18,577   Epoch = 0 iter 6099 step
2021-08-08 18:01:18,577   Num examples = 9815
2021-08-08 18:01:18,577   Batch size = 32
2021-08-08 18:01:18,578 ***** Eval results *****
2021-08-08 18:01:18,578   att_loss = 2.2430783809843873
2021-08-08 18:01:18,578   cls_loss = 0.0
2021-08-08 18:01:18,578   global_step = 6099
2021-08-08 18:01:18,578   loss = 3.116799677252906
2021-08-08 18:01:18,578   rep_loss = 0.8737212950957761
2021-08-08 18:01:18,579 ***** Save model *****
2021-08-08 18:01:24,616 ***** Running evaluation *****
2021-08-08 18:01:24,616   Epoch = 0 iter 6149 step
2021-08-08 18:01:24,616   Num examples = 9815
2021-08-08 18:01:24,616   Batch size = 32
2021-08-08 18:01:24,617 ***** Eval results *****
2021-08-08 18:01:24,617   att_loss = 2.24262982049518
2021-08-08 18:01:24,617   cls_loss = 0.0
2021-08-08 18:01:24,617   global_step = 6149
2021-08-08 18:01:24,617   loss = 3.1162342165597843
2021-08-08 18:01:24,617   rep_loss = 0.873604394930478
2021-08-08 18:01:24,617 ***** Save model *****
2021-08-08 18:01:30,547 ***** Running evaluation *****
2021-08-08 18:01:30,547   Epoch = 0 iter 6199 step
2021-08-08 18:01:30,547   Num examples = 9815
2021-08-08 18:01:30,547   Batch size = 32
2021-08-08 18:01:30,548 ***** Eval results *****
2021-08-08 18:01:30,548   att_loss = 2.241911710664368
2021-08-08 18:01:30,548   cls_loss = 0.0
2021-08-08 18:01:30,549   global_step = 6199
2021-08-08 18:01:30,549   loss = 3.1153887086653826
2021-08-08 18:01:30,549   rep_loss = 0.8734769968952666
2021-08-08 18:01:30,549 ***** Save model *****
2021-08-08 18:01:36,498 ***** Running evaluation *****
2021-08-08 18:01:36,499   Epoch = 0 iter 6249 step
2021-08-08 18:01:36,499   Num examples = 9815
2021-08-08 18:01:36,499   Batch size = 32
2021-08-08 18:01:36,500 ***** Eval results *****
2021-08-08 18:01:36,500   att_loss = 2.241425702926311
2021-08-08 18:01:36,500   cls_loss = 0.0
2021-08-08 18:01:36,500   global_step = 6249
2021-08-08 18:01:36,500   loss = 3.1148092427011833
2021-08-08 18:01:36,500   rep_loss = 0.8733835386779714
2021-08-08 18:01:36,500 ***** Save model *****
2021-08-08 18:01:42,448 ***** Running evaluation *****
2021-08-08 18:01:42,448   Epoch = 0 iter 6299 step
2021-08-08 18:01:42,448   Num examples = 9815
2021-08-08 18:01:42,448   Batch size = 32
2021-08-08 18:01:42,449 ***** Eval results *****
2021-08-08 18:01:42,449   att_loss = 2.2414046663919276
2021-08-08 18:01:42,449   cls_loss = 0.0
2021-08-08 18:01:42,449   global_step = 6299
2021-08-08 18:01:42,449   loss = 3.1147142090595303
2021-08-08 18:01:42,449   rep_loss = 0.8733095415604836
2021-08-08 18:01:42,449 ***** Save model *****
2021-08-08 18:01:48,407 ***** Running evaluation *****
2021-08-08 18:01:48,407   Epoch = 0 iter 6349 step
2021-08-08 18:01:48,407   Num examples = 9815
2021-08-08 18:01:48,407   Batch size = 32
2021-08-08 18:01:48,408 ***** Eval results *****
2021-08-08 18:01:48,408   att_loss = 2.240642346665885
2021-08-08 18:01:48,408   cls_loss = 0.0
2021-08-08 18:01:48,408   global_step = 6349
2021-08-08 18:01:48,408   loss = 3.1138135051892526
2021-08-08 18:01:48,408   rep_loss = 0.8731711575470115
2021-08-08 18:01:48,408 ***** Save model *****
2021-08-08 18:01:54,364 ***** Running evaluation *****
2021-08-08 18:01:54,364   Epoch = 0 iter 6399 step
2021-08-08 18:01:54,364   Num examples = 9815
2021-08-08 18:01:54,364   Batch size = 32
2021-08-08 18:01:54,365 ***** Eval results *****
2021-08-08 18:01:54,365   att_loss = 2.239975400260881
2021-08-08 18:01:54,365   cls_loss = 0.0
2021-08-08 18:01:54,365   global_step = 6399
2021-08-08 18:01:54,365   loss = 3.113012166670811
2021-08-08 18:01:54,365   rep_loss = 0.8730367653480562
2021-08-08 18:01:54,365 ***** Save model *****
2021-08-08 18:02:00,317 ***** Running evaluation *****
2021-08-08 18:02:00,318   Epoch = 0 iter 6449 step
2021-08-08 18:02:00,318   Num examples = 9815
2021-08-08 18:02:00,318   Batch size = 32
2021-08-08 18:02:00,319 ***** Eval results *****
2021-08-08 18:02:00,319   att_loss = 2.2394812492312046
2021-08-08 18:02:00,319   cls_loss = 0.0
2021-08-08 18:02:00,319   global_step = 6449
2021-08-08 18:02:00,319   loss = 3.1123845739167946
2021-08-08 18:02:00,319   rep_loss = 0.8729033236596765
2021-08-08 18:02:00,319 ***** Save model *****
2021-08-08 18:02:08,478 ***** Running evaluation *****
2021-08-08 18:02:08,479   Epoch = 0 iter 6499 step
2021-08-08 18:02:08,479   Num examples = 9815
2021-08-08 18:02:08,479   Batch size = 32
2021-08-08 18:02:08,480 ***** Eval results *****
2021-08-08 18:02:08,480   att_loss = 2.2386696863438575
2021-08-08 18:02:08,480   cls_loss = 0.0
2021-08-08 18:02:08,480   global_step = 6499
2021-08-08 18:02:08,480   loss = 3.1114396561767745
2021-08-08 18:02:08,480   rep_loss = 0.8727699688240682
2021-08-08 18:02:08,480 ***** Save model *****
2021-08-08 18:02:14,521 ***** Running evaluation *****
2021-08-08 18:02:14,521   Epoch = 0 iter 6549 step
2021-08-08 18:02:14,521   Num examples = 9815
2021-08-08 18:02:14,521   Batch size = 32
2021-08-08 18:02:14,522 ***** Eval results *****
2021-08-08 18:02:14,522   att_loss = 2.2379679633453797
2021-08-08 18:02:14,522   cls_loss = 0.0
2021-08-08 18:02:14,522   global_step = 6549
2021-08-08 18:02:14,522   loss = 3.1106038508587783
2021-08-08 18:02:14,522   rep_loss = 0.8726358865577586
2021-08-08 18:02:14,522 ***** Save model *****
2021-08-08 18:02:20,478 ***** Running evaluation *****
2021-08-08 18:02:20,478   Epoch = 0 iter 6599 step
2021-08-08 18:02:20,478   Num examples = 9815
2021-08-08 18:02:20,478   Batch size = 32
2021-08-08 18:02:20,479 ***** Eval results *****
2021-08-08 18:02:20,479   att_loss = 2.2374554959404267
2021-08-08 18:02:20,479   cls_loss = 0.0
2021-08-08 18:02:20,479   global_step = 6599
2021-08-08 18:02:20,479   loss = 3.1099652700630855
2021-08-08 18:02:20,479   rep_loss = 0.8725097731020003
2021-08-08 18:02:20,479 ***** Save model *****
2021-08-08 18:02:26,396 ***** Running evaluation *****
2021-08-08 18:02:26,396   Epoch = 0 iter 6649 step
2021-08-08 18:02:26,396   Num examples = 9815
2021-08-08 18:02:26,396   Batch size = 32
2021-08-08 18:02:26,397 ***** Eval results *****
2021-08-08 18:02:26,397   att_loss = 2.2373263082713684
2021-08-08 18:02:26,397   cls_loss = 0.0
2021-08-08 18:02:26,397   global_step = 6649
2021-08-08 18:02:26,397   loss = 3.109757014048119
2021-08-08 18:02:26,397   rep_loss = 0.8724307047816968
2021-08-08 18:02:26,398 ***** Save model *****
2021-08-08 18:02:32,324 ***** Running evaluation *****
2021-08-08 18:02:32,324   Epoch = 0 iter 6699 step
2021-08-08 18:02:32,325   Num examples = 9815
2021-08-08 18:02:32,325   Batch size = 32
2021-08-08 18:02:32,325 ***** Eval results *****
2021-08-08 18:02:32,325   att_loss = 2.236453299612087
2021-08-08 18:02:32,325   cls_loss = 0.0
2021-08-08 18:02:32,326   global_step = 6699
2021-08-08 18:02:32,326   loss = 3.1087390626966287
2021-08-08 18:02:32,326   rep_loss = 0.8722857622570699
2021-08-08 18:02:32,326 ***** Save model *****
2021-08-08 18:02:38,232 ***** Running evaluation *****
2021-08-08 18:02:38,232   Epoch = 0 iter 6749 step
2021-08-08 18:02:38,232   Num examples = 9815
2021-08-08 18:02:38,232   Batch size = 32
2021-08-08 18:02:38,233 ***** Eval results *****
2021-08-08 18:02:38,233   att_loss = 2.236107591965195
2021-08-08 18:02:38,233   cls_loss = 0.0
2021-08-08 18:02:38,233   global_step = 6749
2021-08-08 18:02:38,233   loss = 3.1082846726960014
2021-08-08 18:02:38,233   rep_loss = 0.8721770800772659
2021-08-08 18:02:38,234 ***** Save model *****
2021-08-08 18:02:44,143 ***** Running evaluation *****
2021-08-08 18:02:44,143   Epoch = 0 iter 6799 step
2021-08-08 18:02:44,143   Num examples = 9815
2021-08-08 18:02:44,143   Batch size = 32
2021-08-08 18:02:44,144 ***** Eval results *****
2021-08-08 18:02:44,144   att_loss = 2.235261386011362
2021-08-08 18:02:44,144   cls_loss = 0.0
2021-08-08 18:02:44,144   global_step = 6799
2021-08-08 18:02:44,144   loss = 3.1072894400543176
2021-08-08 18:02:44,144   rep_loss = 0.8720280532188881
2021-08-08 18:02:44,144 ***** Save model *****
2021-08-08 18:02:50,034 ***** Running evaluation *****
2021-08-08 18:02:50,034   Epoch = 0 iter 6849 step
2021-08-08 18:02:50,034   Num examples = 9815
2021-08-08 18:02:50,034   Batch size = 32
2021-08-08 18:02:50,035 ***** Eval results *****
2021-08-08 18:02:50,035   att_loss = 2.234768747099822
2021-08-08 18:02:50,035   cls_loss = 0.0
2021-08-08 18:02:50,035   global_step = 6849
2021-08-08 18:02:50,035   loss = 3.106696393302208
2021-08-08 18:02:50,035   rep_loss = 0.8719276454452529
2021-08-08 18:02:50,035 ***** Save model *****
2021-08-08 18:02:55,933 ***** Running evaluation *****
2021-08-08 18:02:55,934   Epoch = 0 iter 6899 step
2021-08-08 18:02:55,934   Num examples = 9815
2021-08-08 18:02:55,934   Batch size = 32
2021-08-08 18:02:55,935 ***** Eval results *****
2021-08-08 18:02:55,935   att_loss = 2.2342292824799226
2021-08-08 18:02:55,935   cls_loss = 0.0
2021-08-08 18:02:55,935   global_step = 6899
2021-08-08 18:02:55,935   loss = 3.106045693275738
2021-08-08 18:02:55,935   rep_loss = 0.8718164099491339
2021-08-08 18:02:55,935 ***** Save model *****
2021-08-08 18:03:01,851 ***** Running evaluation *****
2021-08-08 18:03:01,851   Epoch = 0 iter 6949 step
2021-08-08 18:03:01,851   Num examples = 9815
2021-08-08 18:03:01,851   Batch size = 32
2021-08-08 18:03:01,852 ***** Eval results *****
2021-08-08 18:03:01,852   att_loss = 2.233603149356559
2021-08-08 18:03:01,852   cls_loss = 0.0
2021-08-08 18:03:01,852   global_step = 6949
2021-08-08 18:03:01,852   loss = 3.105299853304167
2021-08-08 18:03:01,852   rep_loss = 0.8716967030469763
2021-08-08 18:03:01,852 ***** Save model *****
2021-08-08 18:03:07,745 ***** Running evaluation *****
2021-08-08 18:03:07,745   Epoch = 0 iter 6999 step
2021-08-08 18:03:07,745   Num examples = 9815
2021-08-08 18:03:07,745   Batch size = 32
2021-08-08 18:03:07,746 ***** Eval results *****
2021-08-08 18:03:07,746   att_loss = 2.2328869823149775
2021-08-08 18:03:07,746   cls_loss = 0.0
2021-08-08 18:03:07,746   global_step = 6999
2021-08-08 18:03:07,746   loss = 3.104453687004267
2021-08-08 18:03:07,746   rep_loss = 0.8715667038376731
2021-08-08 18:03:07,746 ***** Save model *****
2021-08-08 18:03:13,903 ***** Running evaluation *****
2021-08-08 18:03:13,903   Epoch = 0 iter 7049 step
2021-08-08 18:03:13,903   Num examples = 9815
2021-08-08 18:03:13,903   Batch size = 32
2021-08-08 18:03:13,904 ***** Eval results *****
2021-08-08 18:03:13,904   att_loss = 2.232429826626593
2021-08-08 18:03:13,904   cls_loss = 0.0
2021-08-08 18:03:13,904   global_step = 7049
2021-08-08 18:03:13,904   loss = 3.1038781507486384
2021-08-08 18:03:13,904   rep_loss = 0.8714483231834561
2021-08-08 18:03:13,904 ***** Save model *****
2021-08-08 18:03:19,965 ***** Running evaluation *****
2021-08-08 18:03:19,965   Epoch = 0 iter 7099 step
2021-08-08 18:03:19,965   Num examples = 9815
2021-08-08 18:03:19,965   Batch size = 32
2021-08-08 18:03:19,967 ***** Eval results *****
2021-08-08 18:03:19,967   att_loss = 2.2317104130870002
2021-08-08 18:03:19,967   cls_loss = 0.0
2021-08-08 18:03:19,967   global_step = 7099
2021-08-08 18:03:19,967   loss = 3.1030353758600095
2021-08-08 18:03:19,967   rep_loss = 0.8713249618746156
2021-08-08 18:03:19,967 ***** Save model *****
2021-08-08 18:03:25,937 ***** Running evaluation *****
2021-08-08 18:03:25,937   Epoch = 0 iter 7149 step
2021-08-08 18:03:25,937   Num examples = 9815
2021-08-08 18:03:25,937   Batch size = 32
2021-08-08 18:03:25,938 ***** Eval results *****
2021-08-08 18:03:25,938   att_loss = 2.231226236974631
2021-08-08 18:03:25,938   cls_loss = 0.0
2021-08-08 18:03:25,938   global_step = 7149
2021-08-08 18:03:25,938   loss = 3.102451033584887
2021-08-08 18:03:25,938   rep_loss = 0.8712247958348704
2021-08-08 18:03:25,939 ***** Save model *****
2021-08-08 18:03:31,856 ***** Running evaluation *****
2021-08-08 18:03:31,857   Epoch = 0 iter 7199 step
2021-08-08 18:03:31,857   Num examples = 9815
2021-08-08 18:03:31,857   Batch size = 32
2021-08-08 18:03:31,858 ***** Eval results *****
2021-08-08 18:03:31,858   att_loss = 2.231034253633491
2021-08-08 18:03:31,858   cls_loss = 0.0
2021-08-08 18:03:31,858   global_step = 7199
2021-08-08 18:03:31,858   loss = 3.102172282225954
2021-08-08 18:03:31,858   rep_loss = 0.8711380277313877
2021-08-08 18:03:31,858 ***** Save model *****
2021-08-08 18:03:39,788 ***** Running evaluation *****
2021-08-08 18:03:39,789   Epoch = 0 iter 7249 step
2021-08-08 18:03:39,789   Num examples = 9815
2021-08-08 18:03:39,789   Batch size = 32
2021-08-08 18:03:39,789 ***** Eval results *****
2021-08-08 18:03:39,790   att_loss = 2.230571367079874
2021-08-08 18:03:39,790   cls_loss = 0.0
2021-08-08 18:03:39,790   global_step = 7249
2021-08-08 18:03:39,790   loss = 3.1016026097078098
2021-08-08 18:03:39,790   rep_loss = 0.8710312417234646
2021-08-08 18:03:39,790 ***** Save model *****
2021-08-08 18:03:45,685 ***** Running evaluation *****
2021-08-08 18:03:45,685   Epoch = 0 iter 7299 step
2021-08-08 18:03:45,685   Num examples = 9815
2021-08-08 18:03:45,685   Batch size = 32
2021-08-08 18:03:45,686 ***** Eval results *****
2021-08-08 18:03:45,686   att_loss = 2.2297141105446983
2021-08-08 18:03:45,686   cls_loss = 0.0
2021-08-08 18:03:45,686   global_step = 7299
2021-08-08 18:03:45,686   loss = 3.100598315242742
2021-08-08 18:03:45,686   rep_loss = 0.8708842038895956
2021-08-08 18:03:45,687 ***** Save model *****
2021-08-08 18:03:51,851 ***** Running evaluation *****
2021-08-08 18:03:51,851   Epoch = 0 iter 7349 step
2021-08-08 18:03:51,851   Num examples = 9815
2021-08-08 18:03:51,851   Batch size = 32
2021-08-08 18:03:51,852 ***** Eval results *****
2021-08-08 18:03:51,852   att_loss = 2.2292754550355847
2021-08-08 18:03:51,852   cls_loss = 0.0
2021-08-08 18:03:51,852   global_step = 7349
2021-08-08 18:03:51,852   loss = 3.1000589245046006
2021-08-08 18:03:51,852   rep_loss = 0.8707834686498477
2021-08-08 18:03:51,853 ***** Save model *****
2021-08-08 18:03:58,945 ***** Running evaluation *****
2021-08-08 18:03:58,946   Epoch = 0 iter 7399 step
2021-08-08 18:03:58,946   Num examples = 9815
2021-08-08 18:03:58,946   Batch size = 32
2021-08-08 18:03:58,947 ***** Eval results *****
2021-08-08 18:03:58,947   att_loss = 2.228531745563151
2021-08-08 18:03:58,947   cls_loss = 0.0
2021-08-08 18:03:58,947   global_step = 7399
2021-08-08 18:03:58,947   loss = 3.0991874681097187
2021-08-08 18:03:58,947   rep_loss = 0.8706557216443215
2021-08-08 18:03:58,947 ***** Save model *****
2021-08-08 18:04:04,835 ***** Running evaluation *****
2021-08-08 18:04:04,835   Epoch = 0 iter 7449 step
2021-08-08 18:04:04,835   Num examples = 9815
2021-08-08 18:04:04,835   Batch size = 32
2021-08-08 18:04:04,836 ***** Eval results *****
2021-08-08 18:04:04,836   att_loss = 2.228058038773769
2021-08-08 18:04:04,836   cls_loss = 0.0
2021-08-08 18:04:04,836   global_step = 7449
2021-08-08 18:04:04,836   loss = 3.098598971176762
2021-08-08 18:04:04,836   rep_loss = 0.8705409316188266
2021-08-08 18:04:04,837 ***** Save model *****
2021-08-08 18:04:10,876 ***** Running evaluation *****
2021-08-08 18:04:10,876   Epoch = 0 iter 7499 step
2021-08-08 18:04:10,876   Num examples = 9815
2021-08-08 18:04:10,876   Batch size = 32
2021-08-08 18:04:10,877 ***** Eval results *****
2021-08-08 18:04:10,877   att_loss = 2.2275194202459465
2021-08-08 18:04:10,877   cls_loss = 0.0
2021-08-08 18:04:10,877   global_step = 7499
2021-08-08 18:04:10,877   loss = 3.097940322509971
2021-08-08 18:04:10,877   rep_loss = 0.8704209014373965
2021-08-08 18:04:10,877 ***** Save model *****
2021-08-08 18:04:16,871 ***** Running evaluation *****
2021-08-08 18:04:16,871   Epoch = 0 iter 7549 step
2021-08-08 18:04:16,871   Num examples = 9815
2021-08-08 18:04:16,872   Batch size = 32
2021-08-08 18:04:16,872 ***** Eval results *****
2021-08-08 18:04:16,872   att_loss = 2.2267865794812596
2021-08-08 18:04:16,872   cls_loss = 0.0
2021-08-08 18:04:16,872   global_step = 7549
2021-08-08 18:04:16,872   loss = 3.097100154724101
2021-08-08 18:04:16,873   rep_loss = 0.8703135744769583
2021-08-08 18:04:16,873 ***** Save model *****
2021-08-08 18:04:22,846 ***** Running evaluation *****
2021-08-08 18:04:22,846   Epoch = 0 iter 7599 step
2021-08-08 18:04:22,846   Num examples = 9815
2021-08-08 18:04:22,847   Batch size = 32
2021-08-08 18:04:22,847 ***** Eval results *****
2021-08-08 18:04:22,847   att_loss = 2.226426763524506
2021-08-08 18:04:22,847   cls_loss = 0.0
2021-08-08 18:04:22,847   global_step = 7599
2021-08-08 18:04:22,848   loss = 3.0966521967930425
2021-08-08 18:04:22,848   rep_loss = 0.8702254325155365
2021-08-08 18:04:22,848 ***** Save model *****
2021-08-08 18:04:28,808 ***** Running evaluation *****
2021-08-08 18:04:28,809   Epoch = 0 iter 7649 step
2021-08-08 18:04:28,809   Num examples = 9815
2021-08-08 18:04:28,809   Batch size = 32
2021-08-08 18:04:28,810 ***** Eval results *****
2021-08-08 18:04:28,810   att_loss = 2.2258943867443404
2021-08-08 18:04:28,810   cls_loss = 0.0
2021-08-08 18:04:28,810   global_step = 7649
2021-08-08 18:04:28,810   loss = 3.096026209011968
2021-08-08 18:04:28,810   rep_loss = 0.8701318214572104
2021-08-08 18:04:28,810 ***** Save model *****
2021-08-08 18:04:34,772 ***** Running evaluation *****
2021-08-08 18:04:34,773   Epoch = 0 iter 7699 step
2021-08-08 18:04:34,773   Num examples = 9815
2021-08-08 18:04:34,773   Batch size = 32
2021-08-08 18:04:34,773 ***** Eval results *****
2021-08-08 18:04:34,774   att_loss = 2.2253945181471666
2021-08-08 18:04:34,774   cls_loss = 0.0
2021-08-08 18:04:34,774   global_step = 7699
2021-08-08 18:04:34,774   loss = 3.0954168479865922
2021-08-08 18:04:34,774   rep_loss = 0.8700223289723364
2021-08-08 18:04:34,774 ***** Save model *****
2021-08-08 18:04:40,706 ***** Running evaluation *****
2021-08-08 18:04:40,706   Epoch = 0 iter 7749 step
2021-08-08 18:04:40,706   Num examples = 9815
2021-08-08 18:04:40,706   Batch size = 32
2021-08-08 18:04:40,707 ***** Eval results *****
2021-08-08 18:04:40,707   att_loss = 2.224997617518737
2021-08-08 18:04:40,707   cls_loss = 0.0
2021-08-08 18:04:40,707   global_step = 7749
2021-08-08 18:04:40,707   loss = 3.0949219518329945
2021-08-08 18:04:40,707   rep_loss = 0.8699243335296822
2021-08-08 18:04:40,707 ***** Save model *****
2021-08-08 18:04:48,269 ***** Running evaluation *****
2021-08-08 18:04:48,269   Epoch = 0 iter 7799 step
2021-08-08 18:04:48,269   Num examples = 9815
2021-08-08 18:04:48,269   Batch size = 32
2021-08-08 18:04:48,270 ***** Eval results *****
2021-08-08 18:04:48,270   att_loss = 2.223902837315161
2021-08-08 18:04:48,270   cls_loss = 0.0
2021-08-08 18:04:48,270   global_step = 7799
2021-08-08 18:04:48,270   loss = 3.093688434263822
2021-08-08 18:04:48,270   rep_loss = 0.8697855961996862
2021-08-08 18:04:48,270 ***** Save model *****
2021-08-08 18:04:54,194 ***** Running evaluation *****
2021-08-08 18:04:54,194   Epoch = 0 iter 7849 step
2021-08-08 18:04:54,194   Num examples = 9815
2021-08-08 18:04:54,194   Batch size = 32
2021-08-08 18:04:54,195 ***** Eval results *****
2021-08-08 18:04:54,195   att_loss = 2.2233032281269067
2021-08-08 18:04:54,195   cls_loss = 0.0
2021-08-08 18:04:54,195   global_step = 7849
2021-08-08 18:04:54,195   loss = 3.092980147228224
2021-08-08 18:04:54,195   rep_loss = 0.8696769183647072
2021-08-08 18:04:54,195 ***** Save model *****
2021-08-08 18:05:00,078 ***** Running evaluation *****
2021-08-08 18:05:00,078   Epoch = 0 iter 7899 step
2021-08-08 18:05:00,078   Num examples = 9815
2021-08-08 18:05:00,078   Batch size = 32
2021-08-08 18:05:00,079 ***** Eval results *****
2021-08-08 18:05:00,079   att_loss = 2.2230137315999556
2021-08-08 18:05:00,079   cls_loss = 0.0
2021-08-08 18:05:00,079   global_step = 7899
2021-08-08 18:05:00,079   loss = 3.0926104121033733
2021-08-08 18:05:00,079   rep_loss = 0.8695966798167453
2021-08-08 18:05:00,080 ***** Save model *****
2021-08-08 18:05:05,975 ***** Running evaluation *****
2021-08-08 18:05:05,975   Epoch = 0 iter 7949 step
2021-08-08 18:05:05,975   Num examples = 9815
2021-08-08 18:05:05,975   Batch size = 32
2021-08-08 18:05:05,976 ***** Eval results *****
2021-08-08 18:05:05,976   att_loss = 2.222200840340424
2021-08-08 18:05:05,976   cls_loss = 0.0
2021-08-08 18:05:05,976   global_step = 7949
2021-08-08 18:05:05,976   loss = 3.0916742757215845
2021-08-08 18:05:05,976   rep_loss = 0.8694734347063066
2021-08-08 18:05:05,977 ***** Save model *****
2021-08-08 18:05:11,916 ***** Running evaluation *****
2021-08-08 18:05:11,916   Epoch = 0 iter 7999 step
2021-08-08 18:05:11,916   Num examples = 9815
2021-08-08 18:05:11,916   Batch size = 32
2021-08-08 18:05:11,917 ***** Eval results *****
2021-08-08 18:05:11,917   att_loss = 2.221409739859627
2021-08-08 18:05:11,917   cls_loss = 0.0
2021-08-08 18:05:11,917   global_step = 7999
2021-08-08 18:05:11,917   loss = 3.090759222679458
2021-08-08 18:05:11,917   rep_loss = 0.8693494821566464
2021-08-08 18:05:11,918 ***** Save model *****
2021-08-08 18:05:18,316 ***** Running evaluation *****
2021-08-08 18:05:18,317   Epoch = 0 iter 8049 step
2021-08-08 18:05:18,317   Num examples = 9815
2021-08-08 18:05:18,317   Batch size = 32
2021-08-08 18:05:18,317 ***** Eval results *****
2021-08-08 18:05:18,318   att_loss = 2.2206935363462184
2021-08-08 18:05:18,318   cls_loss = 0.0
2021-08-08 18:05:18,318   global_step = 8049
2021-08-08 18:05:18,318   loss = 3.08991550572808
2021-08-08 18:05:18,318   rep_loss = 0.8692219687968489
2021-08-08 18:05:18,318 ***** Save model *****
2021-08-08 18:05:24,436 ***** Running evaluation *****
2021-08-08 18:05:24,436   Epoch = 0 iter 8099 step
2021-08-08 18:05:24,437   Num examples = 9815
2021-08-08 18:05:24,437   Batch size = 32
2021-08-08 18:05:24,437 ***** Eval results *****
2021-08-08 18:05:24,437   att_loss = 2.220013695081703
2021-08-08 18:05:24,437   cls_loss = 0.0
2021-08-08 18:05:24,438   global_step = 8099
2021-08-08 18:05:24,438   loss = 3.0891138998604597
2021-08-08 18:05:24,438   rep_loss = 0.8691002040869631
2021-08-08 18:05:24,438 ***** Save model *****
2021-08-08 18:05:30,451 ***** Running evaluation *****
2021-08-08 18:05:30,451   Epoch = 0 iter 8149 step
2021-08-08 18:05:30,451   Num examples = 9815
2021-08-08 18:05:30,451   Batch size = 32
2021-08-08 18:05:30,452 ***** Eval results *****
2021-08-08 18:05:30,452   att_loss = 2.219850898315635
2021-08-08 18:05:30,452   cls_loss = 0.0
2021-08-08 18:05:30,452   global_step = 8149
2021-08-08 18:05:30,452   loss = 3.088876964973078
2021-08-08 18:05:30,452   rep_loss = 0.8690260659625799
2021-08-08 18:05:30,452 ***** Save model *****
2021-08-08 18:05:36,367 ***** Running evaluation *****
2021-08-08 18:05:36,368   Epoch = 0 iter 8199 step
2021-08-08 18:05:36,368   Num examples = 9815
2021-08-08 18:05:36,368   Batch size = 32
2021-08-08 18:05:36,368 ***** Eval results *****
2021-08-08 18:05:36,369   att_loss = 2.219379792534472
2021-08-08 18:05:36,369   cls_loss = 0.0
2021-08-08 18:05:36,369   global_step = 8199
2021-08-08 18:05:36,369   loss = 3.0883084730573334
2021-08-08 18:05:36,369   rep_loss = 0.8689286798031567
2021-08-08 18:05:36,369 ***** Save model *****
2021-08-08 18:05:42,352 ***** Running evaluation *****
2021-08-08 18:05:42,352   Epoch = 0 iter 8249 step
2021-08-08 18:05:42,352   Num examples = 9815
2021-08-08 18:05:42,352   Batch size = 32
2021-08-08 18:05:42,353 ***** Eval results *****
2021-08-08 18:05:42,353   att_loss = 2.2193662374868497
2021-08-08 18:05:42,353   cls_loss = 0.0
2021-08-08 18:05:42,353   global_step = 8249
2021-08-08 18:05:42,353   loss = 3.088228985011498
2021-08-08 18:05:42,353   rep_loss = 0.868862746765952
2021-08-08 18:05:42,354 ***** Save model *****
2021-08-08 18:05:48,334 ***** Running evaluation *****
2021-08-08 18:05:48,334   Epoch = 0 iter 8299 step
2021-08-08 18:05:48,334   Num examples = 9815
2021-08-08 18:05:48,334   Batch size = 32
2021-08-08 18:05:48,335 ***** Eval results *****
2021-08-08 18:05:48,335   att_loss = 2.219097327576989
2021-08-08 18:05:48,335   cls_loss = 0.0
2021-08-08 18:05:48,335   global_step = 8299
2021-08-08 18:05:48,335   loss = 3.087880924331494
2021-08-08 18:05:48,335   rep_loss = 0.8687835960865653
2021-08-08 18:05:48,336 ***** Save model *****
2021-08-08 18:05:54,296 ***** Running evaluation *****
2021-08-08 18:05:54,296   Epoch = 0 iter 8349 step
2021-08-08 18:05:54,296   Num examples = 9815
2021-08-08 18:05:54,296   Batch size = 32
2021-08-08 18:05:54,297 ***** Eval results *****
2021-08-08 18:05:54,297   att_loss = 2.2185851640081045
2021-08-08 18:05:54,297   cls_loss = 0.0
2021-08-08 18:05:54,297   global_step = 8349
2021-08-08 18:05:54,297   loss = 3.08726885324753
2021-08-08 18:05:54,297   rep_loss = 0.8686836885040942
2021-08-08 18:05:54,297 ***** Save model *****
2021-08-08 18:06:00,274 ***** Running evaluation *****
2021-08-08 18:06:00,274   Epoch = 0 iter 8399 step
2021-08-08 18:06:00,274   Num examples = 9815
2021-08-08 18:06:00,274   Batch size = 32
2021-08-08 18:06:00,275 ***** Eval results *****
2021-08-08 18:06:00,275   att_loss = 2.2179395027395685
2021-08-08 18:06:00,275   cls_loss = 0.0
2021-08-08 18:06:00,275   global_step = 8399
2021-08-08 18:06:00,275   loss = 3.0865165864531376
2021-08-08 18:06:00,275   rep_loss = 0.8685770829684224
2021-08-08 18:06:00,276 ***** Save model *****
2021-08-08 18:06:06,187 ***** Running evaluation *****
2021-08-08 18:06:06,188   Epoch = 0 iter 8449 step
2021-08-08 18:06:06,188   Num examples = 9815
2021-08-08 18:06:06,188   Batch size = 32
2021-08-08 18:06:06,189 ***** Eval results *****
2021-08-08 18:06:06,189   att_loss = 2.2172682044463463
2021-08-08 18:06:06,189   cls_loss = 0.0
2021-08-08 18:06:06,189   global_step = 8449
2021-08-08 18:06:06,189   loss = 3.0857432139443284
2021-08-08 18:06:06,189   rep_loss = 0.8684750087501905
2021-08-08 18:06:06,189 ***** Save model *****
2021-08-08 18:06:12,101 ***** Running evaluation *****
2021-08-08 18:06:12,101   Epoch = 0 iter 8499 step
2021-08-08 18:06:12,101   Num examples = 9815
2021-08-08 18:06:12,101   Batch size = 32
2021-08-08 18:06:12,102 ***** Eval results *****
2021-08-08 18:06:12,102   att_loss = 2.216811832799282
2021-08-08 18:06:12,102   cls_loss = 0.0
2021-08-08 18:06:12,102   global_step = 8499
2021-08-08 18:06:12,102   loss = 3.0851942562105963
2021-08-08 18:06:12,102   rep_loss = 0.8683824226258432
2021-08-08 18:06:12,102 ***** Save model *****
2021-08-08 18:06:20,046 ***** Running evaluation *****
2021-08-08 18:06:20,047   Epoch = 0 iter 8549 step
2021-08-08 18:06:20,047   Num examples = 9815
2021-08-08 18:06:20,047   Batch size = 32
2021-08-08 18:06:20,048 ***** Eval results *****
2021-08-08 18:06:20,048   att_loss = 2.2161931246257636
2021-08-08 18:06:20,048   cls_loss = 0.0
2021-08-08 18:06:20,048   global_step = 8549
2021-08-08 18:06:20,048   loss = 3.0844862410333063
2021-08-08 18:06:20,048   rep_loss = 0.8682931156963867
2021-08-08 18:06:20,048 ***** Save model *****
2021-08-08 18:06:28,197 ***** Running evaluation *****
2021-08-08 18:06:28,197   Epoch = 0 iter 8599 step
2021-08-08 18:06:28,198   Num examples = 9815
2021-08-08 18:06:28,198   Batch size = 32
2021-08-08 18:06:28,198 ***** Eval results *****
2021-08-08 18:06:28,198   att_loss = 2.215780577350181
2021-08-08 18:06:28,198   cls_loss = 0.0
2021-08-08 18:06:28,199   global_step = 8599
2021-08-08 18:06:28,199   loss = 3.0839829287954315
2021-08-08 18:06:28,199   rep_loss = 0.8682023506966395
2021-08-08 18:06:28,199 ***** Save model *****
2021-08-08 18:06:34,124 ***** Running evaluation *****
2021-08-08 18:06:34,124   Epoch = 0 iter 8649 step
2021-08-08 18:06:34,124   Num examples = 9815
2021-08-08 18:06:34,124   Batch size = 32
2021-08-08 18:06:34,125 ***** Eval results *****
2021-08-08 18:06:34,125   att_loss = 2.2157122445307738
2021-08-08 18:06:34,125   cls_loss = 0.0
2021-08-08 18:06:34,125   global_step = 8649
2021-08-08 18:06:34,125   loss = 3.0838568735183327
2021-08-08 18:06:34,125   rep_loss = 0.8681446283190826
2021-08-08 18:06:34,125 ***** Save model *****
2021-08-08 18:06:40,059 ***** Running evaluation *****
2021-08-08 18:06:40,059   Epoch = 0 iter 8699 step
2021-08-08 18:06:40,059   Num examples = 9815
2021-08-08 18:06:40,059   Batch size = 32
2021-08-08 18:06:40,060 ***** Eval results *****
2021-08-08 18:06:40,060   att_loss = 2.2149984913182568
2021-08-08 18:06:40,060   cls_loss = 0.0
2021-08-08 18:06:40,060   global_step = 8699
2021-08-08 18:06:40,060   loss = 3.083023909467817
2021-08-08 18:06:40,060   rep_loss = 0.8680254174780744
2021-08-08 18:06:40,060 ***** Save model *****
2021-08-08 18:06:45,998 ***** Running evaluation *****
2021-08-08 18:06:45,998   Epoch = 0 iter 8749 step
2021-08-08 18:06:45,998   Num examples = 9815
2021-08-08 18:06:45,998   Batch size = 32
2021-08-08 18:06:45,999 ***** Eval results *****
2021-08-08 18:06:45,999   att_loss = 2.214329040136347
2021-08-08 18:06:45,999   cls_loss = 0.0
2021-08-08 18:06:45,999   global_step = 8749
2021-08-08 18:06:45,999   loss = 3.0822497004440192
2021-08-08 18:06:45,999   rep_loss = 0.8679206595991472
2021-08-08 18:06:46,000 ***** Save model *****
2021-08-08 18:06:51,916 ***** Running evaluation *****
2021-08-08 18:06:51,916   Epoch = 0 iter 8799 step
2021-08-08 18:06:51,917   Num examples = 9815
2021-08-08 18:06:51,917   Batch size = 32
2021-08-08 18:06:51,917 ***** Eval results *****
2021-08-08 18:06:51,917   att_loss = 2.214010378680103
2021-08-08 18:06:51,917   cls_loss = 0.0
2021-08-08 18:06:51,917   global_step = 8799
2021-08-08 18:06:51,918   loss = 3.0818536199484514
2021-08-08 18:06:51,918   rep_loss = 0.8678432405299795
2021-08-08 18:06:51,918 ***** Save model *****
2021-08-08 18:06:57,840 ***** Running evaluation *****
2021-08-08 18:06:57,840   Epoch = 0 iter 8849 step
2021-08-08 18:06:57,840   Num examples = 9815
2021-08-08 18:06:57,840   Batch size = 32
2021-08-08 18:06:57,841 ***** Eval results *****
2021-08-08 18:06:57,841   att_loss = 2.213679481878269
2021-08-08 18:06:57,841   cls_loss = 0.0
2021-08-08 18:06:57,841   global_step = 8849
2021-08-08 18:06:57,841   loss = 3.0814401817558874
2021-08-08 18:06:57,841   rep_loss = 0.8677606991164791
2021-08-08 18:06:57,841 ***** Save model *****
2021-08-08 18:07:03,823 ***** Running evaluation *****
2021-08-08 18:07:03,823   Epoch = 0 iter 8899 step
2021-08-08 18:07:03,823   Num examples = 9815
2021-08-08 18:07:03,823   Batch size = 32
2021-08-08 18:07:03,824 ***** Eval results *****
2021-08-08 18:07:03,824   att_loss = 2.2130371787224155
2021-08-08 18:07:03,824   cls_loss = 0.0
2021-08-08 18:07:03,824   global_step = 8899
2021-08-08 18:07:03,824   loss = 3.080704985180388
2021-08-08 18:07:03,824   rep_loss = 0.8676678057212028
2021-08-08 18:07:03,825 ***** Save model *****
2021-08-08 18:07:09,725 ***** Running evaluation *****
2021-08-08 18:07:09,725   Epoch = 0 iter 8949 step
2021-08-08 18:07:09,725   Num examples = 9815
2021-08-08 18:07:09,725   Batch size = 32
2021-08-08 18:07:09,726 ***** Eval results *****
2021-08-08 18:07:09,726   att_loss = 2.2126805124662217
2021-08-08 18:07:09,726   cls_loss = 0.0
2021-08-08 18:07:09,726   global_step = 8949
2021-08-08 18:07:09,726   loss = 3.080264971874082
2021-08-08 18:07:09,726   rep_loss = 0.8675844586885286
2021-08-08 18:07:09,726 ***** Save model *****
2021-08-08 18:07:15,661 ***** Running evaluation *****
2021-08-08 18:07:15,661   Epoch = 0 iter 8999 step
2021-08-08 18:07:15,662   Num examples = 9815
2021-08-08 18:07:15,662   Batch size = 32
2021-08-08 18:07:15,662 ***** Eval results *****
2021-08-08 18:07:15,662   att_loss = 2.212073733335284
2021-08-08 18:07:15,662   cls_loss = 0.0
2021-08-08 18:07:15,663   global_step = 8999
2021-08-08 18:07:15,663   loss = 3.0795603478030373
2021-08-08 18:07:15,663   rep_loss = 0.8674866137259244
2021-08-08 18:07:15,663 ***** Save model *****
2021-08-08 18:07:21,596 ***** Running evaluation *****
2021-08-08 18:07:21,596   Epoch = 0 iter 9049 step
2021-08-08 18:07:21,596   Num examples = 9815
2021-08-08 18:07:21,596   Batch size = 32
2021-08-08 18:07:21,597 ***** Eval results *****
2021-08-08 18:07:21,597   att_loss = 2.211474751659435
2021-08-08 18:07:21,597   cls_loss = 0.0
2021-08-08 18:07:21,597   global_step = 9049
2021-08-08 18:07:21,597   loss = 3.0788626827341585
2021-08-08 18:07:21,597   rep_loss = 0.8673879303633409
2021-08-08 18:07:21,597 ***** Save model *****
2021-08-08 18:07:29,551 ***** Running evaluation *****
2021-08-08 18:07:29,551   Epoch = 0 iter 9099 step
2021-08-08 18:07:29,551   Num examples = 9815
2021-08-08 18:07:29,552   Batch size = 32
2021-08-08 18:07:29,552 ***** Eval results *****
2021-08-08 18:07:29,552   att_loss = 2.2108976719523277
2021-08-08 18:07:29,552   cls_loss = 0.0
2021-08-08 18:07:29,552   global_step = 9099
2021-08-08 18:07:29,553   loss = 3.078193155536679
2021-08-08 18:07:29,553   rep_loss = 0.8672954829227323
2021-08-08 18:07:29,553 ***** Save model *****
2021-08-08 18:07:35,450 ***** Running evaluation *****
2021-08-08 18:07:35,451   Epoch = 0 iter 9149 step
2021-08-08 18:07:35,451   Num examples = 9815
2021-08-08 18:07:35,451   Batch size = 32
2021-08-08 18:07:35,451 ***** Eval results *****
2021-08-08 18:07:35,452   att_loss = 2.2102320663576402
2021-08-08 18:07:35,452   cls_loss = 0.0
2021-08-08 18:07:35,452   global_step = 9149
2021-08-08 18:07:35,452   loss = 3.0774172293656457
2021-08-08 18:07:35,452   rep_loss = 0.8671851623695475
2021-08-08 18:07:35,452 ***** Save model *****
2021-08-08 18:07:41,313 ***** Running evaluation *****
2021-08-08 18:07:41,313   Epoch = 0 iter 9199 step
2021-08-08 18:07:41,313   Num examples = 9815
2021-08-08 18:07:41,313   Batch size = 32
2021-08-08 18:07:41,314 ***** Eval results *****
2021-08-08 18:07:41,314   att_loss = 2.2098148743061024
2021-08-08 18:07:41,314   cls_loss = 0.0
2021-08-08 18:07:41,314   global_step = 9199
2021-08-08 18:07:41,314   loss = 3.0769044796997265
2021-08-08 18:07:41,314   rep_loss = 0.8670896047197589
2021-08-08 18:07:41,314 ***** Save model *****
2021-08-08 18:07:47,213 ***** Running evaluation *****
2021-08-08 18:07:47,214   Epoch = 0 iter 9249 step
2021-08-08 18:07:47,214   Num examples = 9815
2021-08-08 18:07:47,214   Batch size = 32
2021-08-08 18:07:47,215 ***** Eval results *****
2021-08-08 18:07:47,215   att_loss = 2.2094933923508284
2021-08-08 18:07:47,215   cls_loss = 0.0
2021-08-08 18:07:47,215   global_step = 9249
2021-08-08 18:07:47,215   loss = 3.076500329472256
2021-08-08 18:07:47,215   rep_loss = 0.8670069364576497
2021-08-08 18:07:47,215 ***** Save model *****
2021-08-08 18:07:53,094 ***** Running evaluation *****
2021-08-08 18:07:53,094   Epoch = 0 iter 9299 step
2021-08-08 18:07:53,094   Num examples = 9815
2021-08-08 18:07:53,094   Batch size = 32
2021-08-08 18:07:53,095 ***** Eval results *****
2021-08-08 18:07:53,095   att_loss = 2.2089741020154436
2021-08-08 18:07:53,095   cls_loss = 0.0
2021-08-08 18:07:53,095   global_step = 9299
2021-08-08 18:07:53,095   loss = 3.0758819250614877
2021-08-08 18:07:53,095   rep_loss = 0.8669078224883924
2021-08-08 18:07:53,125 ***** Save model *****
2021-08-08 18:07:59,066 ***** Running evaluation *****
2021-08-08 18:07:59,066   Epoch = 0 iter 9349 step
2021-08-08 18:07:59,066   Num examples = 9815
2021-08-08 18:07:59,066   Batch size = 32
2021-08-08 18:07:59,067 ***** Eval results *****
2021-08-08 18:07:59,067   att_loss = 2.2086345706601644
2021-08-08 18:07:59,067   cls_loss = 0.0
2021-08-08 18:07:59,067   global_step = 9349
2021-08-08 18:07:59,067   loss = 3.0754595580158646
2021-08-08 18:07:59,067   rep_loss = 0.8668249868456598
2021-08-08 18:07:59,067 ***** Save model *****
2021-08-08 18:08:04,993 ***** Running evaluation *****
2021-08-08 18:08:04,993   Epoch = 0 iter 9399 step
2021-08-08 18:08:04,994   Num examples = 9815
2021-08-08 18:08:04,994   Batch size = 32
2021-08-08 18:08:04,994 ***** Eval results *****
2021-08-08 18:08:04,994   att_loss = 2.208319160174481
2021-08-08 18:08:04,994   cls_loss = 0.0
2021-08-08 18:08:04,995   global_step = 9399
2021-08-08 18:08:04,995   loss = 3.0750660386183424
2021-08-08 18:08:04,995   rep_loss = 0.8667468779175088
2021-08-08 18:08:04,995 ***** Save model *****
2021-08-08 18:08:10,926 ***** Running evaluation *****
2021-08-08 18:08:10,926   Epoch = 0 iter 9449 step
2021-08-08 18:08:10,926   Num examples = 9815
2021-08-08 18:08:10,926   Batch size = 32
2021-08-08 18:08:10,927 ***** Eval results *****
2021-08-08 18:08:10,927   att_loss = 2.2083151303898
2021-08-08 18:08:10,927   cls_loss = 0.0
2021-08-08 18:08:10,927   global_step = 9449
2021-08-08 18:08:10,927   loss = 3.0750029461259425
2021-08-08 18:08:10,928   rep_loss = 0.8666878152756555
2021-08-08 18:08:10,928 ***** Save model *****
2021-08-08 18:08:16,851 ***** Running evaluation *****
2021-08-08 18:08:16,851   Epoch = 0 iter 9499 step
2021-08-08 18:08:16,851   Num examples = 9815
2021-08-08 18:08:16,852   Batch size = 32
2021-08-08 18:08:16,852 ***** Eval results *****
2021-08-08 18:08:16,852   att_loss = 2.2075254638793154
2021-08-08 18:08:16,852   cls_loss = 0.0
2021-08-08 18:08:16,852   global_step = 9499
2021-08-08 18:08:16,852   loss = 3.0740969421261424
2021-08-08 18:08:16,853   rep_loss = 0.8665714777824891
2021-08-08 18:08:16,853 ***** Save model *****
2021-08-08 18:08:22,958 ***** Running evaluation *****
2021-08-08 18:08:22,959   Epoch = 0 iter 9549 step
2021-08-08 18:08:22,959   Num examples = 9815
2021-08-08 18:08:22,959   Batch size = 32
2021-08-08 18:08:22,959 ***** Eval results *****
2021-08-08 18:08:22,959   att_loss = 2.206962327955156
2021-08-08 18:08:22,959   cls_loss = 0.0
2021-08-08 18:08:22,960   global_step = 9549
2021-08-08 18:08:22,960   loss = 3.0734385283333627
2021-08-08 18:08:22,960   rep_loss = 0.8664761999287847
2021-08-08 18:08:22,960 ***** Save model *****
2021-08-08 18:08:28,860 ***** Running evaluation *****
2021-08-08 18:08:28,860   Epoch = 0 iter 9599 step
2021-08-08 18:08:28,861   Num examples = 9815
2021-08-08 18:08:28,861   Batch size = 32
2021-08-08 18:08:28,861 ***** Eval results *****
2021-08-08 18:08:28,861   att_loss = 2.206804607098072
2021-08-08 18:08:28,861   cls_loss = 0.0
2021-08-08 18:08:28,862   global_step = 9599
2021-08-08 18:08:28,862   loss = 3.0732140026933044
2021-08-08 18:08:28,862   rep_loss = 0.8664093951233129
2021-08-08 18:08:28,862 ***** Save model *****
2021-08-08 18:08:34,764 ***** Running evaluation *****
2021-08-08 18:08:34,764   Epoch = 0 iter 9649 step
2021-08-08 18:08:34,764   Num examples = 9815
2021-08-08 18:08:34,765   Batch size = 32
2021-08-08 18:08:34,765 ***** Eval results *****
2021-08-08 18:08:34,765   att_loss = 2.206248181806919
2021-08-08 18:08:34,765   cls_loss = 0.0
2021-08-08 18:08:34,765   global_step = 9649
2021-08-08 18:08:34,765   loss = 3.0725619021024415
2021-08-08 18:08:34,766   rep_loss = 0.8663137198075171
2021-08-08 18:08:34,766 ***** Save model *****
2021-08-08 18:08:40,661 ***** Running evaluation *****
2021-08-08 18:08:40,662   Epoch = 0 iter 9699 step
2021-08-08 18:08:40,662   Num examples = 9815
2021-08-08 18:08:40,662   Batch size = 32
2021-08-08 18:08:40,662 ***** Eval results *****
2021-08-08 18:08:40,663   att_loss = 2.205821658264842
2021-08-08 18:08:40,663   cls_loss = 0.0
2021-08-08 18:08:40,663   global_step = 9699
2021-08-08 18:08:40,663   loss = 3.0720488433186963
2021-08-08 18:08:40,663   rep_loss = 0.8662271847097096
2021-08-08 18:08:40,663 ***** Save model *****
2021-08-08 18:08:46,615 ***** Running evaluation *****
2021-08-08 18:08:46,615   Epoch = 0 iter 9749 step
2021-08-08 18:08:46,615   Num examples = 9815
2021-08-08 18:08:46,615   Batch size = 32
2021-08-08 18:08:46,616 ***** Eval results *****
2021-08-08 18:08:46,616   att_loss = 2.2053765905907317
2021-08-08 18:08:46,616   cls_loss = 0.0
2021-08-08 18:08:46,616   global_step = 9749
2021-08-08 18:08:46,616   loss = 3.071522925203061
2021-08-08 18:08:46,616   rep_loss = 0.8661463342944051
2021-08-08 18:08:46,616 ***** Save model *****
2021-08-08 18:08:52,563 ***** Running evaluation *****
2021-08-08 18:08:52,563   Epoch = 0 iter 9799 step
2021-08-08 18:08:52,563   Num examples = 9815
2021-08-08 18:08:52,563   Batch size = 32
2021-08-08 18:08:52,564 ***** Eval results *****
2021-08-08 18:08:52,564   att_loss = 2.20480053671064
2021-08-08 18:08:52,564   cls_loss = 0.0
2021-08-08 18:08:52,564   global_step = 9799
2021-08-08 18:08:52,564   loss = 3.070852880587881
2021-08-08 18:08:52,564   rep_loss = 0.8660523435244427
2021-08-08 18:08:52,564 ***** Save model *****
2021-08-08 18:08:58,491 ***** Running evaluation *****
2021-08-08 18:08:58,492   Epoch = 0 iter 9849 step
2021-08-08 18:08:58,492   Num examples = 9815
2021-08-08 18:08:58,492   Batch size = 32
2021-08-08 18:08:58,492 ***** Eval results *****
2021-08-08 18:08:58,492   att_loss = 2.2043749323374415
2021-08-08 18:08:58,493   cls_loss = 0.0
2021-08-08 18:08:58,493   global_step = 9849
2021-08-08 18:08:58,493   loss = 3.07033707572913
2021-08-08 18:08:58,493   rep_loss = 0.8659621430527851
2021-08-08 18:08:58,493 ***** Save model *****
2021-08-08 18:09:06,607 ***** Running evaluation *****
2021-08-08 18:09:06,607   Epoch = 0 iter 9899 step
2021-08-08 18:09:06,607   Num examples = 9815
2021-08-08 18:09:06,608   Batch size = 32
2021-08-08 18:09:06,608 ***** Eval results *****
2021-08-08 18:09:06,608   att_loss = 2.2038883997317646
2021-08-08 18:09:06,608   cls_loss = 0.0
2021-08-08 18:09:06,609   global_step = 9899
2021-08-08 18:09:06,609   loss = 3.0697596688043687
2021-08-08 18:09:06,609   rep_loss = 0.8658712686691782
2021-08-08 18:09:06,609 ***** Save model *****
2021-08-08 18:09:13,014 ***** Running evaluation *****
2021-08-08 18:09:13,014   Epoch = 0 iter 9949 step
2021-08-08 18:09:13,014   Num examples = 9815
2021-08-08 18:09:13,014   Batch size = 32
2021-08-08 18:09:13,015 ***** Eval results *****
2021-08-08 18:09:13,015   att_loss = 2.203724105761439
2021-08-08 18:09:13,015   cls_loss = 0.0
2021-08-08 18:09:13,015   global_step = 9949
2021-08-08 18:09:13,015   loss = 3.0695296035770054
2021-08-08 18:09:13,015   rep_loss = 0.8658054974081768
2021-08-08 18:09:13,015 ***** Save model *****
2021-08-08 18:09:18,946 ***** Running evaluation *****
2021-08-08 18:09:18,946   Epoch = 0 iter 9999 step
2021-08-08 18:09:18,946   Num examples = 9815
2021-08-08 18:09:18,946   Batch size = 32
2021-08-08 18:09:18,947 ***** Eval results *****
2021-08-08 18:09:18,947   att_loss = 2.203224386056789
2021-08-08 18:09:18,947   cls_loss = 0.0
2021-08-08 18:09:18,947   global_step = 9999
2021-08-08 18:09:18,947   loss = 3.0689466645305354
2021-08-08 18:09:18,947   rep_loss = 0.8657222779730175
2021-08-08 18:09:18,947 ***** Save model *****
2021-08-08 18:09:24,873 ***** Running evaluation *****
2021-08-08 18:09:24,873   Epoch = 0 iter 10049 step
2021-08-08 18:09:24,873   Num examples = 9815
2021-08-08 18:09:24,873   Batch size = 32
2021-08-08 18:09:24,874 ***** Eval results *****
2021-08-08 18:09:24,874   att_loss = 2.203258338751799
2021-08-08 18:09:24,874   cls_loss = 0.0
2021-08-08 18:09:24,874   global_step = 10049
2021-08-08 18:09:24,874   loss = 3.0689328258008386
2021-08-08 18:09:24,874   rep_loss = 0.8656744866279099
2021-08-08 18:09:24,875 ***** Save model *****
2021-08-08 18:09:31,488 ***** Running evaluation *****
2021-08-08 18:09:31,489   Epoch = 0 iter 10099 step
2021-08-08 18:09:31,489   Num examples = 9815
2021-08-08 18:09:31,489   Batch size = 32
2021-08-08 18:09:31,490 ***** Eval results *****
2021-08-08 18:09:31,490   att_loss = 2.202716993735514
2021-08-08 18:09:31,490   cls_loss = 0.0
2021-08-08 18:09:31,490   global_step = 10099
2021-08-08 18:09:31,490   loss = 3.0683041836840292
2021-08-08 18:09:31,490   rep_loss = 0.8655871894881565
2021-08-08 18:09:31,490 ***** Save model *****
2021-08-08 18:09:37,459 ***** Running evaluation *****
2021-08-08 18:09:37,459   Epoch = 0 iter 10149 step
2021-08-08 18:09:37,459   Num examples = 9815
2021-08-08 18:09:37,459   Batch size = 32
2021-08-08 18:09:37,460 ***** Eval results *****
2021-08-08 18:09:37,460   att_loss = 2.2020950930167187
2021-08-08 18:09:37,460   cls_loss = 0.0
2021-08-08 18:09:37,460   global_step = 10149
2021-08-08 18:09:37,460   loss = 3.067581038931248
2021-08-08 18:09:37,460   rep_loss = 0.8654859455327868
2021-08-08 18:09:37,461 ***** Save model *****
2021-08-08 18:09:43,409 ***** Running evaluation *****
2021-08-08 18:09:43,409   Epoch = 0 iter 10199 step
2021-08-08 18:09:43,409   Num examples = 9815
2021-08-08 18:09:43,409   Batch size = 32
2021-08-08 18:09:43,410 ***** Eval results *****
2021-08-08 18:09:43,410   att_loss = 2.2017652887329775
2021-08-08 18:09:43,410   cls_loss = 0.0
2021-08-08 18:09:43,410   global_step = 10199
2021-08-08 18:09:43,410   loss = 3.0671839847531035
2021-08-08 18:09:43,410   rep_loss = 0.8654186957162295
2021-08-08 18:09:43,411 ***** Save model *****
2021-08-08 18:09:49,337 ***** Running evaluation *****
2021-08-08 18:09:49,337   Epoch = 0 iter 10249 step
2021-08-08 18:09:49,338   Num examples = 9815
2021-08-08 18:09:49,338   Batch size = 32
2021-08-08 18:09:49,338 ***** Eval results *****
2021-08-08 18:09:49,338   att_loss = 2.2013870408353786
2021-08-08 18:09:49,338   cls_loss = 0.0
2021-08-08 18:09:49,338   global_step = 10249
2021-08-08 18:09:49,339   loss = 3.0667388317980224
2021-08-08 18:09:49,339   rep_loss = 0.865351790602073
2021-08-08 18:09:49,339 ***** Save model *****
2021-08-08 18:09:55,305 ***** Running evaluation *****
2021-08-08 18:09:55,306   Epoch = 0 iter 10299 step
2021-08-08 18:09:55,306   Num examples = 9815
2021-08-08 18:09:55,306   Batch size = 32
2021-08-08 18:09:55,306 ***** Eval results *****
2021-08-08 18:09:55,307   att_loss = 2.2008558347834257
2021-08-08 18:09:55,307   cls_loss = 0.0
2021-08-08 18:09:55,307   global_step = 10299
2021-08-08 18:09:55,307   loss = 3.066109477738661
2021-08-08 18:09:55,307   rep_loss = 0.8652536425790528
2021-08-08 18:09:55,307 ***** Save model *****
2021-08-08 18:10:01,242 ***** Running evaluation *****
2021-08-08 18:10:01,243   Epoch = 0 iter 10349 step
2021-08-08 18:10:01,243   Num examples = 9815
2021-08-08 18:10:01,243   Batch size = 32
2021-08-08 18:10:01,243 ***** Eval results *****
2021-08-08 18:10:01,244   att_loss = 2.2005303572290633
2021-08-08 18:10:01,244   cls_loss = 0.0
2021-08-08 18:10:01,244   global_step = 10349
2021-08-08 18:10:01,244   loss = 3.065710142944525
2021-08-08 18:10:01,244   rep_loss = 0.8651797853814126
2021-08-08 18:10:01,244 ***** Save model *****
2021-08-08 18:10:07,195 ***** Running evaluation *****
2021-08-08 18:10:07,196   Epoch = 0 iter 10399 step
2021-08-08 18:10:07,196   Num examples = 9815
2021-08-08 18:10:07,196   Batch size = 32
2021-08-08 18:10:07,197 ***** Eval results *****
2021-08-08 18:10:07,197   att_loss = 2.200084166339251
2021-08-08 18:10:07,197   cls_loss = 0.0
2021-08-08 18:10:07,197   global_step = 10399
2021-08-08 18:10:07,197   loss = 3.0651849044814843
2021-08-08 18:10:07,197   rep_loss = 0.8651007379244263
2021-08-08 18:10:07,198 ***** Save model *****
2021-08-08 18:10:14,975 ***** Running evaluation *****
2021-08-08 18:10:14,975   Epoch = 0 iter 10449 step
2021-08-08 18:10:14,975   Num examples = 9815
2021-08-08 18:10:14,975   Batch size = 32
2021-08-08 18:10:14,976 ***** Eval results *****
2021-08-08 18:10:14,976   att_loss = 2.199642427264931
2021-08-08 18:10:14,976   cls_loss = 0.0
2021-08-08 18:10:14,976   global_step = 10449
2021-08-08 18:10:14,976   loss = 3.064650953529052
2021-08-08 18:10:14,976   rep_loss = 0.8650085261043995
2021-08-08 18:10:14,976 ***** Save model *****
2021-08-08 18:10:20,928 ***** Running evaluation *****
2021-08-08 18:10:20,928   Epoch = 0 iter 10499 step
2021-08-08 18:10:20,928   Num examples = 9815
2021-08-08 18:10:20,928   Batch size = 32
2021-08-08 18:10:20,929 ***** Eval results *****
2021-08-08 18:10:20,929   att_loss = 2.1989991271571894
2021-08-08 18:10:20,929   cls_loss = 0.0
2021-08-08 18:10:20,929   global_step = 10499
2021-08-08 18:10:20,929   loss = 3.0639091109148557
2021-08-08 18:10:20,929   rep_loss = 0.8649099835192252
2021-08-08 18:10:20,929 ***** Save model *****
2021-08-08 18:10:26,888 ***** Running evaluation *****
2021-08-08 18:10:26,888   Epoch = 0 iter 10549 step
2021-08-08 18:10:26,888   Num examples = 9815
2021-08-08 18:10:26,888   Batch size = 32
2021-08-08 18:10:26,889 ***** Eval results *****
2021-08-08 18:10:26,889   att_loss = 2.1985094680843993
2021-08-08 18:10:26,889   cls_loss = 0.0
2021-08-08 18:10:26,889   global_step = 10549
2021-08-08 18:10:26,889   loss = 3.063325153367645
2021-08-08 18:10:26,889   rep_loss = 0.8648156850572355
2021-08-08 18:10:26,890 ***** Save model *****
2021-08-08 18:10:33,578 ***** Running evaluation *****
2021-08-08 18:10:33,578   Epoch = 0 iter 10599 step
2021-08-08 18:10:33,578   Num examples = 9815
2021-08-08 18:10:33,578   Batch size = 32
2021-08-08 18:10:33,579 ***** Eval results *****
2021-08-08 18:10:33,579   att_loss = 2.1980007522026765
2021-08-08 18:10:33,579   cls_loss = 0.0
2021-08-08 18:10:33,579   global_step = 10599
2021-08-08 18:10:33,579   loss = 3.062733439524586
2021-08-08 18:10:33,580   rep_loss = 0.8647326871025888
2021-08-08 18:10:33,580 ***** Save model *****
2021-08-08 18:10:39,642 ***** Running evaluation *****
2021-08-08 18:10:39,642   Epoch = 0 iter 10649 step
2021-08-08 18:10:39,642   Num examples = 9815
2021-08-08 18:10:39,642   Batch size = 32
2021-08-08 18:10:39,643 ***** Eval results *****
2021-08-08 18:10:39,643   att_loss = 2.1974636130360263
2021-08-08 18:10:39,643   cls_loss = 0.0
2021-08-08 18:10:39,643   global_step = 10649
2021-08-08 18:10:39,643   loss = 3.062100268314787
2021-08-08 18:10:39,643   rep_loss = 0.8646366550492752
2021-08-08 18:10:39,643 ***** Save model *****
2021-08-08 18:10:45,621 ***** Running evaluation *****
2021-08-08 18:10:45,621   Epoch = 0 iter 10699 step
2021-08-08 18:10:45,621   Num examples = 9815
2021-08-08 18:10:45,621   Batch size = 32
2021-08-08 18:10:45,622 ***** Eval results *****
2021-08-08 18:10:45,622   att_loss = 2.1970045985724505
2021-08-08 18:10:45,622   cls_loss = 0.0
2021-08-08 18:10:45,622   global_step = 10699
2021-08-08 18:10:45,622   loss = 3.0615612686833598
2021-08-08 18:10:45,622   rep_loss = 0.8645566698769251
2021-08-08 18:10:45,623 ***** Save model *****
2021-08-08 18:10:51,560 ***** Running evaluation *****
2021-08-08 18:10:51,560   Epoch = 0 iter 10749 step
2021-08-08 18:10:51,560   Num examples = 9815
2021-08-08 18:10:51,560   Batch size = 32
2021-08-08 18:10:51,561 ***** Eval results *****
2021-08-08 18:10:51,561   att_loss = 2.196753371880879
2021-08-08 18:10:51,561   cls_loss = 0.0
2021-08-08 18:10:51,561   global_step = 10749
2021-08-08 18:10:51,561   loss = 3.061241836345565
2021-08-08 18:10:51,561   rep_loss = 0.8644884642151547
2021-08-08 18:10:51,562 ***** Save model *****
2021-08-08 18:10:57,485 ***** Running evaluation *****
2021-08-08 18:10:57,486   Epoch = 0 iter 10799 step
2021-08-08 18:10:57,486   Num examples = 9815
2021-08-08 18:10:57,486   Batch size = 32
2021-08-08 18:10:57,487 ***** Eval results *****
2021-08-08 18:10:57,487   att_loss = 2.1962802566988775
2021-08-08 18:10:57,487   cls_loss = 0.0
2021-08-08 18:10:57,487   global_step = 10799
2021-08-08 18:10:57,487   loss = 3.060684212513219
2021-08-08 18:10:57,487   rep_loss = 0.8644039554942128
2021-08-08 18:10:57,487 ***** Save model *****
2021-08-08 18:11:03,381 ***** Running evaluation *****
2021-08-08 18:11:03,381   Epoch = 0 iter 10849 step
2021-08-08 18:11:03,381   Num examples = 9815
2021-08-08 18:11:03,381   Batch size = 32
2021-08-08 18:11:03,382 ***** Eval results *****
2021-08-08 18:11:03,382   att_loss = 2.195975084449412
2021-08-08 18:11:03,382   cls_loss = 0.0
2021-08-08 18:11:03,382   global_step = 10849
2021-08-08 18:11:03,382   loss = 3.0603121339434227
2021-08-08 18:11:03,382   rep_loss = 0.8643370491863451
2021-08-08 18:11:03,383 ***** Save model *****
2021-08-08 18:11:09,452 ***** Running evaluation *****
2021-08-08 18:11:09,452   Epoch = 0 iter 10899 step
2021-08-08 18:11:09,452   Num examples = 9815
2021-08-08 18:11:09,452   Batch size = 32
2021-08-08 18:11:09,453 ***** Eval results *****
2021-08-08 18:11:09,453   att_loss = 2.1954550032266096
2021-08-08 18:11:09,453   cls_loss = 0.0
2021-08-08 18:11:09,453   global_step = 10899
2021-08-08 18:11:09,453   loss = 3.0597032637035686
2021-08-08 18:11:09,453   rep_loss = 0.8642482601816429
2021-08-08 18:11:09,453 ***** Save model *****
2021-08-08 18:11:15,358 ***** Running evaluation *****
2021-08-08 18:11:15,358   Epoch = 0 iter 10949 step
2021-08-08 18:11:15,359   Num examples = 9815
2021-08-08 18:11:15,359   Batch size = 32
2021-08-08 18:11:15,359 ***** Eval results *****
2021-08-08 18:11:15,359   att_loss = 2.1950495940864396
2021-08-08 18:11:15,359   cls_loss = 0.0
2021-08-08 18:11:15,359   global_step = 10949
2021-08-08 18:11:15,360   loss = 3.0592225211343216
2021-08-08 18:11:15,360   rep_loss = 0.864172926737583
2021-08-08 18:11:15,360 ***** Save model *****
2021-08-08 18:11:21,244 ***** Running evaluation *****
2021-08-08 18:11:21,245   Epoch = 0 iter 10999 step
2021-08-08 18:11:21,245   Num examples = 9815
2021-08-08 18:11:21,245   Batch size = 32
2021-08-08 18:11:21,245 ***** Eval results *****
2021-08-08 18:11:21,246   att_loss = 2.1944291894138264
2021-08-08 18:11:21,246   cls_loss = 0.0
2021-08-08 18:11:21,246   global_step = 10999
2021-08-08 18:11:21,246   loss = 3.058509007075969
2021-08-08 18:11:21,246   rep_loss = 0.8640798173640921
2021-08-08 18:11:21,246 ***** Save model *****
2021-08-08 18:11:28,135 ***** Running evaluation *****
2021-08-08 18:11:28,135   Epoch = 0 iter 11049 step
2021-08-08 18:11:28,135   Num examples = 9815
2021-08-08 18:11:28,135   Batch size = 32
2021-08-08 18:11:28,136 ***** Eval results *****
2021-08-08 18:11:28,136   att_loss = 2.1940499168465566
2021-08-08 18:11:28,136   cls_loss = 0.0
2021-08-08 18:11:28,136   global_step = 11049
2021-08-08 18:11:28,136   loss = 3.0580660342652286
2021-08-08 18:11:28,136   rep_loss = 0.864016117143549
2021-08-08 18:11:28,136 ***** Save model *****
2021-08-08 18:11:34,483 ***** Running evaluation *****
2021-08-08 18:11:34,483   Epoch = 0 iter 11099 step
2021-08-08 18:11:34,484   Num examples = 9815
2021-08-08 18:11:34,484   Batch size = 32
2021-08-08 18:11:34,484 ***** Eval results *****
2021-08-08 18:11:34,484   att_loss = 2.1938838148151434
2021-08-08 18:11:34,485   cls_loss = 0.0
2021-08-08 18:11:34,485   global_step = 11099
2021-08-08 18:11:34,485   loss = 3.0578368411471
2021-08-08 18:11:34,485   rep_loss = 0.863953026084924
2021-08-08 18:11:34,485 ***** Save model *****
2021-08-08 18:11:40,746 ***** Running evaluation *****
2021-08-08 18:11:40,746   Epoch = 0 iter 11149 step
2021-08-08 18:11:40,746   Num examples = 9815
2021-08-08 18:11:40,747   Batch size = 32
2021-08-08 18:11:40,747 ***** Eval results *****
2021-08-08 18:11:40,747   att_loss = 2.1935536491570424
2021-08-08 18:11:40,747   cls_loss = 0.0
2021-08-08 18:11:40,747   global_step = 11149
2021-08-08 18:11:40,747   loss = 3.057434555019153
2021-08-08 18:11:40,747   rep_loss = 0.8638809056803404
2021-08-08 18:11:40,748 ***** Save model *****
2021-08-08 18:11:48,713 ***** Running evaluation *****
2021-08-08 18:11:48,713   Epoch = 0 iter 11199 step
2021-08-08 18:11:48,714   Num examples = 9815
2021-08-08 18:11:48,714   Batch size = 32
2021-08-08 18:11:48,714 ***** Eval results *****
2021-08-08 18:11:48,714   att_loss = 2.1934165026803285
2021-08-08 18:11:48,714   cls_loss = 0.0
2021-08-08 18:11:48,715   global_step = 11199
2021-08-08 18:11:48,715   loss = 3.0572479736039613
2021-08-08 18:11:48,715   rep_loss = 0.8638314707107401
2021-08-08 18:11:48,715 ***** Save model *****
2021-08-08 18:11:54,635 ***** Running evaluation *****
2021-08-08 18:11:54,635   Epoch = 0 iter 11249 step
2021-08-08 18:11:54,635   Num examples = 9815
2021-08-08 18:11:54,635   Batch size = 32
2021-08-08 18:11:54,636 ***** Eval results *****
2021-08-08 18:11:54,636   att_loss = 2.1930381659137184
2021-08-08 18:11:54,636   cls_loss = 0.0
2021-08-08 18:11:54,636   global_step = 11249
2021-08-08 18:11:54,636   loss = 3.056801002537879
2021-08-08 18:11:54,636   rep_loss = 0.8637628364228113
2021-08-08 18:11:54,636 ***** Save model *****
2021-08-08 18:12:00,548 ***** Running evaluation *****
2021-08-08 18:12:00,548   Epoch = 0 iter 11299 step
2021-08-08 18:12:00,548   Num examples = 9815
2021-08-08 18:12:00,549   Batch size = 32
2021-08-08 18:12:00,549 ***** Eval results *****
2021-08-08 18:12:00,549   att_loss = 2.19277532852501
2021-08-08 18:12:00,549   cls_loss = 0.0
2021-08-08 18:12:00,549   global_step = 11299
2021-08-08 18:12:00,549   loss = 3.0564775272580054
2021-08-08 18:12:00,550   rep_loss = 0.8637021985378125
2021-08-08 18:12:00,550 ***** Save model *****
2021-08-08 18:12:06,479 ***** Running evaluation *****
2021-08-08 18:12:06,480   Epoch = 0 iter 11349 step
2021-08-08 18:12:06,480   Num examples = 9815
2021-08-08 18:12:06,480   Batch size = 32
2021-08-08 18:12:06,481 ***** Eval results *****
2021-08-08 18:12:06,481   att_loss = 2.1923881028757104
2021-08-08 18:12:06,481   cls_loss = 0.0
2021-08-08 18:12:06,481   global_step = 11349
2021-08-08 18:12:06,481   loss = 3.056016477028886
2021-08-08 18:12:06,481   rep_loss = 0.8636283740271282
2021-08-08 18:12:06,482 ***** Save model *****
2021-08-08 18:12:12,446 ***** Running evaluation *****
2021-08-08 18:12:12,447   Epoch = 0 iter 11399 step
2021-08-08 18:12:12,447   Num examples = 9815
2021-08-08 18:12:12,447   Batch size = 32
2021-08-08 18:12:12,448 ***** Eval results *****
2021-08-08 18:12:12,448   att_loss = 2.1920411975689755
2021-08-08 18:12:12,448   cls_loss = 0.0
2021-08-08 18:12:12,448   global_step = 11399
2021-08-08 18:12:12,448   loss = 3.055586864136951
2021-08-08 18:12:12,448   rep_loss = 0.8635456664477098
2021-08-08 18:12:12,448 ***** Save model *****
2021-08-08 18:12:18,391 ***** Running evaluation *****
2021-08-08 18:12:18,391   Epoch = 0 iter 11449 step
2021-08-08 18:12:18,392   Num examples = 9815
2021-08-08 18:12:18,392   Batch size = 32
2021-08-08 18:12:18,392 ***** Eval results *****
2021-08-08 18:12:18,392   att_loss = 2.1916832378498072
2021-08-08 18:12:18,392   cls_loss = 0.0
2021-08-08 18:12:18,392   global_step = 11449
2021-08-08 18:12:18,393   loss = 3.0551552656530054
2021-08-08 18:12:18,393   rep_loss = 0.863472027688664
2021-08-08 18:12:18,393 ***** Save model *****
2021-08-08 18:12:24,558 ***** Running evaluation *****
2021-08-08 18:12:24,558   Epoch = 0 iter 11499 step
2021-08-08 18:12:24,558   Num examples = 9815
2021-08-08 18:12:24,559   Batch size = 32
2021-08-08 18:12:24,559 ***** Eval results *****
2021-08-08 18:12:24,559   att_loss = 2.1912184545025326
2021-08-08 18:12:24,559   cls_loss = 0.0
2021-08-08 18:12:24,560   global_step = 11499
2021-08-08 18:12:24,560   loss = 3.0546119217458356
2021-08-08 18:12:24,560   rep_loss = 0.8633934671603677
2021-08-08 18:12:24,560 ***** Save model *****
2021-08-08 18:12:31,065 ***** Running evaluation *****
2021-08-08 18:12:31,066   Epoch = 0 iter 11549 step
2021-08-08 18:12:31,066   Num examples = 9815
2021-08-08 18:12:31,066   Batch size = 32
2021-08-08 18:12:31,066 ***** Eval results *****
2021-08-08 18:12:31,067   att_loss = 2.191080432215215
2021-08-08 18:12:31,067   cls_loss = 0.0
2021-08-08 18:12:31,067   global_step = 11549
2021-08-08 18:12:31,067   loss = 3.0544184948502293
2021-08-08 18:12:31,067   rep_loss = 0.863338062557599
2021-08-08 18:12:31,067 ***** Save model *****
2021-08-08 18:12:37,050 ***** Running evaluation *****
2021-08-08 18:12:37,050   Epoch = 0 iter 11599 step
2021-08-08 18:12:37,050   Num examples = 9815
2021-08-08 18:12:37,050   Batch size = 32
2021-08-08 18:12:37,051 ***** Eval results *****
2021-08-08 18:12:37,051   att_loss = 2.190611750342248
2021-08-08 18:12:37,051   cls_loss = 0.0
2021-08-08 18:12:37,051   global_step = 11599
2021-08-08 18:12:37,051   loss = 3.0538641437907414
2021-08-08 18:12:37,051   rep_loss = 0.8632523933457181
2021-08-08 18:12:37,051 ***** Save model *****
2021-08-08 18:12:43,025 ***** Running evaluation *****
2021-08-08 18:12:43,025   Epoch = 0 iter 11649 step
2021-08-08 18:12:43,025   Num examples = 9815
2021-08-08 18:12:43,025   Batch size = 32
2021-08-08 18:12:43,026 ***** Eval results *****
2021-08-08 18:12:43,026   att_loss = 2.1899691755162336
2021-08-08 18:12:43,026   cls_loss = 0.0
2021-08-08 18:12:43,026   global_step = 11649
2021-08-08 18:12:43,026   loss = 3.0531246692021523
2021-08-08 18:12:43,026   rep_loss = 0.863155493532417
2021-08-08 18:12:43,027 ***** Save model *****
2021-08-08 18:12:48,987 ***** Running evaluation *****
2021-08-08 18:12:48,988   Epoch = 0 iter 11699 step
2021-08-08 18:12:48,988   Num examples = 9815
2021-08-08 18:12:48,988   Batch size = 32
2021-08-08 18:12:48,989 ***** Eval results *****
2021-08-08 18:12:48,989   att_loss = 2.189813734007774
2021-08-08 18:12:48,989   cls_loss = 0.0
2021-08-08 18:12:48,989   global_step = 11699
2021-08-08 18:12:48,989   loss = 3.052904538572706
2021-08-08 18:12:48,989   rep_loss = 0.863090804412086
2021-08-08 18:12:48,989 ***** Save model *****
2021-08-08 18:12:57,009 ***** Running evaluation *****
2021-08-08 18:12:57,009   Epoch = 0 iter 11749 step
2021-08-08 18:12:57,009   Num examples = 9815
2021-08-08 18:12:57,009   Batch size = 32
2021-08-08 18:12:57,010 ***** Eval results *****
2021-08-08 18:12:57,010   att_loss = 2.1893907650469395
2021-08-08 18:12:57,010   cls_loss = 0.0
2021-08-08 18:12:57,010   global_step = 11749
2021-08-08 18:12:57,010   loss = 3.0524048671913975
2021-08-08 18:12:57,010   rep_loss = 0.8630141020125557
2021-08-08 18:12:57,011 ***** Save model *****
2021-08-08 18:13:02,953 ***** Running evaluation *****
2021-08-08 18:13:02,953   Epoch = 0 iter 11799 step
2021-08-08 18:13:02,953   Num examples = 9815
2021-08-08 18:13:02,953   Batch size = 32
2021-08-08 18:13:02,954 ***** Eval results *****
2021-08-08 18:13:02,954   att_loss = 2.18895383384917
2021-08-08 18:13:02,954   cls_loss = 0.0
2021-08-08 18:13:02,954   global_step = 11799
2021-08-08 18:13:02,954   loss = 3.0518933132206225
2021-08-08 18:13:02,955   rep_loss = 0.8629394792754708
2021-08-08 18:13:02,955 ***** Save model *****
2021-08-08 18:13:08,910 ***** Running evaluation *****
2021-08-08 18:13:08,910   Epoch = 0 iter 11849 step
2021-08-08 18:13:08,910   Num examples = 9815
2021-08-08 18:13:08,910   Batch size = 32
2021-08-08 18:13:08,911 ***** Eval results *****
2021-08-08 18:13:08,911   att_loss = 2.188504384050732
2021-08-08 18:13:08,911   cls_loss = 0.0
2021-08-08 18:13:08,911   global_step = 11849
2021-08-08 18:13:08,911   loss = 3.0513692003371333
2021-08-08 18:13:08,911   rep_loss = 0.8628648161807637
2021-08-08 18:13:08,911 ***** Save model *****
2021-08-08 18:13:14,873 ***** Running evaluation *****
2021-08-08 18:13:14,873   Epoch = 0 iter 11899 step
2021-08-08 18:13:14,873   Num examples = 9815
2021-08-08 18:13:14,873   Batch size = 32
2021-08-08 18:13:14,874 ***** Eval results *****
2021-08-08 18:13:14,874   att_loss = 2.1882980313859313
2021-08-08 18:13:14,874   cls_loss = 0.0
2021-08-08 18:13:14,874   global_step = 11899
2021-08-08 18:13:14,874   loss = 3.051099422388953
2021-08-08 18:13:14,875   rep_loss = 0.8628013908878095
2021-08-08 18:13:14,875 ***** Save model *****
2021-08-08 18:13:20,809 ***** Running evaluation *****
2021-08-08 18:13:20,809   Epoch = 0 iter 11949 step
2021-08-08 18:13:20,809   Num examples = 9815
2021-08-08 18:13:20,809   Batch size = 32
2021-08-08 18:13:20,810 ***** Eval results *****
2021-08-08 18:13:20,810   att_loss = 2.1880534895394814
2021-08-08 18:13:20,810   cls_loss = 0.0
2021-08-08 18:13:20,810   global_step = 11949
2021-08-08 18:13:20,810   loss = 3.0508057007241702
2021-08-08 18:13:20,810   rep_loss = 0.862752211109865
2021-08-08 18:13:20,811 ***** Save model *****
2021-08-08 18:13:27,407 ***** Running evaluation *****
2021-08-08 18:13:27,407   Epoch = 0 iter 11999 step
2021-08-08 18:13:27,407   Num examples = 9815
2021-08-08 18:13:27,408   Batch size = 32
2021-08-08 18:13:27,408 ***** Eval results *****
2021-08-08 18:13:27,408   att_loss = 2.187667331136021
2021-08-08 18:13:27,408   cls_loss = 0.0
2021-08-08 18:13:27,408   global_step = 11999
2021-08-08 18:13:27,408   loss = 3.050344349553321
2021-08-08 18:13:27,409   rep_loss = 0.8626770183378204
2021-08-08 18:13:27,409 ***** Save model *****
2021-08-08 18:13:33,394 ***** Running evaluation *****
2021-08-08 18:13:33,394   Epoch = 0 iter 12049 step
2021-08-08 18:13:33,394   Num examples = 9815
2021-08-08 18:13:33,394   Batch size = 32
2021-08-08 18:13:33,395 ***** Eval results *****
2021-08-08 18:13:33,395   att_loss = 2.1873915542635305
2021-08-08 18:13:33,395   cls_loss = 0.0
2021-08-08 18:13:33,395   global_step = 12049
2021-08-08 18:13:33,395   loss = 3.0500126952540874
2021-08-08 18:13:33,395   rep_loss = 0.8626211409064604
2021-08-08 18:13:33,395 ***** Save model *****
2021-08-08 18:13:39,340 ***** Running evaluation *****
2021-08-08 18:13:39,340   Epoch = 0 iter 12099 step
2021-08-08 18:13:39,340   Num examples = 9815
2021-08-08 18:13:39,340   Batch size = 32
2021-08-08 18:13:39,341 ***** Eval results *****
2021-08-08 18:13:39,341   att_loss = 2.1872465676321
2021-08-08 18:13:39,341   cls_loss = 0.0
2021-08-08 18:13:39,341   global_step = 12099
2021-08-08 18:13:39,341   loss = 3.049817204524667
2021-08-08 18:13:39,341   rep_loss = 0.8625706367891124
2021-08-08 18:13:39,342 ***** Save model *****
2021-08-08 18:13:45,279 ***** Running evaluation *****
2021-08-08 18:13:45,280   Epoch = 0 iter 12149 step
2021-08-08 18:13:45,280   Num examples = 9815
2021-08-08 18:13:45,280   Batch size = 32
2021-08-08 18:13:45,281 ***** Eval results *****
2021-08-08 18:13:45,281   att_loss = 2.1869971061775306
2021-08-08 18:13:45,281   cls_loss = 0.0
2021-08-08 18:13:45,281   global_step = 12149
2021-08-08 18:13:45,281   loss = 3.049513623296248
2021-08-08 18:13:45,281   rep_loss = 0.8625165170107822
2021-08-08 18:13:45,281 ***** Save model *****
2021-08-08 18:13:51,209 ***** Running evaluation *****
2021-08-08 18:13:51,209   Epoch = 0 iter 12199 step
2021-08-08 18:13:51,209   Num examples = 9815
2021-08-08 18:13:51,209   Batch size = 32
2021-08-08 18:13:51,210 ***** Eval results *****
2021-08-08 18:13:51,210   att_loss = 2.1866410024362133
2021-08-08 18:13:51,210   cls_loss = 0.0
2021-08-08 18:13:51,210   global_step = 12199
2021-08-08 18:13:51,210   loss = 3.049086657261514
2021-08-08 18:13:51,210   rep_loss = 0.8624456546787199
2021-08-08 18:13:51,210 ***** Save model *****
2021-08-08 18:13:57,359 ***** Running evaluation *****
2021-08-08 18:13:57,359   Epoch = 0 iter 12249 step
2021-08-08 18:13:57,360   Num examples = 9815
2021-08-08 18:13:57,360   Batch size = 32
2021-08-08 18:13:57,360 ***** Eval results *****
2021-08-08 18:13:57,360   att_loss = 2.186469920334188
2021-08-08 18:13:57,360   cls_loss = 0.0
2021-08-08 18:13:57,360   global_step = 12249
2021-08-08 18:13:57,361   loss = 3.048862679013039
2021-08-08 18:13:57,361   rep_loss = 0.8623927585961274
2021-08-08 18:13:57,361 ***** Save model *****
2021-08-08 18:14:03,271 ***** Running evaluation *****
2021-08-08 18:14:03,272   Epoch = 0 iter 12299 step
2021-08-08 18:14:03,272   Num examples = 9815
2021-08-08 18:14:03,272   Batch size = 32
2021-08-08 18:14:03,272 ***** Eval results *****
2021-08-08 18:14:03,272   att_loss = 2.186107203498857
2021-08-08 18:14:03,272   cls_loss = 0.0
2021-08-08 18:14:03,273   global_step = 12299
2021-08-08 18:14:03,273   loss = 3.0484333843979003
2021-08-08 18:14:03,273   rep_loss = 0.8623261807972711
2021-08-08 18:14:03,273 ***** Save model *****
2021-08-08 18:14:09,162 ***** Running evaluation *****
2021-08-08 18:14:09,162   Epoch = 0 iter 12349 step
2021-08-08 18:14:09,162   Num examples = 9815
2021-08-08 18:14:09,162   Batch size = 32
2021-08-08 18:14:09,163 ***** Eval results *****
2021-08-08 18:14:09,163   att_loss = 2.185702489748572
2021-08-08 18:14:09,163   cls_loss = 0.0
2021-08-08 18:14:09,163   global_step = 12349
2021-08-08 18:14:09,163   loss = 3.047957793509239
2021-08-08 18:14:09,163   rep_loss = 0.8622553036786137
2021-08-08 18:14:09,163 ***** Save model *****
2021-08-08 18:14:15,983 ***** Running evaluation *****
2021-08-08 18:14:15,983   Epoch = 0 iter 12399 step
2021-08-08 18:14:15,983   Num examples = 9815
2021-08-08 18:14:15,983   Batch size = 32
2021-08-08 18:14:15,984 ***** Eval results *****
2021-08-08 18:14:15,984   att_loss = 2.185246892499966
2021-08-08 18:14:15,984   cls_loss = 0.0
2021-08-08 18:14:15,984   global_step = 12399
2021-08-08 18:14:15,984   loss = 3.0474276657267554
2021-08-08 18:14:15,984   rep_loss = 0.8621807732267893
2021-08-08 18:14:15,984 ***** Save model *****
2021-08-08 18:14:21,908 ***** Running evaluation *****
2021-08-08 18:14:21,908   Epoch = 0 iter 12449 step
2021-08-08 18:14:21,908   Num examples = 9815
2021-08-08 18:14:21,908   Batch size = 32
2021-08-08 18:14:21,909 ***** Eval results *****
2021-08-08 18:14:21,909   att_loss = 2.184907562532179
2021-08-08 18:14:21,909   cls_loss = 0.0
2021-08-08 18:14:21,909   global_step = 12449
2021-08-08 18:14:21,909   loss = 3.0470199254786925
2021-08-08 18:14:21,909   rep_loss = 0.8621123629321495
2021-08-08 18:14:21,910 ***** Save model *****
2021-08-08 18:14:30,207 ***** Running evaluation *****
2021-08-08 18:14:30,207   Epoch = 0 iter 12499 step
2021-08-08 18:14:30,207   Num examples = 9815
2021-08-08 18:14:30,208   Batch size = 32
2021-08-08 18:14:30,208 ***** Eval results *****
2021-08-08 18:14:30,208   att_loss = 2.1846569674388343
2021-08-08 18:14:30,208   cls_loss = 0.0
2021-08-08 18:14:30,209   global_step = 12499
2021-08-08 18:14:30,209   loss = 3.046705010071002
2021-08-08 18:14:30,209   rep_loss = 0.8620480426083241
2021-08-08 18:14:30,209 ***** Save model *****
2021-08-08 18:14:36,229 ***** Running evaluation *****
2021-08-08 18:14:36,229   Epoch = 0 iter 12549 step
2021-08-08 18:14:36,229   Num examples = 9815
2021-08-08 18:14:36,229   Batch size = 32
2021-08-08 18:14:36,230 ***** Eval results *****
2021-08-08 18:14:36,230   att_loss = 2.18437535452723
2021-08-08 18:14:36,230   cls_loss = 0.0
2021-08-08 18:14:36,230   global_step = 12549
2021-08-08 18:14:36,230   loss = 3.046358075538226
2021-08-08 18:14:36,230   rep_loss = 0.8619827209919969
2021-08-08 18:14:36,231 ***** Save model *****
2021-08-08 18:14:42,285 ***** Running evaluation *****
2021-08-08 18:14:42,285   Epoch = 0 iter 12599 step
2021-08-08 18:14:42,285   Num examples = 9815
2021-08-08 18:14:42,285   Batch size = 32
2021-08-08 18:14:42,286 ***** Eval results *****
2021-08-08 18:14:42,286   att_loss = 2.184040291733813
2021-08-08 18:14:42,286   cls_loss = 0.0
2021-08-08 18:14:42,286   global_step = 12599
2021-08-08 18:14:42,286   loss = 3.0459664357504264
2021-08-08 18:14:42,286   rep_loss = 0.861926143931457
2021-08-08 18:14:42,286 ***** Save model *****
2021-08-08 18:14:48,312 ***** Running evaluation *****
2021-08-08 18:14:48,313   Epoch = 0 iter 12649 step
2021-08-08 18:14:48,313   Num examples = 9815
2021-08-08 18:14:48,313   Batch size = 32
2021-08-08 18:14:48,313 ***** Eval results *****
2021-08-08 18:14:48,314   att_loss = 2.1836430813749934
2021-08-08 18:14:48,314   cls_loss = 0.0
2021-08-08 18:14:48,314   global_step = 12649
2021-08-08 18:14:48,314   loss = 3.0454884340135617
2021-08-08 18:14:48,314   rep_loss = 0.8618453524972018
2021-08-08 18:14:48,314 ***** Save model *****
2021-08-08 18:14:54,275 ***** Running evaluation *****
2021-08-08 18:14:54,275   Epoch = 0 iter 12699 step
2021-08-08 18:14:54,275   Num examples = 9815
2021-08-08 18:14:54,275   Batch size = 32
2021-08-08 18:14:54,276 ***** Eval results *****
2021-08-08 18:14:54,276   att_loss = 2.1834908001572515
2021-08-08 18:14:54,276   cls_loss = 0.0
2021-08-08 18:14:54,276   global_step = 12699
2021-08-08 18:14:54,276   loss = 3.0452833627563676
2021-08-08 18:14:54,276   rep_loss = 0.861792562505243
2021-08-08 18:14:54,277 ***** Save model *****
2021-08-08 18:15:00,259 ***** Running evaluation *****
2021-08-08 18:15:00,259   Epoch = 0 iter 12749 step
2021-08-08 18:15:00,259   Num examples = 9815
2021-08-08 18:15:00,259   Batch size = 32
2021-08-08 18:15:00,260 ***** Eval results *****
2021-08-08 18:15:00,260   att_loss = 2.183112216235161
2021-08-08 18:15:00,260   cls_loss = 0.0
2021-08-08 18:15:00,260   global_step = 12749
2021-08-08 18:15:00,260   loss = 3.044834667490514
2021-08-08 18:15:00,260   rep_loss = 0.8617224511711987
2021-08-08 18:15:00,261 ***** Save model *****
2021-08-08 18:15:06,241 ***** Running evaluation *****
2021-08-08 18:15:06,241   Epoch = 0 iter 12799 step
2021-08-08 18:15:06,241   Num examples = 9815
2021-08-08 18:15:06,241   Batch size = 32
2021-08-08 18:15:06,242 ***** Eval results *****
2021-08-08 18:15:06,242   att_loss = 2.183012185622793
2021-08-08 18:15:06,242   cls_loss = 0.0
2021-08-08 18:15:06,242   global_step = 12799
2021-08-08 18:15:06,242   loss = 3.044695988028522
2021-08-08 18:15:06,242   rep_loss = 0.8616838022846478
2021-08-08 18:15:06,242 ***** Save model *****
2021-08-08 18:15:12,159 ***** Running evaluation *****
2021-08-08 18:15:12,159   Epoch = 0 iter 12849 step
2021-08-08 18:15:12,159   Num examples = 9815
2021-08-08 18:15:12,159   Batch size = 32
2021-08-08 18:15:12,160 ***** Eval results *****
2021-08-08 18:15:12,160   att_loss = 2.1828628377196706
2021-08-08 18:15:12,160   cls_loss = 0.0
2021-08-08 18:15:12,160   global_step = 12849
2021-08-08 18:15:12,160   loss = 3.044494821530055
2021-08-08 18:15:12,160   rep_loss = 0.8616319837083296
2021-08-08 18:15:12,161 ***** Save model *****
2021-08-08 18:15:18,089 ***** Running evaluation *****
2021-08-08 18:15:18,089   Epoch = 0 iter 12899 step
2021-08-08 18:15:18,089   Num examples = 9815
2021-08-08 18:15:18,090   Batch size = 32
2021-08-08 18:15:18,090 ***** Eval results *****
2021-08-08 18:15:18,090   att_loss = 2.182661567162022
2021-08-08 18:15:18,090   cls_loss = 0.0
2021-08-08 18:15:18,090   global_step = 12899
2021-08-08 18:15:18,090   loss = 3.0442344390825666
2021-08-08 18:15:18,090   rep_loss = 0.8615728718281273
2021-08-08 18:15:18,091 ***** Save model *****
2021-08-08 18:15:24,038 ***** Running evaluation *****
2021-08-08 18:15:24,039   Epoch = 0 iter 12949 step
2021-08-08 18:15:24,039   Num examples = 9815
2021-08-08 18:15:24,039   Batch size = 32
2021-08-08 18:15:24,039 ***** Eval results *****
2021-08-08 18:15:24,040   att_loss = 2.182353358110762
2021-08-08 18:15:24,040   cls_loss = 0.0
2021-08-08 18:15:24,040   global_step = 12949
2021-08-08 18:15:24,040   loss = 3.043860621448281
2021-08-08 18:15:24,040   rep_loss = 0.8615072631672069
2021-08-08 18:15:24,040 ***** Save model *****
2021-08-08 18:15:30,109 ***** Running evaluation *****
2021-08-08 18:15:30,109   Epoch = 0 iter 12999 step
2021-08-08 18:15:30,109   Num examples = 9815
2021-08-08 18:15:30,109   Batch size = 32
2021-08-08 18:15:30,110 ***** Eval results *****
2021-08-08 18:15:30,110   att_loss = 2.182161014624234
2021-08-08 18:15:30,110   cls_loss = 0.0
2021-08-08 18:15:30,110   global_step = 12999
2021-08-08 18:15:30,111   loss = 3.043613148392214
2021-08-08 18:15:30,111   rep_loss = 0.8614521336762737
2021-08-08 18:15:30,111 ***** Save model *****
2021-08-08 18:15:38,127 ***** Running evaluation *****
2021-08-08 18:15:38,128   Epoch = 0 iter 13049 step
2021-08-08 18:15:38,128   Num examples = 9815
2021-08-08 18:15:38,128   Batch size = 32
2021-08-08 18:15:38,129 ***** Eval results *****
2021-08-08 18:15:38,129   att_loss = 2.1819194872741616
2021-08-08 18:15:38,129   cls_loss = 0.0
2021-08-08 18:15:38,129   global_step = 13049
2021-08-08 18:15:38,129   loss = 3.043312243313906
2021-08-08 18:15:38,129   rep_loss = 0.8613927559894989
2021-08-08 18:15:38,129 ***** Save model *****
2021-08-08 18:15:44,089 ***** Running evaluation *****
2021-08-08 18:15:44,090   Epoch = 0 iter 13099 step
2021-08-08 18:15:44,090   Num examples = 9815
2021-08-08 18:15:44,090   Batch size = 32
2021-08-08 18:15:44,090 ***** Eval results *****
2021-08-08 18:15:44,091   att_loss = 2.1815657766004204
2021-08-08 18:15:44,091   cls_loss = 0.0
2021-08-08 18:15:44,091   global_step = 13099
2021-08-08 18:15:44,091   loss = 3.0428943990778308
2021-08-08 18:15:44,091   rep_loss = 0.8613286223864044
2021-08-08 18:15:44,091 ***** Save model *****
2021-08-08 18:15:50,046 ***** Running evaluation *****
2021-08-08 18:15:50,046   Epoch = 0 iter 13149 step
2021-08-08 18:15:50,046   Num examples = 9815
2021-08-08 18:15:50,046   Batch size = 32
2021-08-08 18:15:50,047 ***** Eval results *****
2021-08-08 18:15:50,047   att_loss = 2.1812594658757845
2021-08-08 18:15:50,047   cls_loss = 0.0
2021-08-08 18:15:50,047   global_step = 13149
2021-08-08 18:15:50,047   loss = 3.0425299523819427
2021-08-08 18:15:50,047   rep_loss = 0.8612704864064318
2021-08-08 18:15:50,047 ***** Save model *****
2021-08-08 18:15:56,164 ***** Running evaluation *****
2021-08-08 18:15:56,165   Epoch = 0 iter 13199 step
2021-08-08 18:15:56,165   Num examples = 9815
2021-08-08 18:15:56,165   Batch size = 32
2021-08-08 18:15:56,166 ***** Eval results *****
2021-08-08 18:15:56,166   att_loss = 2.18115554959388
2021-08-08 18:15:56,166   cls_loss = 0.0
2021-08-08 18:15:56,166   global_step = 13199
2021-08-08 18:15:56,166   loss = 3.042381709936451
2021-08-08 18:15:56,166   rep_loss = 0.8612261602522542
2021-08-08 18:15:56,166 ***** Save model *****
2021-08-08 18:16:02,167 ***** Running evaluation *****
2021-08-08 18:16:02,167   Epoch = 0 iter 13249 step
2021-08-08 18:16:02,168   Num examples = 9815
2021-08-08 18:16:02,168   Batch size = 32
2021-08-08 18:16:02,168 ***** Eval results *****
2021-08-08 18:16:02,168   att_loss = 2.181231136771542
2021-08-08 18:16:02,169   cls_loss = 0.0
2021-08-08 18:16:02,169   global_step = 13249
2021-08-08 18:16:02,169   loss = 3.0424205328313456
2021-08-08 18:16:02,169   rep_loss = 0.8611893960508061
2021-08-08 18:16:02,169 ***** Save model *****
2021-08-08 18:16:09,096 ***** Running evaluation *****
2021-08-08 18:16:09,097   Epoch = 0 iter 13299 step
2021-08-08 18:16:09,097   Num examples = 9815
2021-08-08 18:16:09,097   Batch size = 32
2021-08-08 18:16:09,143 ***** Eval results *****
2021-08-08 18:16:09,144   att_loss = 2.180820407325302
2021-08-08 18:16:09,144   cls_loss = 0.0
2021-08-08 18:16:09,144   global_step = 13299
2021-08-08 18:16:09,144   loss = 3.041942521175663
2021-08-08 18:16:09,144   rep_loss = 0.8611221138727702
2021-08-08 18:16:09,144 ***** Save model *****
2021-08-08 18:16:15,030 ***** Running evaluation *****
2021-08-08 18:16:15,030   Epoch = 0 iter 13349 step
2021-08-08 18:16:15,030   Num examples = 9815
2021-08-08 18:16:15,030   Batch size = 32
2021-08-08 18:16:15,031 ***** Eval results *****
2021-08-08 18:16:15,031   att_loss = 2.1804599081392064
2021-08-08 18:16:15,031   cls_loss = 0.0
2021-08-08 18:16:15,031   global_step = 13349
2021-08-08 18:16:15,031   loss = 3.0415228322272228
2021-08-08 18:16:15,031   rep_loss = 0.8610629240790862
2021-08-08 18:16:15,031 ***** Save model *****
2021-08-08 18:16:20,970 ***** Running evaluation *****
2021-08-08 18:16:20,970   Epoch = 0 iter 13399 step
2021-08-08 18:16:20,970   Num examples = 9815
2021-08-08 18:16:20,970   Batch size = 32
2021-08-08 18:16:20,971 ***** Eval results *****
2021-08-08 18:16:20,971   att_loss = 2.180262516208421
2021-08-08 18:16:20,971   cls_loss = 0.0
2021-08-08 18:16:20,971   global_step = 13399
2021-08-08 18:16:20,971   loss = 3.0412671429175515
2021-08-08 18:16:20,971   rep_loss = 0.8610046266913369
2021-08-08 18:16:20,971 ***** Save model *****
2021-08-08 18:16:26,909 ***** Running evaluation *****
2021-08-08 18:16:26,910   Epoch = 0 iter 13449 step
2021-08-08 18:16:26,910   Num examples = 9815
2021-08-08 18:16:26,910   Batch size = 32
2021-08-08 18:16:26,911 ***** Eval results *****
2021-08-08 18:16:26,911   att_loss = 2.179958453071331
2021-08-08 18:16:26,911   cls_loss = 0.0
2021-08-08 18:16:26,911   global_step = 13449
2021-08-08 18:16:26,911   loss = 3.04090022214665
2021-08-08 18:16:26,911   rep_loss = 0.8609417690442959
2021-08-08 18:16:26,911 ***** Save model *****
2021-08-08 18:16:32,816 ***** Running evaluation *****
2021-08-08 18:16:32,816   Epoch = 0 iter 13499 step
2021-08-08 18:16:32,816   Num examples = 9815
2021-08-08 18:16:32,816   Batch size = 32
2021-08-08 18:16:32,817 ***** Eval results *****
2021-08-08 18:16:32,817   att_loss = 2.179747546758552
2021-08-08 18:16:32,817   cls_loss = 0.0
2021-08-08 18:16:32,817   global_step = 13499
2021-08-08 18:16:32,817   loss = 3.040644896755449
2021-08-08 18:16:32,817   rep_loss = 0.8608973499836502
2021-08-08 18:16:32,817 ***** Save model *****
2021-08-08 18:16:39,262 ***** Running evaluation *****
2021-08-08 18:16:39,262   Epoch = 0 iter 13549 step
2021-08-08 18:16:39,262   Num examples = 9815
2021-08-08 18:16:39,262   Batch size = 32
2021-08-08 18:16:39,263 ***** Eval results *****
2021-08-08 18:16:39,263   att_loss = 2.17946965446841
2021-08-08 18:16:39,263   cls_loss = 0.0
2021-08-08 18:16:39,263   global_step = 13549
2021-08-08 18:16:39,263   loss = 3.0403077300601944
2021-08-08 18:16:39,263   rep_loss = 0.8608380756445746
2021-08-08 18:16:39,263 ***** Save model *****
2021-08-08 18:16:45,214 ***** Running evaluation *****
2021-08-08 18:16:45,215   Epoch = 0 iter 13599 step
2021-08-08 18:16:45,215   Num examples = 9815
2021-08-08 18:16:45,215   Batch size = 32
2021-08-08 18:16:45,215 ***** Eval results *****
2021-08-08 18:16:45,216   att_loss = 2.1791360321373823
2021-08-08 18:16:45,216   cls_loss = 0.0
2021-08-08 18:16:45,216   global_step = 13599
2021-08-08 18:16:45,216   loss = 3.0399083864146257
2021-08-08 18:16:45,216   rep_loss = 0.8607723543123074
2021-08-08 18:16:45,216 ***** Save model *****
2021-08-08 18:16:51,147 ***** Running evaluation *****
2021-08-08 18:16:51,147   Epoch = 0 iter 13649 step
2021-08-08 18:16:51,147   Num examples = 9815
2021-08-08 18:16:51,147   Batch size = 32
2021-08-08 18:16:51,148 ***** Eval results *****
2021-08-08 18:16:51,148   att_loss = 2.178680442088567
2021-08-08 18:16:51,148   cls_loss = 0.0
2021-08-08 18:16:51,148   global_step = 13649
2021-08-08 18:16:51,148   loss = 3.0393788696287385
2021-08-08 18:16:51,148   rep_loss = 0.8606984276362448
2021-08-08 18:16:51,149 ***** Save model *****
2021-08-08 18:16:57,062 ***** Running evaluation *****
2021-08-08 18:16:57,062   Epoch = 0 iter 13699 step
2021-08-08 18:16:57,062   Num examples = 9815
2021-08-08 18:16:57,062   Batch size = 32
2021-08-08 18:16:57,063 ***** Eval results *****
2021-08-08 18:16:57,063   att_loss = 2.1783167232570095
2021-08-08 18:16:57,063   cls_loss = 0.0
2021-08-08 18:16:57,063   global_step = 13699
2021-08-08 18:16:57,063   loss = 3.038949827754904
2021-08-08 18:16:57,063   rep_loss = 0.8606331045675107
2021-08-08 18:16:57,064 ***** Save model *****
2021-08-08 18:17:03,025 ***** Running evaluation *****
2021-08-08 18:17:03,026   Epoch = 0 iter 13749 step
2021-08-08 18:17:03,026   Num examples = 9815
2021-08-08 18:17:03,026   Batch size = 32
2021-08-08 18:17:03,026 ***** Eval results *****
2021-08-08 18:17:03,027   att_loss = 2.17793561574875
2021-08-08 18:17:03,027   cls_loss = 0.0
2021-08-08 18:17:03,027   global_step = 13749
2021-08-08 18:17:03,027   loss = 3.0384950413427507
2021-08-08 18:17:03,027   rep_loss = 0.8605594256460228
2021-08-08 18:17:03,027 ***** Save model *****
2021-08-08 18:17:11,035 ***** Running evaluation *****
2021-08-08 18:17:11,035   Epoch = 0 iter 13799 step
2021-08-08 18:17:11,035   Num examples = 9815
2021-08-08 18:17:11,035   Batch size = 32
2021-08-08 18:17:11,036 ***** Eval results *****
2021-08-08 18:17:11,036   att_loss = 2.1777416860308283
2021-08-08 18:17:11,036   cls_loss = 0.0
2021-08-08 18:17:11,036   global_step = 13799
2021-08-08 18:17:11,036   loss = 3.0382518049374605
2021-08-08 18:17:11,036   rep_loss = 0.8605101189411881
2021-08-08 18:17:11,037 ***** Save model *****
2021-08-08 18:17:16,973 ***** Running evaluation *****
2021-08-08 18:17:16,973   Epoch = 0 iter 13849 step
2021-08-08 18:17:16,973   Num examples = 9815
2021-08-08 18:17:16,973   Batch size = 32
2021-08-08 18:17:16,974 ***** Eval results *****
2021-08-08 18:17:16,974   att_loss = 2.1774350948321346
2021-08-08 18:17:16,974   cls_loss = 0.0
2021-08-08 18:17:16,974   global_step = 13849
2021-08-08 18:17:16,974   loss = 3.0378824575290464
2021-08-08 18:17:16,974   rep_loss = 0.8604473627743819
2021-08-08 18:17:16,975 ***** Save model *****
2021-08-08 18:17:22,884 ***** Running evaluation *****
2021-08-08 18:17:22,885   Epoch = 0 iter 13899 step
2021-08-08 18:17:22,885   Num examples = 9815
2021-08-08 18:17:22,885   Batch size = 32
2021-08-08 18:17:22,885 ***** Eval results *****
2021-08-08 18:17:22,885   att_loss = 2.1771024098987417
2021-08-08 18:17:22,886   cls_loss = 0.0
2021-08-08 18:17:22,886   global_step = 13899
2021-08-08 18:17:22,886   loss = 3.0374823173213086
2021-08-08 18:17:22,886   rep_loss = 0.8603799075254889
2021-08-08 18:17:22,886 ***** Save model *****
2021-08-08 18:17:29,090 ***** Running evaluation *****
2021-08-08 18:17:29,091   Epoch = 0 iter 13949 step
2021-08-08 18:17:29,091   Num examples = 9815
2021-08-08 18:17:29,091   Batch size = 32
2021-08-08 18:17:29,092 ***** Eval results *****
2021-08-08 18:17:29,092   att_loss = 2.176755789232319
2021-08-08 18:17:29,092   cls_loss = 0.0
2021-08-08 18:17:29,092   global_step = 13949
2021-08-08 18:17:29,092   loss = 3.037063463758556
2021-08-08 18:17:29,092   rep_loss = 0.8603076746587011
2021-08-08 18:17:29,092 ***** Save model *****
2021-08-08 18:17:34,989 ***** Running evaluation *****
2021-08-08 18:17:34,990   Epoch = 0 iter 13999 step
2021-08-08 18:17:34,990   Num examples = 9815
2021-08-08 18:17:34,990   Batch size = 32
2021-08-08 18:17:34,990 ***** Eval results *****
2021-08-08 18:17:34,991   att_loss = 2.1764056010283404
2021-08-08 18:17:34,991   cls_loss = 0.0
2021-08-08 18:17:34,991   global_step = 13999
2021-08-08 18:17:34,991   loss = 3.0366504066492355
2021-08-08 18:17:34,991   rep_loss = 0.8602448057571439
2021-08-08 18:17:34,991 ***** Save model *****
2021-08-08 18:17:40,892 ***** Running evaluation *****
2021-08-08 18:17:40,892   Epoch = 0 iter 14049 step
2021-08-08 18:17:40,892   Num examples = 9815
2021-08-08 18:17:40,892   Batch size = 32
2021-08-08 18:17:40,893 ***** Eval results *****
2021-08-08 18:17:40,893   att_loss = 2.1760959192750904
2021-08-08 18:17:40,893   cls_loss = 0.0
2021-08-08 18:17:40,893   global_step = 14049
2021-08-08 18:17:40,893   loss = 3.0362782011813425
2021-08-08 18:17:40,893   rep_loss = 0.8601822820505015
2021-08-08 18:17:40,894 ***** Save model *****
2021-08-08 18:17:46,826 ***** Running evaluation *****
2021-08-08 18:17:46,827   Epoch = 0 iter 14099 step
2021-08-08 18:17:46,827   Num examples = 9815
2021-08-08 18:17:46,827   Batch size = 32
2021-08-08 18:17:46,828 ***** Eval results *****
2021-08-08 18:17:46,828   att_loss = 2.1755673666272286
2021-08-08 18:17:46,828   cls_loss = 0.0
2021-08-08 18:17:46,828   global_step = 14099
2021-08-08 18:17:46,828   loss = 3.0356684294869116
2021-08-08 18:17:46,828   rep_loss = 0.8601010630076485
2021-08-08 18:17:46,828 ***** Save model *****
2021-08-08 18:17:52,738 ***** Running evaluation *****
2021-08-08 18:17:52,738   Epoch = 0 iter 14149 step
2021-08-08 18:17:52,738   Num examples = 9815
2021-08-08 18:17:52,738   Batch size = 32
2021-08-08 18:17:52,739 ***** Eval results *****
2021-08-08 18:17:52,739   att_loss = 2.1751345905265804
2021-08-08 18:17:52,739   cls_loss = 0.0
2021-08-08 18:17:52,739   global_step = 14149
2021-08-08 18:17:52,740   loss = 3.0351605636255083
2021-08-08 18:17:52,740   rep_loss = 0.8600259732800715
2021-08-08 18:17:52,740 ***** Save model *****
2021-08-08 18:17:59,102 ***** Running evaluation *****
2021-08-08 18:17:59,102   Epoch = 0 iter 14199 step
2021-08-08 18:17:59,102   Num examples = 9815
2021-08-08 18:17:59,102   Batch size = 32
2021-08-08 18:17:59,103 ***** Eval results *****
2021-08-08 18:17:59,103   att_loss = 2.1744697274637184
2021-08-08 18:17:59,103   cls_loss = 0.0
2021-08-08 18:17:59,103   global_step = 14199
2021-08-08 18:17:59,103   loss = 3.0344053855185122
2021-08-08 18:17:59,103   rep_loss = 0.859935658243695
2021-08-08 18:17:59,104 ***** Save model *****
2021-08-08 18:18:05,217 ***** Running evaluation *****
2021-08-08 18:18:05,217   Epoch = 0 iter 14249 step
2021-08-08 18:18:05,217   Num examples = 9815
2021-08-08 18:18:05,217   Batch size = 32
2021-08-08 18:18:05,218 ***** Eval results *****
2021-08-08 18:18:05,218   att_loss = 2.1739752029852246
2021-08-08 18:18:05,218   cls_loss = 0.0
2021-08-08 18:18:05,218   global_step = 14249
2021-08-08 18:18:05,218   loss = 3.033831698759471
2021-08-08 18:18:05,218   rep_loss = 0.8598564959290204
2021-08-08 18:18:05,218 ***** Save model *****
2021-08-08 18:18:11,114 ***** Running evaluation *****
2021-08-08 18:18:11,115   Epoch = 0 iter 14299 step
2021-08-08 18:18:11,115   Num examples = 9815
2021-08-08 18:18:11,115   Batch size = 32
2021-08-08 18:18:11,115 ***** Eval results *****
2021-08-08 18:18:11,115   att_loss = 2.1736538086715167
2021-08-08 18:18:11,116   cls_loss = 0.0
2021-08-08 18:18:11,116   global_step = 14299
2021-08-08 18:18:11,116   loss = 3.033455746400122
2021-08-08 18:18:11,116   rep_loss = 0.8598019378536589
2021-08-08 18:18:11,116 ***** Save model *****
2021-08-08 18:18:18,929 ***** Running evaluation *****
2021-08-08 18:18:18,929   Epoch = 0 iter 14349 step
2021-08-08 18:18:18,929   Num examples = 9815
2021-08-08 18:18:18,929   Batch size = 32
2021-08-08 18:18:18,930 ***** Eval results *****
2021-08-08 18:18:18,930   att_loss = 2.1734086492849367
2021-08-08 18:18:18,930   cls_loss = 0.0
2021-08-08 18:18:18,930   global_step = 14349
2021-08-08 18:18:18,930   loss = 3.033156713497947
2021-08-08 18:18:18,930   rep_loss = 0.8597480643210125
2021-08-08 18:18:18,930 ***** Save model *****
2021-08-08 18:18:24,958 ***** Running evaluation *****
2021-08-08 18:18:24,959   Epoch = 0 iter 14399 step
2021-08-08 18:18:24,959   Num examples = 9815
2021-08-08 18:18:24,959   Batch size = 32
2021-08-08 18:18:24,959 ***** Eval results *****
2021-08-08 18:18:24,959   att_loss = 2.173152738937496
2021-08-08 18:18:24,959   cls_loss = 0.0
2021-08-08 18:18:24,960   global_step = 14399
2021-08-08 18:18:24,960   loss = 3.0328534851223905
2021-08-08 18:18:24,960   rep_loss = 0.8597007462552657
2021-08-08 18:18:24,960 ***** Save model *****
2021-08-08 18:18:31,476 ***** Running evaluation *****
2021-08-08 18:18:31,476   Epoch = 0 iter 14449 step
2021-08-08 18:18:31,476   Num examples = 9815
2021-08-08 18:18:31,476   Batch size = 32
2021-08-08 18:18:31,477 ***** Eval results *****
2021-08-08 18:18:31,477   att_loss = 2.1727684786001435
2021-08-08 18:18:31,477   cls_loss = 0.0
2021-08-08 18:18:31,477   global_step = 14449
2021-08-08 18:18:31,477   loss = 3.032396568873488
2021-08-08 18:18:31,477   rep_loss = 0.8596280902980958
2021-08-08 18:18:31,477 ***** Save model *****
2021-08-08 18:18:37,422 ***** Running evaluation *****
2021-08-08 18:18:37,422   Epoch = 0 iter 14499 step
2021-08-08 18:18:37,422   Num examples = 9815
2021-08-08 18:18:37,422   Batch size = 32
2021-08-08 18:18:37,423 ***** Eval results *****
2021-08-08 18:18:37,423   att_loss = 2.1725614131898814
2021-08-08 18:18:37,423   cls_loss = 0.0
2021-08-08 18:18:37,423   global_step = 14499
2021-08-08 18:18:37,423   loss = 3.0321417390103917
2021-08-08 18:18:37,423   rep_loss = 0.8595803258122884
2021-08-08 18:18:37,424 ***** Save model *****
2021-08-08 18:18:46,364 ***** Running evaluation *****
2021-08-08 18:18:46,364   Epoch = 0 iter 14549 step
2021-08-08 18:18:46,364   Num examples = 9815
2021-08-08 18:18:46,364   Batch size = 32
2021-08-08 18:18:46,365 ***** Eval results *****
2021-08-08 18:18:46,365   att_loss = 2.172447623424214
2021-08-08 18:18:46,365   cls_loss = 0.0
2021-08-08 18:18:46,365   global_step = 14549
2021-08-08 18:18:46,365   loss = 3.0319783833110625
2021-08-08 18:18:46,365   rep_loss = 0.8595307598540739
2021-08-08 18:18:46,365 ***** Save model *****
2021-08-08 18:18:57,772 ***** Running evaluation *****
2021-08-08 18:18:57,772   Epoch = 0 iter 14599 step
2021-08-08 18:18:57,772   Num examples = 9815
2021-08-08 18:18:57,772   Batch size = 32
2021-08-08 18:18:57,773 ***** Eval results *****
2021-08-08 18:18:57,773   att_loss = 2.1719685807621016
2021-08-08 18:18:57,773   cls_loss = 0.0
2021-08-08 18:18:57,773   global_step = 14599
2021-08-08 18:18:57,773   loss = 3.0314228380695663
2021-08-08 18:18:57,773   rep_loss = 0.8594542572870508
2021-08-08 18:18:57,773 ***** Save model *****
2021-08-08 18:19:06,035 ***** Running evaluation *****
2021-08-08 18:19:06,035   Epoch = 0 iter 14649 step
2021-08-08 18:19:06,035   Num examples = 9815
2021-08-08 18:19:06,035   Batch size = 32
2021-08-08 18:19:06,036 ***** Eval results *****
2021-08-08 18:19:06,036   att_loss = 2.1717576965373304
2021-08-08 18:19:06,036   cls_loss = 0.0
2021-08-08 18:19:06,036   global_step = 14649
2021-08-08 18:19:06,036   loss = 3.0311516248062533
2021-08-08 18:19:06,036   rep_loss = 0.8593939282689227
2021-08-08 18:19:06,037 ***** Save model *****
2021-08-08 18:19:12,867 ***** Running evaluation *****
2021-08-08 18:19:12,867   Epoch = 0 iter 14699 step
2021-08-08 18:19:12,867   Num examples = 9815
2021-08-08 18:19:12,868   Batch size = 32
2021-08-08 18:19:12,868 ***** Eval results *****
2021-08-08 18:19:12,868   att_loss = 2.171549663623828
2021-08-08 18:19:12,868   cls_loss = 0.0
2021-08-08 18:19:12,868   global_step = 14699
2021-08-08 18:19:12,868   loss = 3.030894634618071
2021-08-08 18:19:12,868   rep_loss = 0.8593449710307377
2021-08-08 18:19:12,869 ***** Save model *****
2021-08-08 18:19:18,792 ***** Running evaluation *****
2021-08-08 18:19:18,792   Epoch = 0 iter 14749 step
2021-08-08 18:19:18,792   Num examples = 9815
2021-08-08 18:19:18,792   Batch size = 32
2021-08-08 18:19:18,793 ***** Eval results *****
2021-08-08 18:19:18,793   att_loss = 2.171174226287939
2021-08-08 18:19:18,793   cls_loss = 0.0
2021-08-08 18:19:18,793   global_step = 14749
2021-08-08 18:19:18,793   loss = 3.0304517015795587
2021-08-08 18:19:18,793   rep_loss = 0.8592774753966924
2021-08-08 18:19:18,793 ***** Save model *****
2021-08-08 18:19:24,721 ***** Running evaluation *****
2021-08-08 18:19:24,721   Epoch = 0 iter 14799 step
2021-08-08 18:19:24,721   Num examples = 9815
2021-08-08 18:19:24,721   Batch size = 32
2021-08-08 18:19:24,722 ***** Eval results *****
2021-08-08 18:19:24,722   att_loss = 2.1710498260077498
2021-08-08 18:19:24,722   cls_loss = 0.0
2021-08-08 18:19:24,722   global_step = 14799
2021-08-08 18:19:24,722   loss = 3.030279045016695
2021-08-08 18:19:24,722   rep_loss = 0.8592292191015803
2021-08-08 18:19:24,722 ***** Save model *****
2021-08-08 18:19:30,642 ***** Running evaluation *****
2021-08-08 18:19:30,643   Epoch = 0 iter 14849 step
2021-08-08 18:19:30,643   Num examples = 9815
2021-08-08 18:19:30,643   Batch size = 32
2021-08-08 18:19:30,643 ***** Eval results *****
2021-08-08 18:19:30,643   att_loss = 2.170845949807964
2021-08-08 18:19:30,644   cls_loss = 0.0
2021-08-08 18:19:30,644   global_step = 14849
2021-08-08 18:19:30,644   loss = 3.0300231376828806
2021-08-08 18:19:30,644   rep_loss = 0.8591771879511833
2021-08-08 18:19:30,644 ***** Save model *****
2021-08-08 18:19:36,553 ***** Running evaluation *****
2021-08-08 18:19:36,553   Epoch = 0 iter 14899 step
2021-08-08 18:19:36,553   Num examples = 9815
2021-08-08 18:19:36,554   Batch size = 32
2021-08-08 18:19:36,554 ***** Eval results *****
2021-08-08 18:19:36,554   att_loss = 2.170568124264933
2021-08-08 18:19:36,555   cls_loss = 0.0
2021-08-08 18:19:36,555   global_step = 14899
2021-08-08 18:19:36,555   loss = 3.0296911133304603
2021-08-08 18:19:36,555   rep_loss = 0.8591229891255363
2021-08-08 18:19:36,555 ***** Save model *****
2021-08-08 18:19:43,615 ***** Running evaluation *****
2021-08-08 18:19:43,616   Epoch = 0 iter 14949 step
2021-08-08 18:19:43,616   Num examples = 9815
2021-08-08 18:19:43,616   Batch size = 32
2021-08-08 18:19:43,617 ***** Eval results *****
2021-08-08 18:19:43,617   att_loss = 2.17036950585834
2021-08-08 18:19:43,617   cls_loss = 0.0
2021-08-08 18:19:43,617   global_step = 14949
2021-08-08 18:19:43,617   loss = 3.029440623109927
2021-08-08 18:19:43,617   rep_loss = 0.8590711172475999
2021-08-08 18:19:43,617 ***** Save model *****
2021-08-08 18:19:49,519 ***** Running evaluation *****
2021-08-08 18:19:49,520   Epoch = 0 iter 14999 step
2021-08-08 18:19:49,520   Num examples = 9815
2021-08-08 18:19:49,520   Batch size = 32
2021-08-08 18:19:49,521 ***** Eval results *****
2021-08-08 18:19:49,521   att_loss = 2.1698829081306634
2021-08-08 18:19:49,521   cls_loss = 0.0
2021-08-08 18:19:49,521   global_step = 14999
2021-08-08 18:19:49,521   loss = 3.028879384087184
2021-08-08 18:19:49,521   rep_loss = 0.8589964759247296
2021-08-08 18:19:49,521 ***** Save model *****
2021-08-08 18:19:57,521 ***** Running evaluation *****
2021-08-08 18:19:57,521   Epoch = 0 iter 15049 step
2021-08-08 18:19:57,521   Num examples = 9815
2021-08-08 18:19:57,521   Batch size = 32
2021-08-08 18:19:57,522 ***** Eval results *****
2021-08-08 18:19:57,522   att_loss = 2.169516880905615
2021-08-08 18:19:57,522   cls_loss = 0.0
2021-08-08 18:19:57,522   global_step = 15049
2021-08-08 18:19:57,522   loss = 3.0284459645886446
2021-08-08 18:19:57,522   rep_loss = 0.8589290836830299
2021-08-08 18:19:57,523 ***** Save model *****
2021-08-08 18:20:04,248 ***** Running evaluation *****
2021-08-08 18:20:04,248   Epoch = 0 iter 15099 step
2021-08-08 18:20:04,248   Num examples = 9815
2021-08-08 18:20:04,248   Batch size = 32
2021-08-08 18:20:04,249 ***** Eval results *****
2021-08-08 18:20:04,249   att_loss = 2.169348084545268
2021-08-08 18:20:04,249   cls_loss = 0.0
2021-08-08 18:20:04,249   global_step = 15099
2021-08-08 18:20:04,249   loss = 3.028230470445379
2021-08-08 18:20:04,249   rep_loss = 0.85888238586853
2021-08-08 18:20:04,249 ***** Save model *****
2021-08-08 18:20:10,170 ***** Running evaluation *****
2021-08-08 18:20:10,170   Epoch = 0 iter 15149 step
2021-08-08 18:20:10,170   Num examples = 9815
2021-08-08 18:20:10,170   Batch size = 32
2021-08-08 18:20:10,172 ***** Eval results *****
2021-08-08 18:20:10,172   att_loss = 2.169016297355061
2021-08-08 18:20:10,172   cls_loss = 0.0
2021-08-08 18:20:10,172   global_step = 15149
2021-08-08 18:20:10,172   loss = 3.0278353690943693
2021-08-08 18:20:10,172   rep_loss = 0.8588190717078316
2021-08-08 18:20:10,172 ***** Save model *****
2021-08-08 18:20:16,206 ***** Running evaluation *****
2021-08-08 18:20:16,207   Epoch = 0 iter 15199 step
2021-08-08 18:20:16,207   Num examples = 9815
2021-08-08 18:20:16,207   Batch size = 32
2021-08-08 18:20:16,207 ***** Eval results *****
2021-08-08 18:20:16,207   att_loss = 2.1687277376247835
2021-08-08 18:20:16,207   cls_loss = 0.0
2021-08-08 18:20:16,208   global_step = 15199
2021-08-08 18:20:16,208   loss = 3.027482832628721
2021-08-08 18:20:16,208   rep_loss = 0.8587550949686428
2021-08-08 18:20:16,208 ***** Save model *****
2021-08-08 18:20:22,111 ***** Running evaluation *****
2021-08-08 18:20:22,111   Epoch = 0 iter 15249 step
2021-08-08 18:20:22,112   Num examples = 9815
2021-08-08 18:20:22,112   Batch size = 32
2021-08-08 18:20:22,112 ***** Eval results *****
2021-08-08 18:20:22,112   att_loss = 2.1683631930103284
2021-08-08 18:20:22,112   cls_loss = 0.0
2021-08-08 18:20:22,113   global_step = 15249
2021-08-08 18:20:22,113   loss = 3.0270594066772722
2021-08-08 18:20:22,113   rep_loss = 0.8586962136395823
2021-08-08 18:20:22,113 ***** Save model *****
2021-08-08 18:20:28,104 ***** Running evaluation *****
2021-08-08 18:20:28,104   Epoch = 0 iter 15299 step
2021-08-08 18:20:28,104   Num examples = 9815
2021-08-08 18:20:28,104   Batch size = 32
2021-08-08 18:20:28,105 ***** Eval results *****
2021-08-08 18:20:28,105   att_loss = 2.1681721153987983
2021-08-08 18:20:28,105   cls_loss = 0.0
2021-08-08 18:20:28,105   global_step = 15299
2021-08-08 18:20:28,105   loss = 3.026816173038948
2021-08-08 18:20:28,105   rep_loss = 0.8586440575817097
2021-08-08 18:20:28,105 ***** Save model *****
2021-08-08 18:20:34,035 ***** Running evaluation *****
2021-08-08 18:20:34,035   Epoch = 0 iter 15349 step
2021-08-08 18:20:34,035   Num examples = 9815
2021-08-08 18:20:34,035   Batch size = 32
2021-08-08 18:20:34,036 ***** Eval results *****
2021-08-08 18:20:34,036   att_loss = 2.167841644106614
2021-08-08 18:20:34,036   cls_loss = 0.0
2021-08-08 18:20:34,036   global_step = 15349
2021-08-08 18:20:34,036   loss = 3.026424984379276
2021-08-08 18:20:34,036   rep_loss = 0.8585833402105291
2021-08-08 18:20:34,037 ***** Save model *****
2021-08-08 18:20:41,844 ***** Running evaluation *****
2021-08-08 18:20:41,844   Epoch = 0 iter 15399 step
2021-08-08 18:20:41,844   Num examples = 9815
2021-08-08 18:20:41,844   Batch size = 32
2021-08-08 18:20:41,845 ***** Eval results *****
2021-08-08 18:20:41,845   att_loss = 2.167680735138765
2021-08-08 18:20:41,845   cls_loss = 0.0
2021-08-08 18:20:41,845   global_step = 15399
2021-08-08 18:20:41,845   loss = 3.026227506726382
2021-08-08 18:20:41,845   rep_loss = 0.8585467714637555
2021-08-08 18:20:41,845 ***** Save model *****
2021-08-08 18:20:47,780 ***** Running evaluation *****
2021-08-08 18:20:47,781   Epoch = 0 iter 15449 step
2021-08-08 18:20:47,781   Num examples = 9815
2021-08-08 18:20:47,781   Batch size = 32
2021-08-08 18:20:47,781 ***** Eval results *****
2021-08-08 18:20:47,782   att_loss = 2.167343045663769
2021-08-08 18:20:47,782   cls_loss = 0.0
2021-08-08 18:20:47,782   global_step = 15449
2021-08-08 18:20:47,782   loss = 3.025825663611097
2021-08-08 18:20:47,782   rep_loss = 0.8584826177621367
2021-08-08 18:20:47,782 ***** Save model *****
2021-08-08 18:20:53,695 ***** Running evaluation *****
2021-08-08 18:20:53,696   Epoch = 0 iter 15499 step
2021-08-08 18:20:53,696   Num examples = 9815
2021-08-08 18:20:53,696   Batch size = 32
2021-08-08 18:20:53,697 ***** Eval results *****
2021-08-08 18:20:53,697   att_loss = 2.1672211933754375
2021-08-08 18:20:53,697   cls_loss = 0.0
2021-08-08 18:20:53,697   global_step = 15499
2021-08-08 18:20:53,697   loss = 3.025668589620715
2021-08-08 18:20:53,697   rep_loss = 0.8584473960876035
2021-08-08 18:20:53,697 ***** Save model *****
2021-08-08 18:20:59,597 ***** Running evaluation *****
2021-08-08 18:20:59,597   Epoch = 0 iter 15549 step
2021-08-08 18:20:59,597   Num examples = 9815
2021-08-08 18:20:59,597   Batch size = 32
2021-08-08 18:20:59,598 ***** Eval results *****
2021-08-08 18:20:59,598   att_loss = 2.1667719796904645
2021-08-08 18:20:59,598   cls_loss = 0.0
2021-08-08 18:20:59,598   global_step = 15549
2021-08-08 18:20:59,598   loss = 3.025150356679739
2021-08-08 18:20:59,598   rep_loss = 0.8583783768934409
2021-08-08 18:20:59,598 ***** Save model *****
2021-08-08 18:21:07,451 ***** Running evaluation *****
2021-08-08 18:21:07,452   Epoch = 0 iter 15599 step
2021-08-08 18:21:07,452   Num examples = 9815
2021-08-08 18:21:07,452   Batch size = 32
2021-08-08 18:21:07,452 ***** Eval results *****
2021-08-08 18:21:07,452   att_loss = 2.166555280738614
2021-08-08 18:21:07,453   cls_loss = 0.0
2021-08-08 18:21:07,453   global_step = 15599
2021-08-08 18:21:07,453   loss = 3.0248831698004808
2021-08-08 18:21:07,453   rep_loss = 0.858327889008372
2021-08-08 18:21:07,453 ***** Save model *****
2021-08-08 18:21:13,322 ***** Running evaluation *****
2021-08-08 18:21:13,322   Epoch = 0 iter 15649 step
2021-08-08 18:21:13,322   Num examples = 9815
2021-08-08 18:21:13,322   Batch size = 32
2021-08-08 18:21:13,323 ***** Eval results *****
2021-08-08 18:21:13,323   att_loss = 2.166186001995698
2021-08-08 18:21:13,323   cls_loss = 0.0
2021-08-08 18:21:13,323   global_step = 15649
2021-08-08 18:21:13,323   loss = 3.0244512275976376
2021-08-08 18:21:13,323   rep_loss = 0.8582652255828955
2021-08-08 18:21:13,324 ***** Save model *****
2021-08-08 18:21:19,212 ***** Running evaluation *****
2021-08-08 18:21:19,212   Epoch = 0 iter 15699 step
2021-08-08 18:21:19,213   Num examples = 9815
2021-08-08 18:21:19,213   Batch size = 32
2021-08-08 18:21:19,213 ***** Eval results *****
2021-08-08 18:21:19,213   att_loss = 2.1657000459434954
2021-08-08 18:21:19,213   cls_loss = 0.0
2021-08-08 18:21:19,213   global_step = 15699
2021-08-08 18:21:19,214   loss = 3.0238936809424617
2021-08-08 18:21:19,214   rep_loss = 0.8581936349837793
2021-08-08 18:21:19,214 ***** Save model *****
2021-08-08 18:21:25,448 ***** Running evaluation *****
2021-08-08 18:21:25,448   Epoch = 0 iter 15749 step
2021-08-08 18:21:25,448   Num examples = 9815
2021-08-08 18:21:25,448   Batch size = 32
2021-08-08 18:21:25,449 ***** Eval results *****
2021-08-08 18:21:25,449   att_loss = 2.1655187719518763
2021-08-08 18:21:25,449   cls_loss = 0.0
2021-08-08 18:21:25,449   global_step = 15749
2021-08-08 18:21:25,449   loss = 3.0236640331048616
2021-08-08 18:21:25,449   rep_loss = 0.8581452611681242
2021-08-08 18:21:25,450 ***** Save model *****
2021-08-08 18:21:31,513 ***** Running evaluation *****
2021-08-08 18:21:31,514   Epoch = 1 iter 15799 step
2021-08-08 18:21:31,514   Num examples = 9815
2021-08-08 18:21:31,514   Batch size = 32
2021-08-08 18:21:31,515 ***** Eval results *****
2021-08-08 18:21:31,515   att_loss = 2.133369207382202
2021-08-08 18:21:31,515   cls_loss = 0.0
2021-08-08 18:21:31,515   global_step = 15799
2021-08-08 18:21:31,515   loss = 2.988147020339966
2021-08-08 18:21:31,515   rep_loss = 0.8547777533531189
2021-08-08 18:21:31,515 ***** Save model *****
2021-08-08 18:21:38,116 ***** Running evaluation *****
2021-08-08 18:21:38,116   Epoch = 1 iter 15849 step
2021-08-08 18:21:38,116   Num examples = 9815
2021-08-08 18:21:38,116   Batch size = 32
2021-08-08 18:21:38,117 ***** Eval results *****
2021-08-08 18:21:38,117   att_loss = 2.0584526295755423
2021-08-08 18:21:38,117   cls_loss = 0.0
2021-08-08 18:21:38,117   global_step = 15849
2021-08-08 18:21:38,117   loss = 2.895525530272839
2021-08-08 18:21:38,117   rep_loss = 0.8370729018660152
2021-08-08 18:21:38,117 ***** Save model *****
2021-08-08 18:21:44,144 ***** Running evaluation *****
2021-08-08 18:21:44,145   Epoch = 1 iter 15899 step
2021-08-08 18:21:44,145   Num examples = 9815
2021-08-08 18:21:44,145   Batch size = 32
2021-08-08 18:21:44,146 ***** Eval results *****
2021-08-08 18:21:44,146   att_loss = 2.069241710228495
2021-08-08 18:21:44,146   cls_loss = 0.0
2021-08-08 18:21:44,146   global_step = 15899
2021-08-08 18:21:44,146   loss = 2.9090975440374693
2021-08-08 18:21:44,146   rep_loss = 0.8398558403005695
2021-08-08 18:21:44,146 ***** Save model *****
2021-08-08 18:21:50,104 ***** Running evaluation *****
2021-08-08 18:21:50,104   Epoch = 1 iter 15949 step
2021-08-08 18:21:50,104   Num examples = 9815
2021-08-08 18:21:50,104   Batch size = 32
2021-08-08 18:21:50,105 ***** Eval results *****
2021-08-08 18:21:50,105   att_loss = 2.051949414985859
2021-08-08 18:21:50,105   cls_loss = 0.0
2021-08-08 18:21:50,105   global_step = 15949
2021-08-08 18:21:50,105   loss = 2.890029485651989
2021-08-08 18:21:50,105   rep_loss = 0.8380800750081903
2021-08-08 18:21:50,105 ***** Save model *****
2021-08-08 18:21:56,076 ***** Running evaluation *****
2021-08-08 18:21:56,076   Epoch = 1 iter 15999 step
2021-08-08 18:21:56,076   Num examples = 9815
2021-08-08 18:21:56,076   Batch size = 32
2021-08-08 18:21:56,077 ***** Eval results *****
2021-08-08 18:21:56,077   att_loss = 2.054134993410822
2021-08-08 18:21:56,077   cls_loss = 0.0
2021-08-08 18:21:56,077   global_step = 15999
2021-08-08 18:21:56,077   loss = 2.8924713407582905
2021-08-08 18:21:56,077   rep_loss = 0.838336350609414
2021-08-08 18:21:56,077 ***** Save model *****
2021-08-08 18:22:02,096 ***** Running evaluation *****
2021-08-08 18:22:02,096   Epoch = 1 iter 16049 step
2021-08-08 18:22:02,096   Num examples = 9815
2021-08-08 18:22:02,096   Batch size = 32
2021-08-08 18:22:02,097 ***** Eval results *****
2021-08-08 18:22:02,097   att_loss = 2.056394650166728
2021-08-08 18:22:02,097   cls_loss = 0.0
2021-08-08 18:22:02,097   global_step = 16049
2021-08-08 18:22:02,097   loss = 2.8951602561540337
2021-08-08 18:22:02,097   rep_loss = 0.8387656062247744
2021-08-08 18:22:02,098 ***** Save model *****
2021-08-08 18:22:08,055 ***** Running evaluation *****
2021-08-08 18:22:08,055   Epoch = 1 iter 16099 step
2021-08-08 18:22:08,055   Num examples = 9815
2021-08-08 18:22:08,055   Batch size = 32
2021-08-08 18:22:08,056 ***** Eval results *****
2021-08-08 18:22:08,056   att_loss = 2.057198271402885
2021-08-08 18:22:08,056   cls_loss = 0.0
2021-08-08 18:22:08,056   global_step = 16099
2021-08-08 18:22:08,056   loss = 2.8964375934727564
2021-08-08 18:22:08,056   rep_loss = 0.8392393238520701
2021-08-08 18:22:08,057 ***** Save model *****
2021-08-08 18:22:13,993 ***** Running evaluation *****
2021-08-08 18:22:13,993   Epoch = 1 iter 16149 step
2021-08-08 18:22:13,993   Num examples = 9815
2021-08-08 18:22:13,993   Batch size = 32
2021-08-08 18:22:13,994 ***** Eval results *****
2021-08-08 18:22:13,994   att_loss = 2.0591245064368615
2021-08-08 18:22:13,994   cls_loss = 0.0
2021-08-08 18:22:13,994   global_step = 16149
2021-08-08 18:22:13,994   loss = 2.8983499025687194
2021-08-08 18:22:13,994   rep_loss = 0.8392253974903683
2021-08-08 18:22:13,994 ***** Save model *****
2021-08-08 18:22:20,080 ***** Running evaluation *****
2021-08-08 18:22:20,080   Epoch = 1 iter 16199 step
2021-08-08 18:22:20,080   Num examples = 9815
2021-08-08 18:22:20,080   Batch size = 32
2021-08-08 18:22:20,081 ***** Eval results *****
2021-08-08 18:22:20,081   att_loss = 2.058653549957751
2021-08-08 18:22:20,081   cls_loss = 0.0
2021-08-08 18:22:20,081   global_step = 16199
2021-08-08 18:22:20,081   loss = 2.897839964773887
2021-08-08 18:22:20,081   rep_loss = 0.8391864143702158
2021-08-08 18:22:20,081 ***** Save model *****
2021-08-08 18:22:27,181 ***** Running evaluation *****
2021-08-08 18:22:27,182   Epoch = 1 iter 16249 step
2021-08-08 18:22:27,182   Num examples = 9815
2021-08-08 18:22:27,182   Batch size = 32
2021-08-08 18:22:27,183 ***** Eval results *****
2021-08-08 18:22:27,183   att_loss = 2.057480746785183
2021-08-08 18:22:27,183   cls_loss = 0.0
2021-08-08 18:22:27,183   global_step = 16249
2021-08-08 18:22:27,183   loss = 2.896652880370485
2021-08-08 18:22:27,183   rep_loss = 0.8391721326601743
2021-08-08 18:22:27,183 ***** Save model *****
2021-08-08 18:22:33,374 ***** Running evaluation *****
2021-08-08 18:22:33,374   Epoch = 1 iter 16299 step
2021-08-08 18:22:33,374   Num examples = 9815
2021-08-08 18:22:33,374   Batch size = 32
2021-08-08 18:22:33,375 ***** Eval results *****
2021-08-08 18:22:33,375   att_loss = 2.055757497599025
2021-08-08 18:22:33,375   cls_loss = 0.0
2021-08-08 18:22:33,375   global_step = 16299
2021-08-08 18:22:33,375   loss = 2.894672476127
2021-08-08 18:22:33,375   rep_loss = 0.8389149760295769
2021-08-08 18:22:33,375 ***** Save model *****
2021-08-08 18:22:39,484 ***** Running evaluation *****
2021-08-08 18:22:39,485   Epoch = 1 iter 16349 step
2021-08-08 18:22:39,485   Num examples = 9815
2021-08-08 18:22:39,485   Batch size = 32
2021-08-08 18:22:39,486 ***** Eval results *****
2021-08-08 18:22:39,486   att_loss = 2.057607411256503
2021-08-08 18:22:39,486   cls_loss = 0.0
2021-08-08 18:22:39,486   global_step = 16349
2021-08-08 18:22:39,486   loss = 2.8967345769089494
2021-08-08 18:22:39,486   rep_loss = 0.8391271632725876
2021-08-08 18:22:39,486 ***** Save model *****
2021-08-08 18:22:47,672 ***** Running evaluation *****
2021-08-08 18:22:47,672   Epoch = 1 iter 16399 step
2021-08-08 18:22:47,672   Num examples = 9815
2021-08-08 18:22:47,672   Batch size = 32
2021-08-08 18:22:47,673 ***** Eval results *****
2021-08-08 18:22:47,673   att_loss = 2.056592903399031
2021-08-08 18:22:47,673   cls_loss = 0.0
2021-08-08 18:22:47,673   global_step = 16399
2021-08-08 18:22:47,673   loss = 2.8956035917094862
2021-08-08 18:22:47,673   rep_loss = 0.8390106865252909
2021-08-08 18:22:47,674 ***** Save model *****
2021-08-08 18:22:53,615 ***** Running evaluation *****
2021-08-08 18:22:53,615   Epoch = 1 iter 16449 step
2021-08-08 18:22:53,615   Num examples = 9815
2021-08-08 18:22:53,615   Batch size = 32
2021-08-08 18:22:53,616 ***** Eval results *****
2021-08-08 18:22:53,616   att_loss = 2.0590535217349615
2021-08-08 18:22:53,616   cls_loss = 0.0
2021-08-08 18:22:53,616   global_step = 16449
2021-08-08 18:22:53,616   loss = 2.8981440905967983
2021-08-08 18:22:53,616   rep_loss = 0.8390905673053407
2021-08-08 18:22:53,616 ***** Save model *****
2021-08-08 18:22:59,527 ***** Running evaluation *****
2021-08-08 18:22:59,527   Epoch = 1 iter 16499 step
2021-08-08 18:22:59,528   Num examples = 9815
2021-08-08 18:22:59,528   Batch size = 32
2021-08-08 18:22:59,528 ***** Eval results *****
2021-08-08 18:22:59,528   att_loss = 2.0586434771092916
2021-08-08 18:22:59,528   cls_loss = 0.0
2021-08-08 18:22:59,529   global_step = 16499
2021-08-08 18:22:59,529   loss = 2.8977200321736247
2021-08-08 18:22:59,529   rep_loss = 0.8390765520883525
2021-08-08 18:22:59,529 ***** Save model *****
2021-08-08 18:23:05,426 ***** Running evaluation *****
2021-08-08 18:23:05,426   Epoch = 1 iter 16549 step
2021-08-08 18:23:05,426   Num examples = 9815
2021-08-08 18:23:05,426   Batch size = 32
2021-08-08 18:23:05,427 ***** Eval results *****
2021-08-08 18:23:05,427   att_loss = 2.0612908584300116
2021-08-08 18:23:05,427   cls_loss = 0.0
2021-08-08 18:23:05,427   global_step = 16549
2021-08-08 18:23:05,427   loss = 2.900647948807311
2021-08-08 18:23:05,427   rep_loss = 0.8393570879169215
2021-08-08 18:23:05,427 ***** Save model *****
2021-08-08 18:23:11,329 ***** Running evaluation *****
2021-08-08 18:23:11,329   Epoch = 1 iter 16599 step
2021-08-08 18:23:11,329   Num examples = 9815
2021-08-08 18:23:11,329   Batch size = 32
2021-08-08 18:23:11,330 ***** Eval results *****
2021-08-08 18:23:11,330   att_loss = 2.061251684967498
2021-08-08 18:23:11,330   cls_loss = 0.0
2021-08-08 18:23:11,330   global_step = 16599
2021-08-08 18:23:11,330   loss = 2.900439481163739
2021-08-08 18:23:11,330   rep_loss = 0.8391877932197295
2021-08-08 18:23:11,331 ***** Save model *****
2021-08-08 18:23:17,270 ***** Running evaluation *****
2021-08-08 18:23:17,271   Epoch = 1 iter 16649 step
2021-08-08 18:23:17,271   Num examples = 9815
2021-08-08 18:23:17,271   Batch size = 32
2021-08-08 18:23:17,272 ***** Eval results *****
2021-08-08 18:23:17,272   att_loss = 2.063939331561222
2021-08-08 18:23:17,272   cls_loss = 0.0
2021-08-08 18:23:17,272   global_step = 16649
2021-08-08 18:23:17,272   loss = 2.9033250842335363
2021-08-08 18:23:17,272   rep_loss = 0.839385750080808
2021-08-08 18:23:17,272 ***** Save model *****
2021-08-08 18:23:23,214 ***** Running evaluation *****
2021-08-08 18:23:23,214   Epoch = 1 iter 16699 step
2021-08-08 18:23:23,214   Num examples = 9815
2021-08-08 18:23:23,215   Batch size = 32
2021-08-08 18:23:23,215 ***** Eval results *****
2021-08-08 18:23:23,215   att_loss = 2.0618184786393297
2021-08-08 18:23:23,215   cls_loss = 0.0
2021-08-08 18:23:23,216   global_step = 16699
2021-08-08 18:23:23,216   loss = 2.9010404265019525
2021-08-08 18:23:23,216   rep_loss = 0.8392219456133911
2021-08-08 18:23:23,216 ***** Save model *****
2021-08-08 18:23:29,143 ***** Running evaluation *****
2021-08-08 18:23:29,143   Epoch = 1 iter 16749 step
2021-08-08 18:23:29,143   Num examples = 9815
2021-08-08 18:23:29,144   Batch size = 32
2021-08-08 18:23:29,144 ***** Eval results *****
2021-08-08 18:23:29,144   att_loss = 2.0622136302049476
2021-08-08 18:23:29,144   cls_loss = 0.0
2021-08-08 18:23:29,144   global_step = 16749
2021-08-08 18:23:29,145   loss = 2.9014228413157657
2021-08-08 18:23:29,145   rep_loss = 0.8392092094812483
2021-08-08 18:23:29,145 ***** Save model *****
2021-08-08 18:23:35,131 ***** Running evaluation *****
2021-08-08 18:23:35,131   Epoch = 1 iter 16799 step
2021-08-08 18:23:35,131   Num examples = 9815
2021-08-08 18:23:35,131   Batch size = 32
2021-08-08 18:23:35,132 ***** Eval results *****
2021-08-08 18:23:35,132   att_loss = 2.062523667986219
2021-08-08 18:23:35,132   cls_loss = 0.0
2021-08-08 18:23:35,132   global_step = 16799
2021-08-08 18:23:35,132   loss = 2.901723365802746
2021-08-08 18:23:35,132   rep_loss = 0.8391996962683541
2021-08-08 18:23:35,132 ***** Save model *****
2021-08-08 18:23:41,362 ***** Running evaluation *****
2021-08-08 18:23:41,362   Epoch = 1 iter 16849 step
2021-08-08 18:23:41,362   Num examples = 9815
2021-08-08 18:23:41,362   Batch size = 32
2021-08-08 18:23:41,363 ***** Eval results *****
2021-08-08 18:23:41,363   att_loss = 2.0646897439838705
2021-08-08 18:23:41,363   cls_loss = 0.0
2021-08-08 18:23:41,363   global_step = 16849
2021-08-08 18:23:41,363   loss = 2.904054622895144
2021-08-08 18:23:41,363   rep_loss = 0.8393648778904517
2021-08-08 18:23:41,363 ***** Save model *****
2021-08-08 18:23:47,342 ***** Running evaluation *****
2021-08-08 18:23:47,342   Epoch = 1 iter 16899 step
2021-08-08 18:23:47,342   Num examples = 9815
2021-08-08 18:23:47,342   Batch size = 32
2021-08-08 18:23:47,343 ***** Eval results *****
2021-08-08 18:23:47,343   att_loss = 2.0659515475490546
2021-08-08 18:23:47,343   cls_loss = 0.0
2021-08-08 18:23:47,343   global_step = 16899
2021-08-08 18:23:47,343   loss = 2.905324527288328
2021-08-08 18:23:47,343   rep_loss = 0.8393729791437681
2021-08-08 18:23:47,343 ***** Save model *****
2021-08-08 18:23:55,010 ***** Running evaluation *****
2021-08-08 18:23:55,010   Epoch = 1 iter 16949 step
2021-08-08 18:23:55,010   Num examples = 9815
2021-08-08 18:23:55,010   Batch size = 32
2021-08-08 18:23:55,011 ***** Eval results *****
2021-08-08 18:23:55,011   att_loss = 2.0664520188065634
2021-08-08 18:23:55,011   cls_loss = 0.0
2021-08-08 18:23:55,011   global_step = 16949
2021-08-08 18:23:55,011   loss = 2.905856861641467
2021-08-08 18:23:55,011   rep_loss = 0.8394048426795482
2021-08-08 18:23:55,012 ***** Save model *****
2021-08-08 18:24:00,972 ***** Running evaluation *****
2021-08-08 18:24:00,972   Epoch = 1 iter 16999 step
2021-08-08 18:24:00,972   Num examples = 9815
2021-08-08 18:24:00,972   Batch size = 32
2021-08-08 18:24:00,973 ***** Eval results *****
2021-08-08 18:24:00,973   att_loss = 2.0688653407148476
2021-08-08 18:24:00,973   cls_loss = 0.0
2021-08-08 18:24:00,973   global_step = 16999
2021-08-08 18:24:00,973   loss = 2.908489463529817
2021-08-08 18:24:00,973   rep_loss = 0.8396241222194192
2021-08-08 18:24:00,973 ***** Save model *****
2021-08-08 18:24:06,933 ***** Running evaluation *****
2021-08-08 18:24:06,933   Epoch = 1 iter 17049 step
2021-08-08 18:24:06,933   Num examples = 9815
2021-08-08 18:24:06,933   Batch size = 32
2021-08-08 18:24:06,934 ***** Eval results *****
2021-08-08 18:24:06,934   att_loss = 2.0694898171581144
2021-08-08 18:24:06,934   cls_loss = 0.0
2021-08-08 18:24:06,934   global_step = 17049
2021-08-08 18:24:06,934   loss = 2.909169413202958
2021-08-08 18:24:06,934   rep_loss = 0.8396795953301598
2021-08-08 18:24:06,935 ***** Save model *****
2021-08-08 18:24:12,865 ***** Running evaluation *****
2021-08-08 18:24:12,865   Epoch = 1 iter 17099 step
2021-08-08 18:24:12,865   Num examples = 9815
2021-08-08 18:24:12,865   Batch size = 32
2021-08-08 18:24:12,866 ***** Eval results *****
2021-08-08 18:24:12,866   att_loss = 2.0688291465567223
2021-08-08 18:24:12,866   cls_loss = 0.0
2021-08-08 18:24:12,866   global_step = 17099
2021-08-08 18:24:12,867   loss = 2.908536178345501
2021-08-08 18:24:12,867   rep_loss = 0.8397070315597059
2021-08-08 18:24:12,867 ***** Save model *****
2021-08-08 18:27:57,331 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 18:27:57,462 device: cuda n_gpu: 4
2021-08-08 18:28:06,660 Writing example 0 of 505555
2021-08-08 18:28:06,663 *** Example ***
2021-08-08 18:28:06,663 guid: aug-0
2021-08-08 18:28:06,663 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 18:28:06,663 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:28:06,663 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:28:06,663 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:28:06,664 label: neutral
2021-08-08 18:28:06,664 label_id: 2
2021-08-08 18:28:11,553 Writing example 10000 of 505555
2021-08-08 18:28:16,137 Writing example 20000 of 505555
2021-08-08 18:28:20,628 Writing example 30000 of 505555
2021-08-08 18:28:25,310 Writing example 40000 of 505555
2021-08-08 18:28:30,134 Writing example 50000 of 505555
2021-08-08 18:28:34,613 Writing example 60000 of 505555
2021-08-08 18:28:39,225 Writing example 70000 of 505555
2021-08-08 18:28:43,788 Writing example 80000 of 505555
2021-08-08 18:28:48,652 Writing example 90000 of 505555
2021-08-08 18:28:53,260 Writing example 100000 of 505555
2021-08-08 18:28:57,851 Writing example 110000 of 505555
2021-08-08 18:29:02,197 Writing example 120000 of 505555
2021-08-08 18:29:06,840 Writing example 130000 of 505555
2021-08-08 18:29:12,157 Writing example 140000 of 505555
2021-08-08 18:29:16,677 Writing example 150000 of 505555
2021-08-08 18:29:21,436 Writing example 160000 of 505555
2021-08-08 18:29:26,090 Writing example 170000 of 505555
2021-08-08 18:29:30,660 Writing example 180000 of 505555
2021-08-08 18:29:35,250 Writing example 190000 of 505555
2021-08-08 18:29:39,802 Writing example 200000 of 505555
2021-08-08 18:29:45,193 Writing example 210000 of 505555
2021-08-08 18:29:49,640 Writing example 220000 of 505555
2021-08-08 18:29:54,305 Writing example 230000 of 505555
2021-08-08 18:29:58,945 Writing example 240000 of 505555
2021-08-08 18:30:03,688 Writing example 250000 of 505555
2021-08-08 18:30:08,287 Writing example 260000 of 505555
2021-08-08 18:30:12,763 Writing example 270000 of 505555
2021-08-08 18:30:17,199 Writing example 280000 of 505555
2021-08-08 18:30:22,911 Writing example 290000 of 505555
2021-08-08 18:30:27,666 Writing example 300000 of 505555
2021-08-08 18:30:32,169 Writing example 310000 of 505555
2021-08-08 18:30:36,561 Writing example 320000 of 505555
2021-08-08 18:30:41,208 Writing example 330000 of 505555
2021-08-08 18:30:45,631 Writing example 340000 of 505555
2021-08-08 18:30:50,119 Writing example 350000 of 505555
2021-08-08 18:30:54,585 Writing example 360000 of 505555
2021-08-08 18:30:58,969 Writing example 370000 of 505555
2021-08-08 18:31:03,480 Writing example 380000 of 505555
2021-08-08 18:31:09,421 Writing example 390000 of 505555
2021-08-08 18:31:14,101 Writing example 400000 of 505555
2021-08-08 18:31:18,661 Writing example 410000 of 505555
2021-08-08 18:31:23,127 Writing example 420000 of 505555
2021-08-08 18:31:27,541 Writing example 430000 of 505555
2021-08-08 18:31:31,934 Writing example 440000 of 505555
2021-08-08 18:31:36,535 Writing example 450000 of 505555
2021-08-08 18:31:41,113 Writing example 460000 of 505555
2021-08-08 18:31:45,653 Writing example 470000 of 505555
2021-08-08 18:31:50,269 Writing example 480000 of 505555
2021-08-08 18:31:54,927 Writing example 490000 of 505555
2021-08-08 18:31:59,520 Writing example 500000 of 505555
2021-08-08 18:32:07,289 Writing example 0 of 9815
2021-08-08 18:32:07,290 *** Example ***
2021-08-08 18:32:07,290 guid: dev_matched-0
2021-08-08 18:32:07,290 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 18:32:07,290 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:32:07,290 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:32:07,290 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 18:32:07,290 label: neutral
2021-08-08 18:32:07,290 label_id: 2
2021-08-08 18:32:11,848 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 18:32:14,365 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 18:32:26,719 loading model...
2021-08-08 18:32:26,753 done!
2021-08-08 18:32:26,753 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 18:32:26,753 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 18:32:32,685 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 18:32:33,020 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 18:32:35,379 loading model...
2021-08-08 18:32:35,383 done!
2021-08-08 18:32:35,384 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 18:32:35,384 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 18:32:35,402 ***** Running training *****
2021-08-08 18:32:35,402   Num examples = 505555
2021-08-08 18:32:35,402   Batch size = 32
2021-08-08 18:32:35,402   Num steps = 157980
2021-08-08 18:32:35,403 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 18:32:35,403 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 18:32:35,403 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 18:32:35,403 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 18:32:35,403 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 18:32:35,403 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 18:32:35,403 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 18:32:35,403 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 18:32:35,403 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 18:32:35,404 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 18:32:35,405 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 18:32:35,406 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 18:32:35,407 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 18:32:35,407 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 18:32:35,407 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 18:32:35,407 n: module.bert.pooler.dense.weight
2021-08-08 18:32:35,407 n: module.bert.pooler.dense.bias
2021-08-08 18:32:35,407 n: module.classifier.weight
2021-08-08 18:32:35,407 n: module.classifier.bias
2021-08-08 18:32:35,407 n: module.fit_dense.weight
2021-08-08 18:32:35,407 n: module.fit_dense.bias
2021-08-08 18:32:35,407 Total parameters: 14591571
2021-08-08 18:39:38,798 ***** Running evaluation *****
2021-08-08 18:39:38,798   Epoch = 0 iter 2999 step
2021-08-08 18:39:38,798   Num examples = 9815
2021-08-08 18:39:38,798   Batch size = 32
2021-08-08 18:39:40,556 ***** Eval results *****
2021-08-08 18:39:40,556   att_loss = 2.306856822633632
2021-08-08 18:39:40,556   cls_loss = 0.0
2021-08-08 18:39:40,556   global_step = 2999
2021-08-08 18:39:40,557   loss = 3.1923103675956765
2021-08-08 18:39:40,557   rep_loss = 0.8854535441074263
2021-08-08 18:39:40,558 ***** Save model *****
2021-08-08 18:46:35,460 ***** Running evaluation *****
2021-08-08 18:46:35,460   Epoch = 0 iter 5999 step
2021-08-08 18:46:35,460   Num examples = 9815
2021-08-08 18:46:35,460   Batch size = 32
2021-08-08 18:46:35,461 ***** Eval results *****
2021-08-08 18:46:35,461   att_loss = 2.244898810369171
2021-08-08 18:46:35,461   cls_loss = 0.0
2021-08-08 18:46:35,461   global_step = 5999
2021-08-08 18:46:35,461   loss = 3.118862050158677
2021-08-08 18:46:35,461   rep_loss = 0.8739632391039382
2021-08-08 18:46:35,462 ***** Save model *****
2021-08-08 18:53:29,114 ***** Running evaluation *****
2021-08-08 18:53:29,115   Epoch = 0 iter 8999 step
2021-08-08 18:53:29,115   Num examples = 9815
2021-08-08 18:53:29,115   Batch size = 32
2021-08-08 18:53:29,116 ***** Eval results *****
2021-08-08 18:53:29,116   att_loss = 2.2123414198681175
2021-08-08 18:53:29,116   cls_loss = 0.0
2021-08-08 18:53:29,116   global_step = 8999
2021-08-08 18:53:29,116   loss = 3.0798705499004506
2021-08-08 18:53:29,116   rep_loss = 0.8675291289990711
2021-08-08 18:53:29,117 ***** Save model *****
2021-08-08 19:00:25,680 ***** Running evaluation *****
2021-08-08 19:00:25,680   Epoch = 0 iter 11999 step
2021-08-08 19:00:25,680   Num examples = 9815
2021-08-08 19:00:25,680   Batch size = 32
2021-08-08 19:00:25,794 ***** Eval results *****
2021-08-08 19:00:25,794   att_loss = 2.187852327789264
2021-08-08 19:00:25,794   cls_loss = 0.0
2021-08-08 19:00:25,794   global_step = 11999
2021-08-08 19:00:25,794   loss = 3.0505934404784396
2021-08-08 19:00:25,794   rep_loss = 0.8627411122222336
2021-08-08 19:00:25,794 ***** Save model *****
2021-08-08 19:07:21,395 ***** Running evaluation *****
2021-08-08 19:07:21,396   Epoch = 0 iter 14999 step
2021-08-08 19:07:21,396   Num examples = 9815
2021-08-08 19:07:21,396   Batch size = 32
2021-08-08 19:07:21,397 ***** Eval results *****
2021-08-08 19:07:21,397   att_loss = 2.1700018426880643
2021-08-08 19:07:21,397   cls_loss = 0.0
2021-08-08 19:07:21,397   global_step = 14999
2021-08-08 19:07:21,397   loss = 3.029066534421946
2021-08-08 19:07:21,397   rep_loss = 0.8590646911377954
2021-08-08 19:07:21,397 ***** Save model *****
2021-08-08 19:14:18,732 ***** Running evaluation *****
2021-08-08 19:14:18,732   Epoch = 1 iter 17999 step
2021-08-08 19:14:18,732   Num examples = 9815
2021-08-08 19:14:18,732   Batch size = 32
2021-08-08 19:14:18,733 ***** Eval results *****
2021-08-08 19:14:18,734   att_loss = 2.0722401065752756
2021-08-08 19:14:18,734   cls_loss = 0.0
2021-08-08 19:14:18,734   global_step = 17999
2021-08-08 19:14:18,734   loss = 2.9118557229793813
2021-08-08 19:14:18,734   rep_loss = 0.8396156163228636
2021-08-08 19:14:18,734 ***** Save model *****
2021-08-08 19:29:17,679 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:29:17,817 device: cuda n_gpu: 1
2021-08-08 19:29:55,944 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:29:56,021 device: cuda n_gpu: 1
2021-08-08 19:31:08,823 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:31:08,881 device: cuda n_gpu: 1
2021-08-08 19:32:39,759 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:32:39,846 device: cuda n_gpu: 2
2021-08-08 19:33:19,404 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:33:19,486 device: cuda n_gpu: 2
2021-08-08 19:34:52,616 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:34:52,702 device: cuda n_gpu: 2
2021-08-08 19:36:19,245 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:36:19,395 device: cuda n_gpu: 4
2021-08-08 19:37:37,839 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:37:37,983 device: cuda n_gpu: 4
2021-08-08 19:38:46,249 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:38:46,306 device: cuda n_gpu: 1
2021-08-08 19:39:45,198 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:39:45,253 device: cuda n_gpu: 1
2021-08-08 19:40:39,499 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:40:39,554 device: cuda n_gpu: 1
2021-08-08 19:41:03,907 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:41:04,022 device: cuda n_gpu: 1
2021-08-08 19:41:25,591 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:41:25,646 device: cuda n_gpu: 1
2021-08-08 19:41:50,922 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:41:50,976 device: cuda n_gpu: 1
2021-08-08 19:42:02,445 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:42:02,498 device: cuda n_gpu: 1
2021-08-08 19:42:24,237 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:42:24,293 device: cuda n_gpu: 1
2021-08-08 19:42:43,281 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:42:43,337 device: cuda n_gpu: 1
2021-08-08 19:43:33,741 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:43:33,800 device: cuda n_gpu: 1
2021-08-08 19:43:56,620 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:43:56,676 device: cuda n_gpu: 1
2021-08-08 19:44:21,589 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:44:21,647 device: cuda n_gpu: 1
2021-08-08 19:44:33,546 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:44:33,601 device: cuda n_gpu: 1
2021-08-08 19:45:42,080 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:45:42,136 device: cuda n_gpu: 1
2021-08-08 19:46:39,748 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:46:39,813 device: cuda n_gpu: 1
2021-08-08 19:47:14,128 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:47:14,183 device: cuda n_gpu: 1
2021-08-08 19:47:49,174 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:47:49,230 device: cuda n_gpu: 1
2021-08-08 19:48:07,307 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:48:07,361 device: cuda n_gpu: 1
2021-08-08 19:48:22,297 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:48:22,353 device: cuda n_gpu: 1
2021-08-08 19:48:44,576 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:48:44,631 device: cuda n_gpu: 1
2021-08-08 19:50:35,712 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:50:35,753 device: cuda n_gpu: 1
2021-08-08 19:51:37,128 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:51:37,261 device: cuda n_gpu: 4
2021-08-08 19:51:47,998 Writing example 0 of 505555
2021-08-08 19:51:48,001 *** Example ***
2021-08-08 19:51:48,001 guid: aug-0
2021-08-08 19:51:48,001 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:51:48,001 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:51:48,001 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:51:48,002 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:51:48,002 label: neutral
2021-08-08 19:51:48,002 label_id: 2
2021-08-08 19:51:52,659 Writing example 10000 of 505555
2021-08-08 19:51:57,224 Writing example 20000 of 505555
2021-08-08 19:53:05,902 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:53:06,025 device: cuda n_gpu: 4
2021-08-08 19:53:06,170 Writing example 0 of 99
2021-08-08 19:53:06,171 *** Example ***
2021-08-08 19:53:06,171 guid: aug-0
2021-08-08 19:53:06,171 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:53:06,171 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,171 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,172 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,172 label: neutral
2021-08-08 19:53:06,172 label_id: 2
2021-08-08 19:53:06,774 Writing example 0 of 9815
2021-08-08 19:53:06,775 *** Example ***
2021-08-08 19:53:06,775 guid: dev_matched-0
2021-08-08 19:53:06,775 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 19:53:06,775 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,775 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,775 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:53:06,775 label: neutral
2021-08-08 19:53:06,775 label_id: 2
2021-08-08 19:53:11,206 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 19:53:13,645 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 19:53:26,072 loading model...
2021-08-08 19:53:26,106 done!
2021-08-08 19:53:26,107 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 19:53:26,107 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 19:53:32,009 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 19:53:32,360 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 19:53:33,637 loading model...
2021-08-08 19:53:33,642 done!
2021-08-08 19:53:33,642 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 19:53:33,642 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 19:53:33,660 ***** Running training *****
2021-08-08 19:53:33,660   Num examples = 99
2021-08-08 19:53:33,660   Batch size = 32
2021-08-08 19:53:33,660   Num steps = 9
2021-08-08 19:53:33,661 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 19:53:33,661 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 19:53:33,662 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 19:53:33,662 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 19:53:33,662 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 19:53:33,662 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 19:53:33,663 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 19:53:33,664 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 19:53:33,665 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 19:53:33,665 n: module.bert.pooler.dense.weight
2021-08-08 19:53:33,665 n: module.bert.pooler.dense.bias
2021-08-08 19:53:33,665 n: module.classifier.weight
2021-08-08 19:53:33,665 n: module.classifier.bias
2021-08-08 19:53:33,665 n: module.fit_dense.weight
2021-08-08 19:53:33,665 n: module.fit_dense.bias
2021-08-08 19:53:33,665 Total parameters: 14591571
2021-08-08 19:56:00,352 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:56:00,484 device: cuda n_gpu: 4
2021-08-08 19:56:00,515 Writing example 0 of 99
2021-08-08 19:56:00,516 *** Example ***
2021-08-08 19:56:00,516 guid: aug-0
2021-08-08 19:56:00,516 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:56:00,516 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,516 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,516 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,516 label: neutral
2021-08-08 19:56:00,516 label_id: 2
2021-08-08 19:56:00,678 Writing example 0 of 9815
2021-08-08 19:56:00,679 *** Example ***
2021-08-08 19:56:00,679 guid: dev_matched-0
2021-08-08 19:56:00,679 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 19:56:00,679 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,679 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,679 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:56:00,679 label: neutral
2021-08-08 19:56:00,679 label_id: 2
2021-08-08 19:56:05,023 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 19:56:07,428 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 19:56:07,667 loading model...
2021-08-08 19:56:07,693 done!
2021-08-08 19:56:07,693 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 19:56:07,693 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 19:56:10,582 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 19:56:10,916 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 19:56:10,946 loading model...
2021-08-08 19:56:10,950 done!
2021-08-08 19:56:10,950 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 19:56:10,950 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 19:56:10,969 ***** Running training *****
2021-08-08 19:56:10,969   Num examples = 99
2021-08-08 19:56:10,969   Batch size = 32
2021-08-08 19:56:10,969   Num steps = 9
2021-08-08 19:56:10,970 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 19:56:10,970 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 19:56:10,970 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 19:56:10,970 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 19:56:10,970 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 19:56:10,970 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 19:56:10,970 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 19:56:10,970 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 19:56:10,970 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 19:56:10,970 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 19:56:10,971 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 19:56:10,972 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 19:56:10,973 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 19:56:10,974 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 19:56:10,974 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 19:56:10,974 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 19:56:10,974 n: module.bert.pooler.dense.weight
2021-08-08 19:56:10,974 n: module.bert.pooler.dense.bias
2021-08-08 19:56:10,974 n: module.classifier.weight
2021-08-08 19:56:10,974 n: module.classifier.bias
2021-08-08 19:56:10,974 n: module.fit_dense.weight
2021-08-08 19:56:10,974 n: module.fit_dense.bias
2021-08-08 19:56:10,974 Total parameters: 14591571
2021-08-08 19:57:39,987 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:57:40,119 device: cuda n_gpu: 4
2021-08-08 19:57:40,153 Writing example 0 of 99
2021-08-08 19:57:40,154 *** Example ***
2021-08-08 19:57:40,154 guid: aug-0
2021-08-08 19:57:40,154 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:57:40,154 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,154 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,154 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,154 label: neutral
2021-08-08 19:57:40,154 label_id: 2
2021-08-08 19:57:40,318 Writing example 0 of 9815
2021-08-08 19:57:40,318 *** Example ***
2021-08-08 19:57:40,318 guid: dev_matched-0
2021-08-08 19:57:40,318 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 19:57:40,318 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,319 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,319 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:57:40,319 label: neutral
2021-08-08 19:57:40,319 label_id: 2
2021-08-08 19:57:44,645 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 19:57:47,046 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 19:57:47,273 loading model...
2021-08-08 19:57:47,305 done!
2021-08-08 19:57:47,305 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 19:57:47,306 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 19:57:50,148 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 19:57:50,483 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 19:57:50,510 loading model...
2021-08-08 19:57:50,514 done!
2021-08-08 19:57:50,515 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 19:57:50,515 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 19:57:50,533 ***** Running training *****
2021-08-08 19:57:50,533   Num examples = 99
2021-08-08 19:57:50,533   Batch size = 32
2021-08-08 19:57:50,533   Num steps = 9
2021-08-08 19:57:50,534 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 19:57:50,534 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 19:57:50,534 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 19:57:50,534 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 19:57:50,534 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 19:57:50,534 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 19:57:50,534 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 19:57:50,534 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 19:57:50,535 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 19:57:50,536 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 19:57:50,537 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 19:57:50,538 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 19:57:50,538 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 19:57:50,538 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 19:57:50,538 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 19:57:50,538 n: module.bert.pooler.dense.weight
2021-08-08 19:57:50,538 n: module.bert.pooler.dense.bias
2021-08-08 19:57:50,538 n: module.classifier.weight
2021-08-08 19:57:50,538 n: module.classifier.bias
2021-08-08 19:57:50,538 n: module.fit_dense.weight
2021-08-08 19:57:50,538 n: module.fit_dense.bias
2021-08-08 19:57:50,538 Total parameters: 14591571
2021-08-08 19:58:05,999 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:58:06,126 device: cuda n_gpu: 4
2021-08-08 19:58:06,157 Writing example 0 of 99
2021-08-08 19:58:06,158 *** Example ***
2021-08-08 19:58:06,158 guid: aug-0
2021-08-08 19:58:06,158 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:58:06,158 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,158 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,158 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,158 label: neutral
2021-08-08 19:58:06,159 label_id: 2
2021-08-08 19:58:06,321 Writing example 0 of 9815
2021-08-08 19:58:06,321 *** Example ***
2021-08-08 19:58:06,321 guid: dev_matched-0
2021-08-08 19:58:06,321 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 19:58:06,321 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,321 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,321 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:58:06,321 label: neutral
2021-08-08 19:58:06,322 label_id: 2
2021-08-08 19:58:10,662 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 19:58:13,111 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 19:58:13,343 loading model...
2021-08-08 19:58:13,376 done!
2021-08-08 19:58:13,376 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 19:58:13,376 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 19:58:16,261 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 19:58:16,593 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 19:58:16,620 loading model...
2021-08-08 19:58:16,625 done!
2021-08-08 19:58:16,625 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 19:58:16,625 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 19:58:16,643 ***** Running training *****
2021-08-08 19:58:16,643   Num examples = 99
2021-08-08 19:58:16,644   Batch size = 32
2021-08-08 19:58:16,644   Num steps = 9
2021-08-08 19:58:16,644 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 19:58:16,645 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 19:58:16,645 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 19:58:16,645 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 19:58:16,645 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 19:58:16,645 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 19:58:16,646 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 19:58:16,647 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 19:58:16,648 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 19:58:16,648 n: module.bert.pooler.dense.weight
2021-08-08 19:58:16,648 n: module.bert.pooler.dense.bias
2021-08-08 19:58:16,648 n: module.classifier.weight
2021-08-08 19:58:16,648 n: module.classifier.bias
2021-08-08 19:58:16,648 n: module.fit_dense.weight
2021-08-08 19:58:16,648 n: module.fit_dense.bias
2021-08-08 19:58:16,649 Total parameters: 14591571
2021-08-08 19:59:27,605 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 19:59:27,732 device: cuda n_gpu: 4
2021-08-08 19:59:27,762 Writing example 0 of 99
2021-08-08 19:59:27,763 *** Example ***
2021-08-08 19:59:27,763 guid: aug-0
2021-08-08 19:59:27,763 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 19:59:27,763 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,763 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,763 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,764 label: neutral
2021-08-08 19:59:27,764 label_id: 2
2021-08-08 19:59:27,924 Writing example 0 of 9815
2021-08-08 19:59:27,924 *** Example ***
2021-08-08 19:59:27,924 guid: dev_matched-0
2021-08-08 19:59:27,924 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 19:59:27,924 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,924 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,924 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 19:59:27,924 label: neutral
2021-08-08 19:59:27,924 label_id: 2
2021-08-08 19:59:32,236 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 19:59:34,695 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 19:59:34,922 loading model...
2021-08-08 19:59:34,947 done!
2021-08-08 19:59:34,947 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 19:59:34,947 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 19:59:37,827 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 19:59:38,160 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 19:59:38,186 loading model...
2021-08-08 19:59:38,191 done!
2021-08-08 19:59:38,191 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 19:59:38,191 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 19:59:38,209 ***** Running training *****
2021-08-08 19:59:38,209   Num examples = 99
2021-08-08 19:59:38,210   Batch size = 32
2021-08-08 19:59:38,210   Num steps = 9
2021-08-08 19:59:38,211 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 19:59:38,211 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 19:59:38,211 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 19:59:38,211 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 19:59:38,211 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 19:59:38,211 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 19:59:38,212 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 19:59:38,213 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 19:59:38,214 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 19:59:38,214 n: module.bert.pooler.dense.weight
2021-08-08 19:59:38,214 n: module.bert.pooler.dense.bias
2021-08-08 19:59:38,214 n: module.classifier.weight
2021-08-08 19:59:38,215 n: module.classifier.bias
2021-08-08 19:59:38,215 n: module.fit_dense.weight
2021-08-08 19:59:38,215 n: module.fit_dense.bias
2021-08-08 19:59:38,215 Total parameters: 14591571
2021-08-08 20:02:59,788 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:02:59,920 device: cuda n_gpu: 4
2021-08-08 20:03:00,002 Writing example 0 of 99
2021-08-08 20:03:00,002 *** Example ***
2021-08-08 20:03:00,002 guid: aug-0
2021-08-08 20:03:00,002 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:03:00,002 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,003 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,003 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,003 label: neutral
2021-08-08 20:03:00,003 label_id: 2
2021-08-08 20:03:00,171 Writing example 0 of 9815
2021-08-08 20:03:00,172 *** Example ***
2021-08-08 20:03:00,172 guid: dev_matched-0
2021-08-08 20:03:00,172 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:03:00,172 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,172 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,172 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:03:00,172 label: neutral
2021-08-08 20:03:00,172 label_id: 2
2021-08-08 20:03:04,515 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:03:06,940 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:03:07,172 loading model...
2021-08-08 20:03:07,205 done!
2021-08-08 20:03:07,205 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:03:07,205 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:03:10,106 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:03:10,472 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:03:10,499 loading model...
2021-08-08 20:03:10,504 done!
2021-08-08 20:03:10,504 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:03:10,504 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:03:10,522 ***** Running training *****
2021-08-08 20:03:10,523   Num examples = 99
2021-08-08 20:03:10,523   Batch size = 32
2021-08-08 20:03:10,523   Num steps = 9
2021-08-08 20:03:10,524 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:03:10,524 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:03:10,524 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:03:10,524 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:03:10,524 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:03:10,524 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:03:10,525 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:03:10,526 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:03:10,527 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:03:10,528 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:03:10,528 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:03:10,528 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:03:10,528 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:03:10,528 n: module.bert.pooler.dense.weight
2021-08-08 20:03:10,528 n: module.bert.pooler.dense.bias
2021-08-08 20:03:10,528 n: module.classifier.weight
2021-08-08 20:03:10,528 n: module.classifier.bias
2021-08-08 20:03:10,528 n: module.fit_dense.weight
2021-08-08 20:03:10,528 n: module.fit_dense.bias
2021-08-08 20:03:10,528 Total parameters: 14591571
2021-08-08 20:09:29,346 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:09:29,476 device: cuda n_gpu: 4
2021-08-08 20:09:29,533 Writing example 0 of 999
2021-08-08 20:09:29,534 *** Example ***
2021-08-08 20:09:29,534 guid: aug-0
2021-08-08 20:09:29,534 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:09:29,534 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:29,534 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:29,534 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:29,534 label: neutral
2021-08-08 20:09:29,534 label_id: 2
2021-08-08 20:09:30,134 Writing example 0 of 9815
2021-08-08 20:09:30,134 *** Example ***
2021-08-08 20:09:30,134 guid: dev_matched-0
2021-08-08 20:09:30,134 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:09:30,134 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:30,135 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:30,135 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:09:30,135 label: neutral
2021-08-08 20:09:30,135 label_id: 2
2021-08-08 20:09:34,452 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:09:36,873 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:09:37,097 loading model...
2021-08-08 20:09:37,130 done!
2021-08-08 20:09:37,130 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:09:37,130 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:09:40,014 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:09:40,369 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:09:40,397 loading model...
2021-08-08 20:09:40,402 done!
2021-08-08 20:09:40,402 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:09:40,402 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:09:40,420 ***** Running training *****
2021-08-08 20:09:40,420   Num examples = 999
2021-08-08 20:09:40,421   Batch size = 32
2021-08-08 20:09:40,421   Num steps = 93
2021-08-08 20:09:40,422 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:09:40,422 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:09:40,422 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:09:40,422 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:09:40,422 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:09:40,422 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:09:40,423 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:09:40,424 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:09:40,425 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:09:40,425 n: module.bert.pooler.dense.weight
2021-08-08 20:09:40,426 n: module.bert.pooler.dense.bias
2021-08-08 20:09:40,426 n: module.classifier.weight
2021-08-08 20:09:40,426 n: module.classifier.bias
2021-08-08 20:09:40,426 n: module.fit_dense.weight
2021-08-08 20:09:40,426 n: module.fit_dense.bias
2021-08-08 20:09:40,426 Total parameters: 14591571
2021-08-08 20:10:53,636 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:10:53,765 device: cuda n_gpu: 4
2021-08-08 20:10:53,808 Writing example 0 of 999
2021-08-08 20:10:53,809 *** Example ***
2021-08-08 20:10:53,809 guid: aug-0
2021-08-08 20:10:53,809 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:10:53,809 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:53,810 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:53,810 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:53,810 label: neutral
2021-08-08 20:10:53,810 label_id: 2
2021-08-08 20:10:54,407 Writing example 0 of 9815
2021-08-08 20:10:54,407 *** Example ***
2021-08-08 20:10:54,407 guid: dev_matched-0
2021-08-08 20:10:54,408 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:10:54,408 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:54,408 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:54,408 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:10:54,408 label: neutral
2021-08-08 20:10:54,408 label_id: 2
2021-08-08 20:10:58,744 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:11:01,144 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:11:01,369 loading model...
2021-08-08 20:11:01,401 done!
2021-08-08 20:11:01,401 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:11:01,401 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:11:04,252 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:11:04,584 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:11:04,610 loading model...
2021-08-08 20:11:04,615 done!
2021-08-08 20:11:04,615 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:11:04,615 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:11:04,633 ***** Running training *****
2021-08-08 20:11:04,633   Num examples = 999
2021-08-08 20:11:04,633   Batch size = 32
2021-08-08 20:11:04,633   Num steps = 93
2021-08-08 20:11:04,634 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:11:04,634 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:11:04,634 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:11:04,634 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:11:04,634 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:11:04,634 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:11:04,634 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:11:04,635 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:11:04,636 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:11:04,637 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:11:04,638 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:11:04,638 n: module.bert.pooler.dense.weight
2021-08-08 20:11:04,638 n: module.bert.pooler.dense.bias
2021-08-08 20:11:04,638 n: module.classifier.weight
2021-08-08 20:11:04,638 n: module.classifier.bias
2021-08-08 20:11:04,638 n: module.fit_dense.weight
2021-08-08 20:11:04,638 n: module.fit_dense.bias
2021-08-08 20:11:04,638 Total parameters: 14591571
2021-08-08 20:11:26,092 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:11:26,222 device: cuda n_gpu: 4
2021-08-08 20:11:26,265 Writing example 0 of 999
2021-08-08 20:11:26,265 *** Example ***
2021-08-08 20:11:26,266 guid: aug-0
2021-08-08 20:11:26,266 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:11:26,266 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,266 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,266 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,266 label: neutral
2021-08-08 20:11:26,266 label_id: 2
2021-08-08 20:11:26,867 Writing example 0 of 9815
2021-08-08 20:11:26,868 *** Example ***
2021-08-08 20:11:26,868 guid: dev_matched-0
2021-08-08 20:11:26,868 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:11:26,868 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,868 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,868 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:11:26,868 label: neutral
2021-08-08 20:11:26,868 label_id: 2
2021-08-08 20:11:31,207 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:11:33,614 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:11:33,838 loading model...
2021-08-08 20:11:33,863 done!
2021-08-08 20:11:33,863 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:11:33,863 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:11:36,744 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:11:37,078 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:11:37,105 loading model...
2021-08-08 20:11:37,109 done!
2021-08-08 20:11:37,109 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:11:37,109 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:11:37,127 ***** Running training *****
2021-08-08 20:11:37,128   Num examples = 999
2021-08-08 20:11:37,128   Batch size = 32
2021-08-08 20:11:37,128   Num steps = 93
2021-08-08 20:11:37,129 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:11:37,129 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:11:37,129 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:11:37,129 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:11:37,129 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:11:37,129 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:11:37,130 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:11:37,131 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:11:37,132 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:11:37,133 n: module.bert.pooler.dense.weight
2021-08-08 20:11:37,133 n: module.bert.pooler.dense.bias
2021-08-08 20:11:37,133 n: module.classifier.weight
2021-08-08 20:11:37,133 n: module.classifier.bias
2021-08-08 20:11:37,133 n: module.fit_dense.weight
2021-08-08 20:11:37,133 n: module.fit_dense.bias
2021-08-08 20:11:37,133 Total parameters: 14591571
2021-08-08 20:12:20,455 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:12:20,586 device: cuda n_gpu: 4
2021-08-08 20:12:20,630 Writing example 0 of 999
2021-08-08 20:12:20,630 *** Example ***
2021-08-08 20:12:20,631 guid: aug-0
2021-08-08 20:12:20,631 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:12:20,631 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:20,631 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:20,631 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:20,631 label: neutral
2021-08-08 20:12:20,631 label_id: 2
2021-08-08 20:12:21,235 Writing example 0 of 9815
2021-08-08 20:12:21,236 *** Example ***
2021-08-08 20:12:21,236 guid: dev_matched-0
2021-08-08 20:12:21,236 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:12:21,236 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:21,236 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:21,236 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:12:21,236 label: neutral
2021-08-08 20:12:21,236 label_id: 2
2021-08-08 20:12:25,678 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:12:28,084 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:12:28,310 loading model...
2021-08-08 20:12:28,335 done!
2021-08-08 20:12:28,335 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:12:28,336 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:12:31,204 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:12:31,536 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:12:31,563 loading model...
2021-08-08 20:12:31,567 done!
2021-08-08 20:12:31,568 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:12:31,568 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:12:31,586 ***** Running training *****
2021-08-08 20:12:31,586   Num examples = 999
2021-08-08 20:12:31,586   Batch size = 32
2021-08-08 20:12:31,586   Num steps = 93
2021-08-08 20:12:31,587 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:12:31,587 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:12:31,587 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:12:31,587 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:12:31,587 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:12:31,587 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:12:31,587 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:12:31,587 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:12:31,587 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:12:31,588 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:12:31,589 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:12:31,590 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:12:31,591 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:12:31,591 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:12:31,591 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:12:31,591 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:12:31,591 n: module.bert.pooler.dense.weight
2021-08-08 20:12:31,591 n: module.bert.pooler.dense.bias
2021-08-08 20:12:31,591 n: module.classifier.weight
2021-08-08 20:12:31,591 n: module.classifier.bias
2021-08-08 20:12:31,591 n: module.fit_dense.weight
2021-08-08 20:12:31,591 n: module.fit_dense.bias
2021-08-08 20:12:31,591 Total parameters: 14591571
2021-08-08 20:13:31,909 The args: Namespace(aug_train=True, cache_dir='', cluster_distill=False, data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, student_model='/home/mcao610/scratch/General_TinyBERT_4L_312D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 20:13:32,040 device: cuda n_gpu: 4
2021-08-08 20:13:32,084 Writing example 0 of 999
2021-08-08 20:13:32,085 *** Example ***
2021-08-08 20:13:32,085 guid: aug-0
2021-08-08 20:13:32,085 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 20:13:32,085 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,085 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,085 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,085 label: neutral
2021-08-08 20:13:32,086 label_id: 2
2021-08-08 20:13:32,683 Writing example 0 of 9815
2021-08-08 20:13:32,684 *** Example ***
2021-08-08 20:13:32,684 guid: dev_matched-0
2021-08-08 20:13:32,684 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 20:13:32,684 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,684 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,684 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 20:13:32,684 label: neutral
2021-08-08 20:13:32,684 label_id: 2
2021-08-08 20:13:36,990 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 20:13:39,407 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 20:13:39,636 loading model...
2021-08-08 20:13:39,669 done!
2021-08-08 20:13:39,669 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 20:13:39,669 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 20:13:42,574 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 20:13:42,907 Loading model /home/mcao610/scratch/General_TinyBERT_4L_312D/pytorch_model.bin
2021-08-08 20:13:42,935 loading model...
2021-08-08 20:13:42,939 done!
2021-08-08 20:13:42,939 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-08 20:13:42,939 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-08 20:13:42,958 ***** Running training *****
2021-08-08 20:13:42,958   Num examples = 999
2021-08-08 20:13:42,958   Batch size = 32
2021-08-08 20:13:42,958   Num steps = 93
2021-08-08 20:13:42,959 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 20:13:42,959 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 20:13:42,959 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 20:13:42,959 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 20:13:42,959 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 20:13:42,959 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 20:13:42,960 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 20:13:42,961 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 20:13:42,962 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 20:13:42,963 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 20:13:42,963 n: module.bert.pooler.dense.weight
2021-08-08 20:13:42,963 n: module.bert.pooler.dense.bias
2021-08-08 20:13:42,963 n: module.classifier.weight
2021-08-08 20:13:42,963 n: module.classifier.bias
2021-08-08 20:13:42,963 n: module.fit_dense.weight
2021-08-08 20:13:42,963 n: module.fit_dense.bias
2021-08-08 20:13:42,963 Total parameters: 14591571
2021-08-08 21:53:14,299 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 21:53:14,455 device: cuda n_gpu: 4
2021-08-08 21:55:10,729 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 21:55:10,856 device: cuda n_gpu: 4
2021-08-08 21:55:18,110 Writing example 0 of 999
2021-08-08 21:55:18,111 *** Example ***
2021-08-08 21:55:18,111 guid: aug-0
2021-08-08 21:55:18,111 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 21:55:18,111 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:18,111 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:18,112 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:18,112 label: neutral
2021-08-08 21:55:18,112 label_id: 2
2021-08-08 21:55:19,270 Writing example 0 of 9815
2021-08-08 21:55:19,270 *** Example ***
2021-08-08 21:55:19,270 guid: dev_matched-0
2021-08-08 21:55:19,270 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 21:55:19,270 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:19,270 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:19,270 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:55:19,270 label: neutral
2021-08-08 21:55:19,270 label_id: 2
2021-08-08 21:55:23,776 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 21:55:26,241 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 21:55:37,955 loading model...
2021-08-08 21:55:37,987 done!
2021-08-08 21:55:37,987 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 21:55:37,987 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 21:55:44,592 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 21:55:46,089 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 21:55:49,196 loading model...
2021-08-08 21:55:49,209 done!
2021-08-08 21:55:49,209 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 21:55:49,209 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 21:55:49,276 ***** Running training *****
2021-08-08 21:55:49,276   Num examples = 999
2021-08-08 21:55:49,277   Batch size = 48
2021-08-08 21:55:49,277   Num steps = 60
2021-08-08 21:55:49,278 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 21:55:49,278 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 21:55:49,278 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 21:55:49,278 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 21:55:49,278 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 21:55:49,278 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 21:55:49,279 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 21:55:49,280 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 21:55:49,281 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 21:55:49,282 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 21:55:49,283 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 21:55:49,283 n: module.bert.pooler.dense.weight
2021-08-08 21:55:49,283 n: module.bert.pooler.dense.bias
2021-08-08 21:55:49,283 n: module.classifier.weight
2021-08-08 21:55:49,283 n: module.classifier.bias
2021-08-08 21:55:49,283 n: module.fit_dense.weight
2021-08-08 21:55:49,283 n: module.fit_dense.bias
2021-08-08 21:55:49,283 Total parameters: 67547907
2021-08-08 21:57:08,710 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 21:57:08,846 device: cuda n_gpu: 4
2021-08-08 21:57:17,316 Writing example 0 of 505555
2021-08-08 21:57:17,317 *** Example ***
2021-08-08 21:57:17,317 guid: aug-0
2021-08-08 21:57:17,317 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 21:57:17,317 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:57:17,317 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:57:17,317 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 21:57:17,317 label: neutral
2021-08-08 21:57:17,317 label_id: 2
2021-08-08 21:57:22,173 Writing example 10000 of 505555
2021-08-08 21:57:26,816 Writing example 20000 of 505555
2021-08-08 21:57:31,326 Writing example 30000 of 505555
2021-08-08 21:57:36,048 Writing example 40000 of 505555
2021-08-08 21:57:40,849 Writing example 50000 of 505555
2021-08-08 21:57:45,335 Writing example 60000 of 505555
2021-08-08 21:57:49,957 Writing example 70000 of 505555
2021-08-08 21:57:54,686 Writing example 80000 of 505555
2021-08-08 21:57:59,575 Writing example 90000 of 505555
2021-08-08 21:58:04,145 Writing example 100000 of 505555
2021-08-08 21:58:08,836 Writing example 110000 of 505555
2021-08-08 21:58:13,234 Writing example 120000 of 505555
2021-08-08 21:58:17,887 Writing example 130000 of 505555
2021-08-08 21:58:23,204 Writing example 140000 of 505555
2021-08-08 21:58:27,658 Writing example 150000 of 505555
2021-08-08 21:58:32,418 Writing example 160000 of 505555
2021-08-08 21:58:37,166 Writing example 170000 of 505555
2021-08-08 21:58:42,051 Writing example 180000 of 505555
2021-08-08 21:58:46,689 Writing example 190000 of 505555
2021-08-08 21:58:51,363 Writing example 200000 of 505555
2021-08-08 21:58:56,773 Writing example 210000 of 505555
2021-08-08 21:59:01,230 Writing example 220000 of 505555
2021-08-08 21:59:05,921 Writing example 230000 of 505555
2021-08-08 21:59:10,450 Writing example 240000 of 505555
2021-08-08 21:59:15,072 Writing example 250000 of 505555
2021-08-08 21:59:19,635 Writing example 260000 of 505555
2021-08-08 21:59:24,130 Writing example 270000 of 505555
2021-08-08 21:59:28,605 Writing example 280000 of 505555
2021-08-08 21:59:34,252 Writing example 290000 of 505555
2021-08-08 21:59:38,915 Writing example 300000 of 505555
2021-08-08 21:59:43,563 Writing example 310000 of 505555
2021-08-08 21:59:47,963 Writing example 320000 of 505555
2021-08-08 21:59:52,632 Writing example 330000 of 505555
2021-08-08 21:59:57,080 Writing example 340000 of 505555
2021-08-08 22:00:01,592 Writing example 350000 of 505555
2021-08-08 22:00:06,081 Writing example 360000 of 505555
2021-08-08 22:00:10,502 Writing example 370000 of 505555
2021-08-08 22:00:14,993 Writing example 380000 of 505555
2021-08-08 22:00:20,907 Writing example 390000 of 505555
2021-08-08 22:00:25,516 Writing example 400000 of 505555
2021-08-08 22:00:30,095 Writing example 410000 of 505555
2021-08-08 22:00:34,530 Writing example 420000 of 505555
2021-08-08 22:00:38,910 Writing example 430000 of 505555
2021-08-08 22:00:43,327 Writing example 440000 of 505555
2021-08-08 22:00:47,941 Writing example 450000 of 505555
2021-08-08 22:00:52,716 Writing example 460000 of 505555
2021-08-08 22:00:57,488 Writing example 470000 of 505555
2021-08-08 22:01:02,129 Writing example 480000 of 505555
2021-08-08 22:01:06,707 Writing example 490000 of 505555
2021-08-08 22:01:11,263 Writing example 500000 of 505555
2021-08-08 22:01:18,212 Writing example 0 of 9815
2021-08-08 22:01:18,212 *** Example ***
2021-08-08 22:01:18,212 guid: dev_matched-0
2021-08-08 22:01:18,212 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:01:18,212 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:01:18,213 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:01:18,213 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:01:18,213 label: neutral
2021-08-08 22:01:18,213 label_id: 2
2021-08-08 22:01:22,625 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:01:25,024 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:01:25,261 loading model...
2021-08-08 22:01:25,286 done!
2021-08-08 22:01:25,286 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:01:25,286 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:01:28,171 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:01:29,660 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 22:01:29,789 loading model...
2021-08-08 22:01:29,801 done!
2021-08-08 22:01:29,802 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:01:29,802 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 22:01:29,868 ***** Running training *****
2021-08-08 22:01:29,868   Num examples = 505555
2021-08-08 22:01:29,868   Batch size = 48
2021-08-08 22:01:29,869   Num steps = 31596
2021-08-08 22:01:29,869 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:01:29,870 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:01:29,870 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:01:29,870 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:01:29,870 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:01:29,870 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:01:29,871 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:01:29,872 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 22:01:29,873 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 22:01:29,874 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 22:01:29,875 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 22:01:29,875 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 22:01:29,875 n: module.bert.pooler.dense.weight
2021-08-08 22:01:29,875 n: module.bert.pooler.dense.bias
2021-08-08 22:01:29,875 n: module.classifier.weight
2021-08-08 22:01:29,875 n: module.classifier.bias
2021-08-08 22:01:29,875 n: module.fit_dense.weight
2021-08-08 22:01:29,875 n: module.fit_dense.bias
2021-08-08 22:01:29,875 Total parameters: 67547907
2021-08-08 22:02:48,755 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:02:48,886 device: cuda n_gpu: 4
2021-08-08 22:02:56,468 Writing example 0 of 505555
2021-08-08 22:02:56,469 *** Example ***
2021-08-08 22:02:56,469 guid: aug-0
2021-08-08 22:02:56,469 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 22:02:56,469 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:02:56,469 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:02:56,469 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:02:56,469 label: neutral
2021-08-08 22:02:56,469 label_id: 2
2021-08-08 22:03:01,301 Writing example 10000 of 505555
2021-08-08 22:03:05,960 Writing example 20000 of 505555
2021-08-08 22:03:10,747 Writing example 30000 of 505555
2021-08-08 22:03:15,460 Writing example 40000 of 505555
2021-08-08 22:03:20,262 Writing example 50000 of 505555
2021-08-08 22:03:24,808 Writing example 60000 of 505555
2021-08-08 22:03:29,484 Writing example 70000 of 505555
2021-08-08 22:03:34,048 Writing example 80000 of 505555
2021-08-08 22:03:39,086 Writing example 90000 of 505555
2021-08-08 22:03:43,736 Writing example 100000 of 505555
2021-08-08 22:03:48,326 Writing example 110000 of 505555
2021-08-08 22:03:52,691 Writing example 120000 of 505555
2021-08-08 22:03:57,435 Writing example 130000 of 505555
2021-08-08 22:04:02,808 Writing example 140000 of 505555
2021-08-08 22:04:07,240 Writing example 150000 of 505555
2021-08-08 22:04:12,024 Writing example 160000 of 505555
2021-08-08 22:04:16,674 Writing example 170000 of 505555
2021-08-08 22:04:21,254 Writing example 180000 of 505555
2021-08-08 22:04:25,857 Writing example 190000 of 505555
2021-08-08 22:04:30,526 Writing example 200000 of 505555
2021-08-08 22:04:35,955 Writing example 210000 of 505555
2021-08-08 22:04:40,418 Writing example 220000 of 505555
2021-08-08 22:04:45,050 Writing example 230000 of 505555
2021-08-08 22:04:49,594 Writing example 240000 of 505555
2021-08-08 22:04:54,233 Writing example 250000 of 505555
2021-08-08 22:04:58,853 Writing example 260000 of 505555
2021-08-08 22:05:03,477 Writing example 270000 of 505555
2021-08-08 22:05:07,944 Writing example 280000 of 505555
2021-08-08 22:05:13,654 Writing example 290000 of 505555
2021-08-08 22:05:18,339 Writing example 300000 of 505555
2021-08-08 22:05:22,895 Writing example 310000 of 505555
2021-08-08 22:05:27,356 Writing example 320000 of 505555
2021-08-08 22:05:32,039 Writing example 330000 of 505555
2021-08-08 22:05:36,479 Writing example 340000 of 505555
2021-08-08 22:05:40,993 Writing example 350000 of 505555
2021-08-08 22:05:45,579 Writing example 360000 of 505555
2021-08-08 22:05:49,988 Writing example 370000 of 505555
2021-08-08 22:05:54,472 Writing example 380000 of 505555
2021-08-08 22:06:00,490 Writing example 390000 of 505555
2021-08-08 22:06:05,096 Writing example 400000 of 505555
2021-08-08 22:06:09,644 Writing example 410000 of 505555
2021-08-08 22:06:14,096 Writing example 420000 of 505555
2021-08-08 22:06:18,483 Writing example 430000 of 505555
2021-08-08 22:06:22,883 Writing example 440000 of 505555
2021-08-08 22:06:27,613 Writing example 450000 of 505555
2021-08-08 22:06:32,231 Writing example 460000 of 505555
2021-08-08 22:06:36,799 Writing example 470000 of 505555
2021-08-08 22:06:41,467 Writing example 480000 of 505555
2021-08-08 22:06:46,053 Writing example 490000 of 505555
2021-08-08 22:06:50,608 Writing example 500000 of 505555
2021-08-08 22:06:57,669 Writing example 0 of 9815
2021-08-08 22:06:57,669 *** Example ***
2021-08-08 22:06:57,669 guid: dev_matched-0
2021-08-08 22:06:57,669 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:06:57,669 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:06:57,670 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:06:57,670 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:06:57,670 label: neutral
2021-08-08 22:06:57,670 label_id: 2
2021-08-08 22:07:01,968 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:07:04,367 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:07:04,596 loading model...
2021-08-08 22:07:04,629 done!
2021-08-08 22:07:04,629 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:07:04,629 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:07:07,496 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:07:08,988 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 22:07:09,109 loading model...
2021-08-08 22:07:09,122 done!
2021-08-08 22:07:09,122 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:07:09,122 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 22:07:09,189 ***** Running training *****
2021-08-08 22:07:09,189   Num examples = 505555
2021-08-08 22:07:09,189   Batch size = 48
2021-08-08 22:07:09,190   Num steps = 31596
2021-08-08 22:07:09,191 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:07:09,191 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:07:09,191 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:07:09,191 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:07:09,191 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:07:09,191 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:07:09,192 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:07:09,193 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 22:07:09,194 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 22:07:09,195 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 22:07:09,196 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 22:07:09,196 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 22:07:09,196 n: module.bert.pooler.dense.weight
2021-08-08 22:07:09,196 n: module.bert.pooler.dense.bias
2021-08-08 22:07:09,196 n: module.classifier.weight
2021-08-08 22:07:09,196 n: module.classifier.bias
2021-08-08 22:07:09,196 n: module.fit_dense.weight
2021-08-08 22:07:09,196 n: module.fit_dense.bias
2021-08-08 22:07:09,196 Total parameters: 67547907
2021-08-08 22:07:58,986 ***** Running evaluation *****
2021-08-08 22:07:58,988   Epoch = 0 iter 199 step
2021-08-08 22:07:58,988   Num examples = 9815
2021-08-08 22:07:58,988   Batch size = 32
2021-08-08 22:08:09,844 ***** Eval results *****
2021-08-08 22:08:09,845   acc = 0.8235354049923587
2021-08-08 22:08:09,845   att_loss = 2.4075173804508383
2021-08-08 22:08:09,845   cls_loss = 0.0
2021-08-08 22:08:09,845   eval_loss = 0.47203458102984225
2021-08-08 22:08:09,845   global_step = 199
2021-08-08 22:08:09,845   loss = 4.365899439433112
2021-08-08 22:08:09,845   rep_loss = 1.9583820595813157
2021-08-08 22:08:09,846 ***** Save model *****
2021-08-08 22:08:13,059 Writing example 0 of 9832
2021-08-08 22:08:13,060 *** Example ***
2021-08-08 22:08:13,060 guid: dev_matched-0
2021-08-08 22:08:13,060 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:08:13,060 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:08:13,060 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:08:13,060 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:08:13,060 label: contradiction
2021-08-08 22:08:13,060 label_id: 0
2021-08-08 22:08:17,644 ***** Running mm evaluation *****
2021-08-08 22:08:17,645   Num examples = 9832
2021-08-08 22:08:17,645   Batch size = 32
2021-08-08 22:08:28,575 ***** Eval results *****
2021-08-08 22:08:28,575   acc = 0.8324857607811229
2021-08-08 22:08:28,575   eval_loss = 0.4595888331048674
2021-08-08 22:08:28,575   global_step = 199
2021-08-08 22:09:20,429 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:09:20,567 device: cuda n_gpu: 4
2021-08-08 22:09:53,319 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:09:53,444 device: cuda n_gpu: 4
2021-08-08 22:09:53,505 Writing example 0 of 999
2021-08-08 22:09:53,505 *** Example ***
2021-08-08 22:09:53,506 guid: aug-0
2021-08-08 22:09:53,506 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 22:09:53,506 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:53,506 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:53,506 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:53,506 label: neutral
2021-08-08 22:09:53,506 label_id: 2
2021-08-08 22:09:54,114 Writing example 0 of 9815
2021-08-08 22:09:54,114 *** Example ***
2021-08-08 22:09:54,114 guid: dev_matched-0
2021-08-08 22:09:54,114 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:09:54,114 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:54,114 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:54,114 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:09:54,114 label: neutral
2021-08-08 22:09:54,114 label_id: 2
2021-08-08 22:09:58,545 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:10:00,951 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:10:01,183 loading model...
2021-08-08 22:10:01,208 done!
2021-08-08 22:10:01,208 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:10:01,208 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:10:04,068 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:10:05,559 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 22:10:05,681 loading model...
2021-08-08 22:10:05,694 done!
2021-08-08 22:10:05,694 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:10:05,694 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 22:10:05,761 ***** Running training *****
2021-08-08 22:10:05,761   Num examples = 999
2021-08-08 22:10:05,762   Batch size = 48
2021-08-08 22:10:05,762   Num steps = 60
2021-08-08 22:10:05,763 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:10:05,763 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:10:05,763 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:10:05,763 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:10:05,763 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:10:05,763 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:10:05,764 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:10:05,765 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 22:10:05,766 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 22:10:05,767 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 22:10:05,768 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 22:10:05,768 n: module.bert.pooler.dense.weight
2021-08-08 22:10:05,768 n: module.bert.pooler.dense.bias
2021-08-08 22:10:05,768 n: module.classifier.weight
2021-08-08 22:10:05,768 n: module.classifier.bias
2021-08-08 22:10:05,768 n: module.fit_dense.weight
2021-08-08 22:10:05,768 n: module.fit_dense.bias
2021-08-08 22:10:05,768 Total parameters: 67547907
2021-08-08 22:12:04,435 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/TinyBERT_4L_312D/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:12:04,570 device: cuda n_gpu: 4
2021-08-08 22:12:04,666 Writing example 0 of 999
2021-08-08 22:12:04,667 *** Example ***
2021-08-08 22:12:04,667 guid: aug-0
2021-08-08 22:12:04,667 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 22:12:04,667 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:04,667 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:04,667 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:04,667 label: neutral
2021-08-08 22:12:04,667 label_id: 2
2021-08-08 22:12:05,269 Writing example 0 of 9815
2021-08-08 22:12:05,269 *** Example ***
2021-08-08 22:12:05,269 guid: dev_matched-0
2021-08-08 22:12:05,269 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:12:05,270 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:05,270 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:05,270 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:12:05,270 label: neutral
2021-08-08 22:12:05,270 label_id: 2
2021-08-08 22:12:09,613 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:12:12,019 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:12:12,250 loading model...
2021-08-08 22:12:12,275 done!
2021-08-08 22:12:12,275 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:12:12,275 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:12:15,134 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:12:15,468 Loading model /home/mcao610/scratch/TinyBERT_4L_312D/MNLI/pytorch_model.bin
2021-08-08 22:12:15,819 loading model...
2021-08-08 22:12:15,823 done!
2021-08-08 22:12:15,842 ***** Running training *****
2021-08-08 22:12:15,842   Num examples = 999
2021-08-08 22:12:15,842   Batch size = 48
2021-08-08 22:12:15,842   Num steps = 60
2021-08-08 22:12:15,843 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:12:15,843 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:12:15,843 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:12:15,843 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:12:15,843 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:12:15,843 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:12:15,844 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:12:15,845 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:12:15,846 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:12:15,847 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:12:15,847 n: module.bert.pooler.dense.weight
2021-08-08 22:12:15,847 n: module.bert.pooler.dense.bias
2021-08-08 22:12:15,847 n: module.classifier.weight
2021-08-08 22:12:15,847 n: module.classifier.bias
2021-08-08 22:12:15,847 n: module.fit_dense.weight
2021-08-08 22:12:15,847 n: module.fit_dense.bias
2021-08-08 22:12:15,847 Total parameters: 14591571
2021-08-08 22:13:12,658 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:13:12,791 device: cuda n_gpu: 4
2021-08-08 22:13:20,919 Writing example 0 of 505555
2021-08-08 22:13:20,921 *** Example ***
2021-08-08 22:13:20,921 guid: aug-0
2021-08-08 22:13:20,921 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 22:13:20,921 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:13:20,921 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:13:20,921 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:13:20,921 label: neutral
2021-08-08 22:13:20,921 label_id: 2
2021-08-08 22:13:25,602 Writing example 10000 of 505555
2021-08-08 22:13:30,323 Writing example 20000 of 505555
2021-08-08 22:13:34,845 Writing example 30000 of 505555
2021-08-08 22:13:39,546 Writing example 40000 of 505555
2021-08-08 22:13:44,380 Writing example 50000 of 505555
2021-08-08 22:13:48,871 Writing example 60000 of 505555
2021-08-08 22:13:53,496 Writing example 70000 of 505555
2021-08-08 22:13:58,109 Writing example 80000 of 505555
2021-08-08 22:14:02,992 Writing example 90000 of 505555
2021-08-08 22:14:07,873 Writing example 100000 of 505555
2021-08-08 22:14:12,618 Writing example 110000 of 505555
2021-08-08 22:14:16,975 Writing example 120000 of 505555
2021-08-08 22:14:21,666 Writing example 130000 of 505555
2021-08-08 22:14:27,010 Writing example 140000 of 505555
2021-08-08 22:14:31,466 Writing example 150000 of 505555
2021-08-08 22:14:36,232 Writing example 160000 of 505555
2021-08-08 22:14:40,951 Writing example 170000 of 505555
2021-08-08 22:14:45,547 Writing example 180000 of 505555
2021-08-08 22:14:50,148 Writing example 190000 of 505555
2021-08-08 22:14:54,713 Writing example 200000 of 505555
2021-08-08 22:15:00,351 Writing example 210000 of 505555
2021-08-08 22:15:04,914 Writing example 220000 of 505555
2021-08-08 22:15:09,544 Writing example 230000 of 505555
2021-08-08 22:15:14,131 Writing example 240000 of 505555
2021-08-08 22:15:18,760 Writing example 250000 of 505555
2021-08-08 22:15:23,442 Writing example 260000 of 505555
2021-08-08 22:15:28,046 Writing example 270000 of 505555
2021-08-08 22:15:32,760 Writing example 280000 of 505555
2021-08-08 22:15:38,475 Writing example 290000 of 505555
2021-08-08 22:15:43,202 Writing example 300000 of 505555
2021-08-08 22:15:47,736 Writing example 310000 of 505555
2021-08-08 22:15:52,224 Writing example 320000 of 505555
2021-08-08 22:15:57,046 Writing example 330000 of 505555
2021-08-08 22:16:01,476 Writing example 340000 of 505555
2021-08-08 22:16:05,977 Writing example 350000 of 505555
2021-08-08 22:16:10,464 Writing example 360000 of 505555
2021-08-08 22:16:14,877 Writing example 370000 of 505555
2021-08-08 22:16:19,366 Writing example 380000 of 505555
2021-08-08 22:16:25,401 Writing example 390000 of 505555
2021-08-08 22:16:30,042 Writing example 400000 of 505555
2021-08-08 22:16:34,581 Writing example 410000 of 505555
2021-08-08 22:16:39,019 Writing example 420000 of 505555
2021-08-08 22:16:43,412 Writing example 430000 of 505555
2021-08-08 22:16:47,815 Writing example 440000 of 505555
2021-08-08 22:16:52,432 Writing example 450000 of 505555
2021-08-08 22:16:57,139 Writing example 460000 of 505555
2021-08-08 22:17:01,878 Writing example 470000 of 505555
2021-08-08 22:17:06,621 Writing example 480000 of 505555
2021-08-08 22:17:11,200 Writing example 490000 of 505555
2021-08-08 22:17:15,754 Writing example 500000 of 505555
2021-08-08 22:17:22,833 Writing example 0 of 9815
2021-08-08 22:17:22,834 *** Example ***
2021-08-08 22:17:22,834 guid: dev_matched-0
2021-08-08 22:17:22,834 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:17:22,834 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:17:22,834 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:17:22,834 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:17:22,834 label: neutral
2021-08-08 22:17:22,834 label_id: 2
2021-08-08 22:17:27,222 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:17:29,621 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:17:29,856 loading model...
2021-08-08 22:17:29,881 done!
2021-08-08 22:17:29,881 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:17:29,881 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:17:32,732 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:17:34,217 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 22:17:34,341 loading model...
2021-08-08 22:17:34,354 done!
2021-08-08 22:17:34,354 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:17:34,354 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 22:17:34,422 ***** Running training *****
2021-08-08 22:17:34,422   Num examples = 505555
2021-08-08 22:17:34,422   Batch size = 48
2021-08-08 22:17:34,422   Num steps = 31596
2021-08-08 22:17:34,423 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:17:34,423 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:17:34,423 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:17:34,423 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:17:34,423 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:17:34,423 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:17:34,423 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:17:34,423 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:17:34,424 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:17:34,425 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:17:34,426 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 22:17:34,427 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 22:17:34,428 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 22:17:34,428 n: module.bert.pooler.dense.weight
2021-08-08 22:17:34,428 n: module.bert.pooler.dense.bias
2021-08-08 22:17:34,428 n: module.classifier.weight
2021-08-08 22:17:34,428 n: module.classifier.bias
2021-08-08 22:17:34,428 n: module.fit_dense.weight
2021-08-08 22:17:34,428 n: module.fit_dense.bias
2021-08-08 22:17:34,428 Total parameters: 67547907
2021-08-08 22:18:12,236 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-08 22:18:12,372 device: cuda n_gpu: 4
2021-08-08 22:18:19,894 Writing example 0 of 505555
2021-08-08 22:18:19,896 *** Example ***
2021-08-08 22:18:19,896 guid: aug-0
2021-08-08 22:18:19,896 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-08 22:18:19,896 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:18:19,896 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:18:19,896 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:18:19,896 label: neutral
2021-08-08 22:18:19,896 label_id: 2
2021-08-08 22:18:24,613 Writing example 10000 of 505555
2021-08-08 22:18:29,300 Writing example 20000 of 505555
2021-08-08 22:18:33,811 Writing example 30000 of 505555
2021-08-08 22:18:38,510 Writing example 40000 of 505555
2021-08-08 22:18:43,456 Writing example 50000 of 505555
2021-08-08 22:18:48,087 Writing example 60000 of 505555
2021-08-08 22:18:52,752 Writing example 70000 of 505555
2021-08-08 22:18:57,385 Writing example 80000 of 505555
2021-08-08 22:19:02,254 Writing example 90000 of 505555
2021-08-08 22:19:07,003 Writing example 100000 of 505555
2021-08-08 22:19:11,583 Writing example 110000 of 505555
2021-08-08 22:19:15,939 Writing example 120000 of 505555
2021-08-08 22:19:20,640 Writing example 130000 of 505555
2021-08-08 22:19:25,992 Writing example 140000 of 505555
2021-08-08 22:19:30,436 Writing example 150000 of 505555
2021-08-08 22:19:35,201 Writing example 160000 of 505555
2021-08-08 22:19:39,847 Writing example 170000 of 505555
2021-08-08 22:19:44,454 Writing example 180000 of 505555
2021-08-08 22:19:49,055 Writing example 190000 of 505555
2021-08-08 22:19:53,619 Writing example 200000 of 505555
2021-08-08 22:19:59,248 Writing example 210000 of 505555
2021-08-08 22:20:03,848 Writing example 220000 of 505555
2021-08-08 22:20:08,513 Writing example 230000 of 505555
2021-08-08 22:20:13,049 Writing example 240000 of 505555
2021-08-08 22:20:17,672 Writing example 250000 of 505555
2021-08-08 22:20:22,255 Writing example 260000 of 505555
2021-08-08 22:20:26,787 Writing example 270000 of 505555
2021-08-08 22:20:31,238 Writing example 280000 of 505555
2021-08-08 22:20:36,945 Writing example 290000 of 505555
2021-08-08 22:20:41,614 Writing example 300000 of 505555
2021-08-08 22:20:46,162 Writing example 310000 of 505555
2021-08-08 22:20:50,605 Writing example 320000 of 505555
2021-08-08 22:20:55,324 Writing example 330000 of 505555
2021-08-08 22:20:59,759 Writing example 340000 of 505555
2021-08-08 22:21:04,475 Writing example 350000 of 505555
2021-08-08 22:21:08,961 Writing example 360000 of 505555
2021-08-08 22:21:13,351 Writing example 370000 of 505555
2021-08-08 22:21:17,823 Writing example 380000 of 505555
2021-08-08 22:21:23,786 Writing example 390000 of 505555
2021-08-08 22:21:28,537 Writing example 400000 of 505555
2021-08-08 22:21:33,164 Writing example 410000 of 505555
2021-08-08 22:21:37,594 Writing example 420000 of 505555
2021-08-08 22:21:41,983 Writing example 430000 of 505555
2021-08-08 22:21:46,389 Writing example 440000 of 505555
2021-08-08 22:21:51,016 Writing example 450000 of 505555
2021-08-08 22:21:55,636 Writing example 460000 of 505555
2021-08-08 22:22:00,219 Writing example 470000 of 505555
2021-08-08 22:22:04,841 Writing example 480000 of 505555
2021-08-08 22:22:09,408 Writing example 490000 of 505555
2021-08-08 22:22:13,944 Writing example 500000 of 505555
2021-08-08 22:22:20,936 Writing example 0 of 9815
2021-08-08 22:22:20,936 *** Example ***
2021-08-08 22:22:20,936 guid: dev_matched-0
2021-08-08 22:22:20,936 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-08 22:22:20,936 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:22:20,937 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:22:20,937 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:22:20,937 label: neutral
2021-08-08 22:22:20,937 label_id: 2
2021-08-08 22:22:25,238 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-08 22:22:27,656 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-08 22:22:27,884 loading model...
2021-08-08 22:22:27,909 done!
2021-08-08 22:22:27,910 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:22:27,910 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-08 22:22:30,721 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-08 22:22:32,208 Loading model /home/mcao610/scratch/6L_768D_FinalModel/MNLI/pytorch_model.bin
2021-08-08 22:22:32,330 loading model...
2021-08-08 22:22:32,343 done!
2021-08-08 22:22:32,343 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-08 22:22:32,343 Weights from pretrained model not used in TinyBertForSequenceClassification: ['fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2021-08-08 22:22:32,410 ***** Running training *****
2021-08-08 22:22:32,410   Num examples = 505555
2021-08-08 22:22:32,410   Batch size = 48
2021-08-08 22:22:32,410   Num steps = 31596
2021-08-08 22:22:32,411 n: module.bert.embeddings.word_embeddings.weight
2021-08-08 22:22:32,411 n: module.bert.embeddings.position_embeddings.weight
2021-08-08 22:22:32,411 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-08 22:22:32,411 n: module.bert.embeddings.LayerNorm.weight
2021-08-08 22:22:32,411 n: module.bert.embeddings.LayerNorm.bias
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-08 22:22:32,411 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-08 22:22:32,412 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-08 22:22:32,413 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-08 22:22:32,414 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-08 22:22:32,415 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-08 22:22:32,416 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-08 22:22:32,416 n: module.bert.pooler.dense.weight
2021-08-08 22:22:32,416 n: module.bert.pooler.dense.bias
2021-08-08 22:22:32,416 n: module.classifier.weight
2021-08-08 22:22:32,416 n: module.classifier.bias
2021-08-08 22:22:32,416 n: module.fit_dense.weight
2021-08-08 22:22:32,416 n: module.fit_dense.bias
2021-08-08 22:22:32,416 Total parameters: 67547907
2021-08-08 22:23:18,354 ***** Running evaluation *****
2021-08-08 22:23:18,354   Epoch = 0 iter 199 step
2021-08-08 22:23:18,354   Num examples = 9815
2021-08-08 22:23:18,354   Batch size = 32
2021-08-08 22:23:29,226 ***** Eval results *****
2021-08-08 22:23:29,226   acc = 0.8382068262862965
2021-08-08 22:23:29,226   att_loss = 0.0
2021-08-08 22:23:29,226   cls_loss = 0.07908048817710062
2021-08-08 22:23:29,226   eval_loss = 0.45151644814092096
2021-08-08 22:23:29,226   global_step = 199
2021-08-08 22:23:29,226   loss = 0.07908048817710062
2021-08-08 22:23:29,226   rep_loss = 0.0
2021-08-08 22:23:29,227 ***** Save model *****
2021-08-08 22:23:31,494 Writing example 0 of 9832
2021-08-08 22:23:31,495 *** Example ***
2021-08-08 22:23:31,495 guid: dev_matched-0
2021-08-08 22:23:31,495 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:23:31,495 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:23:31,495 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:23:31,495 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:23:31,495 label: contradiction
2021-08-08 22:23:31,495 label_id: 0
2021-08-08 22:23:36,090 ***** Running mm evaluation *****
2021-08-08 22:23:36,090   Num examples = 9832
2021-08-08 22:23:36,090   Batch size = 32
2021-08-08 22:23:46,918 ***** Eval results *****
2021-08-08 22:23:46,918   acc = 0.8342148087876322
2021-08-08 22:23:46,918   eval_loss = 0.45684694885820537
2021-08-08 22:23:46,918   global_step = 199
2021-08-08 22:24:25,289 ***** Running evaluation *****
2021-08-08 22:24:25,290   Epoch = 0 iter 399 step
2021-08-08 22:24:25,290   Num examples = 9832
2021-08-08 22:24:25,290   Batch size = 32
2021-08-08 22:24:38,241 ***** Eval results *****
2021-08-08 22:24:38,241   acc = 0.8252644426362896
2021-08-08 22:24:38,241   att_loss = 0.0
2021-08-08 22:24:38,241   cls_loss = 0.07882245138922431
2021-08-08 22:24:38,241   eval_loss = 0.5420242345758847
2021-08-08 22:24:38,241   global_step = 399
2021-08-08 22:24:38,241   loss = 0.07882245138922431
2021-08-08 22:24:38,241   rep_loss = 0.0
2021-08-08 22:24:38,243 ***** Save model *****
2021-08-08 22:24:49,417 Writing example 0 of 9832
2021-08-08 22:24:49,418 *** Example ***
2021-08-08 22:24:49,418 guid: dev_matched-0
2021-08-08 22:24:49,418 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:24:49,418 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:24:49,418 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:24:49,418 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:24:49,418 label: contradiction
2021-08-08 22:24:49,418 label_id: 0
2021-08-08 22:24:54,095 ***** Running mm evaluation *****
2021-08-08 22:24:54,095   Num examples = 9832
2021-08-08 22:24:54,096   Batch size = 32
2021-08-08 22:25:04,932 ***** Eval results *****
2021-08-08 22:25:04,933   acc = 0.8252644426362896
2021-08-08 22:25:04,933   eval_loss = 0.5420242345758847
2021-08-08 22:25:04,933   global_step = 399
2021-08-08 22:25:41,390 ***** Running evaluation *****
2021-08-08 22:25:41,391   Epoch = 0 iter 599 step
2021-08-08 22:25:41,391   Num examples = 9832
2021-08-08 22:25:41,391   Batch size = 32
2021-08-08 22:25:54,142 ***** Eval results *****
2021-08-08 22:25:54,142   acc = 0.8234336859235151
2021-08-08 22:25:54,142   att_loss = 0.0
2021-08-08 22:25:54,142   cls_loss = 0.07870971622918803
2021-08-08 22:25:54,142   eval_loss = 0.6004836280624588
2021-08-08 22:25:54,142   global_step = 599
2021-08-08 22:25:54,142   loss = 0.07870971622918803
2021-08-08 22:25:54,142   rep_loss = 0.0
2021-08-08 22:25:54,143 ***** Save model *****
2021-08-08 22:25:55,131 Writing example 0 of 9832
2021-08-08 22:25:55,132 *** Example ***
2021-08-08 22:25:55,132 guid: dev_matched-0
2021-08-08 22:25:55,132 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:25:55,132 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:25:55,132 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:25:55,132 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:25:55,132 label: contradiction
2021-08-08 22:25:55,132 label_id: 0
2021-08-08 22:25:59,731 ***** Running mm evaluation *****
2021-08-08 22:25:59,731   Num examples = 9832
2021-08-08 22:25:59,731   Batch size = 32
2021-08-08 22:26:10,594 ***** Eval results *****
2021-08-08 22:26:10,594   acc = 0.8234336859235151
2021-08-08 22:26:10,594   eval_loss = 0.6004836280624588
2021-08-08 22:26:10,594   global_step = 599
2021-08-08 22:26:49,027 ***** Running evaluation *****
2021-08-08 22:26:49,028   Epoch = 0 iter 799 step
2021-08-08 22:26:49,028   Num examples = 9832
2021-08-08 22:26:49,028   Batch size = 32
2021-08-08 22:26:59,909 ***** Eval results *****
2021-08-08 22:26:59,909   acc = 0.807160292921074
2021-08-08 22:26:59,909   att_loss = 0.0
2021-08-08 22:26:59,909   cls_loss = 0.07864614519219523
2021-08-08 22:26:59,909   eval_loss = 0.6552420178210581
2021-08-08 22:26:59,909   global_step = 799
2021-08-08 22:26:59,909   loss = 0.07864614519219523
2021-08-08 22:26:59,909   rep_loss = 0.0
2021-08-08 22:26:59,910 ***** Save model *****
2021-08-08 22:27:00,815 Writing example 0 of 9832
2021-08-08 22:27:00,816 *** Example ***
2021-08-08 22:27:00,816 guid: dev_matched-0
2021-08-08 22:27:00,816 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:27:00,816 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:27:00,816 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:27:00,816 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:27:00,816 label: contradiction
2021-08-08 22:27:00,816 label_id: 0
2021-08-08 22:27:05,390 ***** Running mm evaluation *****
2021-08-08 22:27:05,390   Num examples = 9832
2021-08-08 22:27:05,390   Batch size = 32
2021-08-08 22:27:16,251 ***** Eval results *****
2021-08-08 22:27:16,251   acc = 0.807160292921074
2021-08-08 22:27:16,251   eval_loss = 0.6552420178210581
2021-08-08 22:27:16,251   global_step = 799
2021-08-08 22:27:52,812 ***** Running evaluation *****
2021-08-08 22:27:52,813   Epoch = 0 iter 999 step
2021-08-08 22:27:52,813   Num examples = 9832
2021-08-08 22:27:52,813   Batch size = 32
2021-08-08 22:28:05,618 ***** Eval results *****
2021-08-08 22:28:05,618   acc = 0.8070585842148088
2021-08-08 22:28:05,618   att_loss = 0.0
2021-08-08 22:28:05,618   cls_loss = 0.07860975288175368
2021-08-08 22:28:05,618   eval_loss = 0.6848450726889944
2021-08-08 22:28:05,618   global_step = 999
2021-08-08 22:28:05,618   loss = 0.07860975288175368
2021-08-08 22:28:05,618   rep_loss = 0.0
2021-08-08 22:28:05,618 ***** Save model *****
2021-08-08 22:28:17,281 Writing example 0 of 9832
2021-08-08 22:28:17,281 *** Example ***
2021-08-08 22:28:17,281 guid: dev_matched-0
2021-08-08 22:28:17,281 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:28:17,281 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:28:17,282 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:28:17,282 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:28:17,282 label: contradiction
2021-08-08 22:28:17,282 label_id: 0
2021-08-08 22:28:21,864 ***** Running mm evaluation *****
2021-08-08 22:28:21,864   Num examples = 9832
2021-08-08 22:28:21,864   Batch size = 32
2021-08-08 22:28:32,725 ***** Eval results *****
2021-08-08 22:28:32,725   acc = 0.8070585842148088
2021-08-08 22:28:32,725   eval_loss = 0.6848450726889944
2021-08-08 22:28:32,725   global_step = 999
2021-08-08 22:29:11,209 ***** Running evaluation *****
2021-08-08 22:29:11,210   Epoch = 0 iter 1199 step
2021-08-08 22:29:11,210   Num examples = 9832
2021-08-08 22:29:11,210   Batch size = 32
2021-08-08 22:29:22,070 ***** Eval results *****
2021-08-08 22:29:22,071   acc = 0.7960740439381611
2021-08-08 22:29:22,071   att_loss = 0.0
2021-08-08 22:29:22,071   cls_loss = 0.07859194233354874
2021-08-08 22:29:22,071   eval_loss = 0.7037909657924206
2021-08-08 22:29:22,071   global_step = 1199
2021-08-08 22:29:22,071   loss = 0.07859194233354874
2021-08-08 22:29:22,071   rep_loss = 0.0
2021-08-08 22:29:22,071 ***** Save model *****
2021-08-08 22:29:23,588 Writing example 0 of 9832
2021-08-08 22:29:23,588 *** Example ***
2021-08-08 22:29:23,588 guid: dev_matched-0
2021-08-08 22:29:23,588 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:29:23,589 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:29:23,589 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:29:23,589 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:29:23,589 label: contradiction
2021-08-08 22:29:23,589 label_id: 0
2021-08-08 22:29:28,253 ***** Running mm evaluation *****
2021-08-08 22:29:28,253   Num examples = 9832
2021-08-08 22:29:28,253   Batch size = 32
2021-08-08 22:29:39,207 ***** Eval results *****
2021-08-08 22:29:39,208   acc = 0.7960740439381611
2021-08-08 22:29:39,208   eval_loss = 0.7037909657924206
2021-08-08 22:29:39,208   global_step = 1199
2021-08-08 22:30:17,751 ***** Running evaluation *****
2021-08-08 22:30:17,752   Epoch = 0 iter 1399 step
2021-08-08 22:30:17,752   Num examples = 9832
2021-08-08 22:30:17,752   Batch size = 32
2021-08-08 22:30:28,611 ***** Eval results *****
2021-08-08 22:30:28,611   acc = 0.7741049633848658
2021-08-08 22:30:28,611   att_loss = 0.0
2021-08-08 22:30:28,611   cls_loss = 0.07857349279035067
2021-08-08 22:30:28,611   eval_loss = 0.7306953780836873
2021-08-08 22:30:28,612   global_step = 1399
2021-08-08 22:30:28,612   loss = 0.07857349279035067
2021-08-08 22:30:28,612   rep_loss = 0.0
2021-08-08 22:30:28,612 ***** Save model *****
2021-08-08 22:30:29,610 Writing example 0 of 9832
2021-08-08 22:30:29,611 *** Example ***
2021-08-08 22:30:29,611 guid: dev_matched-0
2021-08-08 22:30:29,611 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:30:29,611 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:30:29,611 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:30:29,611 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:30:29,611 label: contradiction
2021-08-08 22:30:29,611 label_id: 0
2021-08-08 22:30:34,206 ***** Running mm evaluation *****
2021-08-08 22:30:34,206   Num examples = 9832
2021-08-08 22:30:34,206   Batch size = 32
2021-08-08 22:30:45,067 ***** Eval results *****
2021-08-08 22:30:45,067   acc = 0.7741049633848658
2021-08-08 22:30:45,067   eval_loss = 0.7306953780836873
2021-08-08 22:30:45,067   global_step = 1399
2021-08-08 22:31:23,190 ***** Running evaluation *****
2021-08-08 22:31:23,190   Epoch = 0 iter 1599 step
2021-08-08 22:31:23,190   Num examples = 9832
2021-08-08 22:31:23,190   Batch size = 32
2021-08-08 22:31:34,094 ***** Eval results *****
2021-08-08 22:31:34,094   acc = 0.7671887713588283
2021-08-08 22:31:34,094   att_loss = 0.0
2021-08-08 22:31:34,094   cls_loss = 0.07855418696300621
2021-08-08 22:31:34,094   eval_loss = 0.7477457471095122
2021-08-08 22:31:34,094   global_step = 1599
2021-08-08 22:31:34,094   loss = 0.07855418696300621
2021-08-08 22:31:34,094   rep_loss = 0.0
2021-08-08 22:31:34,094 ***** Save model *****
2021-08-08 22:31:35,653 Writing example 0 of 9832
2021-08-08 22:31:35,654 *** Example ***
2021-08-08 22:31:35,654 guid: dev_matched-0
2021-08-08 22:31:35,654 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:31:35,654 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:31:35,654 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:31:35,654 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:31:35,654 label: contradiction
2021-08-08 22:31:35,654 label_id: 0
2021-08-08 22:31:40,251 ***** Running mm evaluation *****
2021-08-08 22:31:40,251   Num examples = 9832
2021-08-08 22:31:40,251   Batch size = 32
2021-08-08 22:31:51,137 ***** Eval results *****
2021-08-08 22:31:51,137   acc = 0.7671887713588283
2021-08-08 22:31:51,137   eval_loss = 0.7477457471095122
2021-08-08 22:31:51,137   global_step = 1599
2021-08-08 22:32:31,309 ***** Running evaluation *****
2021-08-08 22:32:31,310   Epoch = 0 iter 1799 step
2021-08-08 22:32:31,310   Num examples = 9832
2021-08-08 22:32:31,310   Batch size = 32
2021-08-08 22:32:42,178 ***** Eval results *****
2021-08-08 22:32:42,178   acc = 0.7483726606997559
2021-08-08 22:32:42,178   att_loss = 0.0
2021-08-08 22:32:42,178   cls_loss = 0.07853740192804819
2021-08-08 22:32:42,178   eval_loss = 0.7548726756464351
2021-08-08 22:32:42,178   global_step = 1799
2021-08-08 22:32:42,178   loss = 0.07853740192804819
2021-08-08 22:32:42,178   rep_loss = 0.0
2021-08-08 22:32:42,179 ***** Save model *****
2021-08-08 22:32:43,244 Writing example 0 of 9832
2021-08-08 22:32:43,245 *** Example ***
2021-08-08 22:32:43,245 guid: dev_matched-0
2021-08-08 22:32:43,245 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:32:43,245 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:32:43,245 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:32:43,245 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:32:43,245 label: contradiction
2021-08-08 22:32:43,245 label_id: 0
2021-08-08 22:32:47,875 ***** Running mm evaluation *****
2021-08-08 22:32:47,875   Num examples = 9832
2021-08-08 22:32:47,875   Batch size = 32
2021-08-08 22:33:00,375 ***** Eval results *****
2021-08-08 22:33:00,375   acc = 0.7483726606997559
2021-08-08 22:33:00,375   eval_loss = 0.7548726756464351
2021-08-08 22:33:00,375   global_step = 1799
2021-08-08 22:33:36,876 ***** Running evaluation *****
2021-08-08 22:33:36,876   Epoch = 0 iter 1999 step
2021-08-08 22:33:36,876   Num examples = 9832
2021-08-08 22:33:36,876   Batch size = 32
2021-08-08 22:33:49,466 ***** Eval results *****
2021-08-08 22:33:49,466   acc = 0.725793327908869
2021-08-08 22:33:49,466   att_loss = 0.0
2021-08-08 22:33:49,466   cls_loss = 0.07852501894769935
2021-08-08 22:33:49,466   eval_loss = 0.776931800625541
2021-08-08 22:33:49,466   global_step = 1999
2021-08-08 22:33:49,466   loss = 0.07852501894769935
2021-08-08 22:33:49,466   rep_loss = 0.0
2021-08-08 22:33:49,466 ***** Save model *****
2021-08-08 22:33:50,695 Writing example 0 of 9832
2021-08-08 22:33:50,696 *** Example ***
2021-08-08 22:33:50,696 guid: dev_matched-0
2021-08-08 22:33:50,696 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:33:50,696 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:33:50,696 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:33:50,697 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:33:50,697 label: contradiction
2021-08-08 22:33:50,697 label_id: 0
2021-08-08 22:33:55,398 ***** Running mm evaluation *****
2021-08-08 22:33:55,398   Num examples = 9832
2021-08-08 22:33:55,398   Batch size = 32
2021-08-08 22:34:06,271 ***** Eval results *****
2021-08-08 22:34:06,272   acc = 0.725793327908869
2021-08-08 22:34:06,272   eval_loss = 0.776931800625541
2021-08-08 22:34:06,272   global_step = 1999
2021-08-08 22:34:42,772 ***** Running evaluation *****
2021-08-08 22:34:42,772   Epoch = 0 iter 2199 step
2021-08-08 22:34:42,773   Num examples = 9832
2021-08-08 22:34:42,773   Batch size = 32
2021-08-08 22:34:53,644 ***** Eval results *****
2021-08-08 22:34:53,644   acc = 0.7028071602929211
2021-08-08 22:34:53,644   att_loss = 0.0
2021-08-08 22:34:53,644   cls_loss = 0.07851592540442807
2021-08-08 22:34:53,644   eval_loss = 0.7830038062937847
2021-08-08 22:34:53,644   global_step = 2199
2021-08-08 22:34:53,644   loss = 0.07851592540442807
2021-08-08 22:34:53,645   rep_loss = 0.0
2021-08-08 22:34:53,645 ***** Save model *****
2021-08-08 22:34:55,639 Writing example 0 of 9832
2021-08-08 22:34:55,640 *** Example ***
2021-08-08 22:34:55,640 guid: dev_matched-0
2021-08-08 22:34:55,640 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:34:55,640 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:34:55,640 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:34:55,640 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:34:55,640 label: contradiction
2021-08-08 22:34:55,640 label_id: 0
2021-08-08 22:35:00,238 ***** Running mm evaluation *****
2021-08-08 22:35:00,239   Num examples = 9832
2021-08-08 22:35:00,239   Batch size = 32
2021-08-08 22:35:12,892 ***** Eval results *****
2021-08-08 22:35:12,892   acc = 0.7028071602929211
2021-08-08 22:35:12,892   eval_loss = 0.7830038062937847
2021-08-08 22:35:12,892   global_step = 2199
2021-08-08 22:35:49,430 ***** Running evaluation *****
2021-08-08 22:35:49,430   Epoch = 0 iter 2399 step
2021-08-08 22:35:49,430   Num examples = 9832
2021-08-08 22:35:49,430   Batch size = 32
2021-08-08 22:36:03,015 ***** Eval results *****
2021-08-08 22:36:03,015   acc = 0.6950772986167616
2021-08-08 22:36:03,015   att_loss = 0.0
2021-08-08 22:36:03,016   cls_loss = 0.07850770907582914
2021-08-08 22:36:03,016   eval_loss = 0.7893565235199866
2021-08-08 22:36:03,016   global_step = 2399
2021-08-08 22:36:03,016   loss = 0.07850770907582914
2021-08-08 22:36:03,016   rep_loss = 0.0
2021-08-08 22:36:03,020 ***** Save model *****
2021-08-08 22:36:17,447 Writing example 0 of 9832
2021-08-08 22:36:17,448 *** Example ***
2021-08-08 22:36:17,448 guid: dev_matched-0
2021-08-08 22:36:17,448 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:36:17,448 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:36:17,448 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:36:17,448 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:36:17,448 label: contradiction
2021-08-08 22:36:17,448 label_id: 0
2021-08-08 22:36:22,004 ***** Running mm evaluation *****
2021-08-08 22:36:22,004   Num examples = 9832
2021-08-08 22:36:22,004   Batch size = 32
2021-08-08 22:36:32,880 ***** Eval results *****
2021-08-08 22:36:32,880   acc = 0.6950772986167616
2021-08-08 22:36:32,880   eval_loss = 0.7893565235199866
2021-08-08 22:36:32,881   global_step = 2399
2021-08-08 22:37:11,170 ***** Running evaluation *****
2021-08-08 22:37:11,170   Epoch = 0 iter 2599 step
2021-08-08 22:37:11,170   Num examples = 9832
2021-08-08 22:37:11,170   Batch size = 32
2021-08-08 22:37:22,059 ***** Eval results *****
2021-08-08 22:37:22,059   acc = 0.6914157851912124
2021-08-08 22:37:22,059   att_loss = 0.0
2021-08-08 22:37:22,059   cls_loss = 0.0784997406568652
2021-08-08 22:37:22,059   eval_loss = 0.7961196245311143
2021-08-08 22:37:22,059   global_step = 2599
2021-08-08 22:37:22,059   loss = 0.0784997406568652
2021-08-08 22:37:22,059   rep_loss = 0.0
2021-08-08 22:37:22,059 ***** Save model *****
2021-08-08 22:37:23,984 Writing example 0 of 9832
2021-08-08 22:37:23,985 *** Example ***
2021-08-08 22:37:23,985 guid: dev_matched-0
2021-08-08 22:37:23,985 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:37:23,985 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:37:23,985 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:37:23,985 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:37:23,985 label: contradiction
2021-08-08 22:37:23,985 label_id: 0
2021-08-08 22:37:28,614 ***** Running mm evaluation *****
2021-08-08 22:37:28,614   Num examples = 9832
2021-08-08 22:37:28,614   Batch size = 32
2021-08-08 22:37:39,990 ***** Eval results *****
2021-08-08 22:37:39,990   acc = 0.6914157851912124
2021-08-08 22:37:39,990   eval_loss = 0.7961196245311143
2021-08-08 22:37:39,990   global_step = 2599
2021-08-08 22:38:16,673 ***** Running evaluation *****
2021-08-08 22:38:16,674   Epoch = 0 iter 2799 step
2021-08-08 22:38:16,674   Num examples = 9832
2021-08-08 22:38:16,674   Batch size = 32
2021-08-08 22:38:27,513 ***** Eval results *****
2021-08-08 22:38:27,513   acc = 0.6882628152969894
2021-08-08 22:38:27,513   att_loss = 0.0
2021-08-08 22:38:27,513   cls_loss = 0.07848974457859098
2021-08-08 22:38:27,513   eval_loss = 0.7945416298779574
2021-08-08 22:38:27,513   global_step = 2799
2021-08-08 22:38:27,513   loss = 0.07848974457859098
2021-08-08 22:38:27,513   rep_loss = 0.0
2021-08-08 22:38:27,514 ***** Save model *****
2021-08-08 22:38:35,641 Writing example 0 of 9832
2021-08-08 22:38:35,642 *** Example ***
2021-08-08 22:38:35,642 guid: dev_matched-0
2021-08-08 22:38:35,642 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:38:35,642 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:38:35,642 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:38:35,642 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:38:35,642 label: contradiction
2021-08-08 22:38:35,642 label_id: 0
2021-08-08 22:38:41,945 ***** Running mm evaluation *****
2021-08-08 22:38:41,945   Num examples = 9832
2021-08-08 22:38:41,945   Batch size = 32
2021-08-08 22:38:52,849 ***** Eval results *****
2021-08-08 22:38:52,849   acc = 0.6882628152969894
2021-08-08 22:38:52,849   eval_loss = 0.7945416298779574
2021-08-08 22:38:52,849   global_step = 2799
2021-08-08 22:39:29,383 ***** Running evaluation *****
2021-08-08 22:39:29,384   Epoch = 0 iter 2999 step
2021-08-08 22:39:29,384   Num examples = 9832
2021-08-08 22:39:29,384   Batch size = 32
2021-08-08 22:39:40,241 ***** Eval results *****
2021-08-08 22:39:40,242   acc = 0.6913140764849471
2021-08-08 22:39:40,242   att_loss = 0.0
2021-08-08 22:39:40,242   cls_loss = 0.07848827106857666
2021-08-08 22:39:40,242   eval_loss = 0.797069150712583
2021-08-08 22:39:40,242   global_step = 2999
2021-08-08 22:39:40,242   loss = 0.07848827106857666
2021-08-08 22:39:40,242   rep_loss = 0.0
2021-08-08 22:39:40,242 ***** Save model *****
2021-08-08 22:39:48,561 Writing example 0 of 9832
2021-08-08 22:39:48,562 *** Example ***
2021-08-08 22:39:48,562 guid: dev_matched-0
2021-08-08 22:39:48,562 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:39:48,562 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:39:48,562 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:39:48,562 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:39:48,562 label: contradiction
2021-08-08 22:39:48,562 label_id: 0
2021-08-08 22:39:53,309 ***** Running mm evaluation *****
2021-08-08 22:39:53,309   Num examples = 9832
2021-08-08 22:39:53,309   Batch size = 32
2021-08-08 22:40:05,843 ***** Eval results *****
2021-08-08 22:40:05,844   acc = 0.6913140764849471
2021-08-08 22:40:05,844   eval_loss = 0.797069150712583
2021-08-08 22:40:05,844   global_step = 2999
2021-08-08 22:40:44,084 ***** Running evaluation *****
2021-08-08 22:40:44,085   Epoch = 0 iter 3199 step
2021-08-08 22:40:44,085   Num examples = 9832
2021-08-08 22:40:44,085   Batch size = 32
2021-08-08 22:40:56,481 ***** Eval results *****
2021-08-08 22:40:56,481   acc = 0.6820585842148088
2021-08-08 22:40:56,481   att_loss = 0.0
2021-08-08 22:40:56,481   cls_loss = 0.0784829689906552
2021-08-08 22:40:56,481   eval_loss = 0.8080895078646673
2021-08-08 22:40:56,481   global_step = 3199
2021-08-08 22:40:56,481   loss = 0.0784829689906552
2021-08-08 22:40:56,481   rep_loss = 0.0
2021-08-08 22:40:56,482 ***** Save model *****
2021-08-08 22:41:05,706 Writing example 0 of 9832
2021-08-08 22:41:05,707 *** Example ***
2021-08-08 22:41:05,707 guid: dev_matched-0
2021-08-08 22:41:05,707 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:41:05,707 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:41:05,707 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:41:05,707 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:41:05,707 label: contradiction
2021-08-08 22:41:05,707 label_id: 0
2021-08-08 22:41:10,257 ***** Running mm evaluation *****
2021-08-08 22:41:10,258   Num examples = 9832
2021-08-08 22:41:10,258   Batch size = 32
2021-08-08 22:41:21,142 ***** Eval results *****
2021-08-08 22:41:21,142   acc = 0.6820585842148088
2021-08-08 22:41:21,142   eval_loss = 0.8080895078646673
2021-08-08 22:41:21,143   global_step = 3199
2021-08-08 22:41:59,391 ***** Running evaluation *****
2021-08-08 22:41:59,391   Epoch = 0 iter 3399 step
2021-08-08 22:41:59,391   Num examples = 9832
2021-08-08 22:41:59,391   Batch size = 32
2021-08-08 22:42:10,287 ***** Eval results *****
2021-08-08 22:42:10,287   acc = 0.6592758340113913
2021-08-08 22:42:10,287   att_loss = 0.0
2021-08-08 22:42:10,287   cls_loss = 0.07848051086037465
2021-08-08 22:42:10,287   eval_loss = 0.8128540194653845
2021-08-08 22:42:10,287   global_step = 3399
2021-08-08 22:42:10,287   loss = 0.07848051086037465
2021-08-08 22:42:10,288   rep_loss = 0.0
2021-08-08 22:42:10,288 ***** Save model *****
2021-08-08 22:42:11,529 Writing example 0 of 9832
2021-08-08 22:42:11,529 *** Example ***
2021-08-08 22:42:11,529 guid: dev_matched-0
2021-08-08 22:42:11,529 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:42:11,529 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:42:11,530 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:42:11,530 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:42:11,530 label: contradiction
2021-08-08 22:42:11,530 label_id: 0
2021-08-08 22:42:16,081 ***** Running mm evaluation *****
2021-08-08 22:42:16,081   Num examples = 9832
2021-08-08 22:42:16,081   Batch size = 32
2021-08-08 22:42:26,950 ***** Eval results *****
2021-08-08 22:42:26,950   acc = 0.6592758340113913
2021-08-08 22:42:26,950   eval_loss = 0.8128540194653845
2021-08-08 22:42:26,950   global_step = 3399
2021-08-08 22:43:05,147 ***** Running evaluation *****
2021-08-08 22:43:05,147   Epoch = 0 iter 3599 step
2021-08-08 22:43:05,147   Num examples = 9832
2021-08-08 22:43:05,147   Batch size = 32
2021-08-08 22:43:15,995 ***** Eval results *****
2021-08-08 22:43:15,996   acc = 0.596826688364524
2021-08-08 22:43:15,996   att_loss = 0.0
2021-08-08 22:43:15,996   cls_loss = 0.07847649266435623
2021-08-08 22:43:15,996   eval_loss = 0.8304234054181483
2021-08-08 22:43:15,996   global_step = 3599
2021-08-08 22:43:15,996   loss = 0.07847649266435623
2021-08-08 22:43:15,996   rep_loss = 0.0
2021-08-08 22:43:15,996 ***** Save model *****
2021-08-08 22:43:18,582 Writing example 0 of 9832
2021-08-08 22:43:18,582 *** Example ***
2021-08-08 22:43:18,583 guid: dev_matched-0
2021-08-08 22:43:18,583 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:43:18,583 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:43:18,583 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:43:18,583 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:43:18,583 label: contradiction
2021-08-08 22:43:18,583 label_id: 0
2021-08-08 22:43:23,246 ***** Running mm evaluation *****
2021-08-08 22:43:23,246   Num examples = 9832
2021-08-08 22:43:23,246   Batch size = 32
2021-08-08 22:43:34,158 ***** Eval results *****
2021-08-08 22:43:34,158   acc = 0.596826688364524
2021-08-08 22:43:34,158   eval_loss = 0.8304234054181483
2021-08-08 22:43:34,158   global_step = 3599
2021-08-08 22:44:12,419 ***** Running evaluation *****
2021-08-08 22:44:12,420   Epoch = 0 iter 3799 step
2021-08-08 22:44:12,420   Num examples = 9832
2021-08-08 22:44:12,420   Batch size = 32
2021-08-08 22:44:23,309 ***** Eval results *****
2021-08-08 22:44:23,309   acc = 0.5764849471114727
2021-08-08 22:44:23,309   att_loss = 0.0
2021-08-08 22:44:23,309   cls_loss = 0.07846989296916033
2021-08-08 22:44:23,309   eval_loss = 0.8395722924501865
2021-08-08 22:44:23,309   global_step = 3799
2021-08-08 22:44:23,310   loss = 0.07846989296916033
2021-08-08 22:44:23,310   rep_loss = 0.0
2021-08-08 22:44:23,310 ***** Save model *****
2021-08-08 22:44:24,650 Writing example 0 of 9832
2021-08-08 22:44:24,651 *** Example ***
2021-08-08 22:44:24,651 guid: dev_matched-0
2021-08-08 22:44:24,651 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:44:24,651 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:44:24,651 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:44:24,651 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:44:24,651 label: contradiction
2021-08-08 22:44:24,651 label_id: 0
2021-08-08 22:44:29,315 ***** Running mm evaluation *****
2021-08-08 22:44:29,315   Num examples = 9832
2021-08-08 22:44:29,315   Batch size = 32
2021-08-08 22:44:40,202 ***** Eval results *****
2021-08-08 22:44:40,202   acc = 0.5764849471114727
2021-08-08 22:44:40,202   eval_loss = 0.8395722924501865
2021-08-08 22:44:40,202   global_step = 3799
2021-08-08 22:45:18,358 ***** Running evaluation *****
2021-08-08 22:45:18,359   Epoch = 0 iter 3999 step
2021-08-08 22:45:18,359   Num examples = 9832
2021-08-08 22:45:18,359   Batch size = 32
2021-08-08 22:45:29,196 ***** Eval results *****
2021-08-08 22:45:29,197   acc = 0.6319161920260374
2021-08-08 22:45:29,197   att_loss = 0.0
2021-08-08 22:45:29,197   cls_loss = 0.07846709238339675
2021-08-08 22:45:29,197   eval_loss = 0.8205796762720331
2021-08-08 22:45:29,197   global_step = 3999
2021-08-08 22:45:29,197   loss = 0.07846709238339675
2021-08-08 22:45:29,197   rep_loss = 0.0
2021-08-08 22:45:29,197 ***** Save model *****
2021-08-08 22:45:32,868 Writing example 0 of 9832
2021-08-08 22:45:32,869 *** Example ***
2021-08-08 22:45:32,869 guid: dev_matched-0
2021-08-08 22:45:32,869 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:45:32,869 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:45:32,869 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:45:32,869 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:45:32,869 label: contradiction
2021-08-08 22:45:32,869 label_id: 0
2021-08-08 22:45:37,422 ***** Running mm evaluation *****
2021-08-08 22:45:37,422   Num examples = 9832
2021-08-08 22:45:37,423   Batch size = 32
2021-08-08 22:45:49,857 ***** Eval results *****
2021-08-08 22:45:49,857   acc = 0.6319161920260374
2021-08-08 22:45:49,857   eval_loss = 0.8205796762720331
2021-08-08 22:45:49,857   global_step = 3999
2021-08-08 22:46:27,998 ***** Running evaluation *****
2021-08-08 22:46:27,998   Epoch = 0 iter 4199 step
2021-08-08 22:46:27,998   Num examples = 9832
2021-08-08 22:46:27,999   Batch size = 32
2021-08-08 22:46:38,838 ***** Eval results *****
2021-08-08 22:46:38,838   acc = 0.6203213995117982
2021-08-08 22:46:38,839   att_loss = 0.0
2021-08-08 22:46:38,839   cls_loss = 0.07846341946453501
2021-08-08 22:46:38,839   eval_loss = 0.8211094315175886
2021-08-08 22:46:38,839   global_step = 4199
2021-08-08 22:46:38,839   loss = 0.07846341946453501
2021-08-08 22:46:38,839   rep_loss = 0.0
2021-08-08 22:46:38,839 ***** Save model *****
2021-08-08 22:46:39,758 Writing example 0 of 9832
2021-08-08 22:46:39,759 *** Example ***
2021-08-08 22:46:39,759 guid: dev_matched-0
2021-08-08 22:46:39,759 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:46:39,759 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:46:39,759 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:46:39,760 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:46:39,760 label: contradiction
2021-08-08 22:46:39,760 label_id: 0
2021-08-08 22:46:44,414 ***** Running mm evaluation *****
2021-08-08 22:46:44,414   Num examples = 9832
2021-08-08 22:46:44,414   Batch size = 32
2021-08-08 22:46:55,251 ***** Eval results *****
2021-08-08 22:46:55,251   acc = 0.6203213995117982
2021-08-08 22:46:55,251   eval_loss = 0.8211094315175886
2021-08-08 22:46:55,251   global_step = 4199
2021-08-08 22:47:33,476 ***** Running evaluation *****
2021-08-08 22:47:33,476   Epoch = 0 iter 4399 step
2021-08-08 22:47:33,476   Num examples = 9832
2021-08-08 22:47:33,476   Batch size = 32
2021-08-08 22:47:44,367 ***** Eval results *****
2021-08-08 22:47:44,367   acc = 0.621847030105777
2021-08-08 22:47:44,367   att_loss = 0.0
2021-08-08 22:47:44,367   cls_loss = 0.07846003729332629
2021-08-08 22:47:44,367   eval_loss = 0.8275591912981751
2021-08-08 22:47:44,367   global_step = 4399
2021-08-08 22:47:44,367   loss = 0.07846003729332629
2021-08-08 22:47:44,367   rep_loss = 0.0
2021-08-08 22:47:44,367 ***** Save model *****
2021-08-08 22:47:45,213 Writing example 0 of 9832
2021-08-08 22:47:45,214 *** Example ***
2021-08-08 22:47:45,214 guid: dev_matched-0
2021-08-08 22:47:45,214 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:47:45,214 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:47:45,214 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:47:45,214 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:47:45,214 label: contradiction
2021-08-08 22:47:45,214 label_id: 0
2021-08-08 22:47:49,811 ***** Running mm evaluation *****
2021-08-08 22:47:49,811   Num examples = 9832
2021-08-08 22:47:49,811   Batch size = 32
2021-08-08 22:48:00,706 ***** Eval results *****
2021-08-08 22:48:00,706   acc = 0.621847030105777
2021-08-08 22:48:00,706   eval_loss = 0.8275591912981751
2021-08-08 22:48:00,707   global_step = 4399
2021-08-08 22:48:37,229 ***** Running evaluation *****
2021-08-08 22:48:37,229   Epoch = 0 iter 4599 step
2021-08-08 22:48:37,230   Num examples = 9832
2021-08-08 22:48:37,230   Batch size = 32
2021-08-08 22:48:49,693 ***** Eval results *****
2021-08-08 22:48:49,693   acc = 0.637713588283157
2021-08-08 22:48:49,693   att_loss = 0.0
2021-08-08 22:48:49,693   cls_loss = 0.07845580040319869
2021-08-08 22:48:49,693   eval_loss = 0.8226690230431495
2021-08-08 22:48:49,693   global_step = 4599
2021-08-08 22:48:49,693   loss = 0.07845580040319869
2021-08-08 22:48:49,693   rep_loss = 0.0
2021-08-08 22:48:49,693 ***** Save model *****
2021-08-08 22:48:53,143 Writing example 0 of 9832
2021-08-08 22:48:53,144 *** Example ***
2021-08-08 22:48:53,144 guid: dev_matched-0
2021-08-08 22:48:53,144 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:48:53,144 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:48:53,144 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:48:53,144 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:48:53,144 label: contradiction
2021-08-08 22:48:53,144 label_id: 0
2021-08-08 22:48:57,786 ***** Running mm evaluation *****
2021-08-08 22:48:57,787   Num examples = 9832
2021-08-08 22:48:57,787   Batch size = 32
2021-08-08 22:49:08,632 ***** Eval results *****
2021-08-08 22:49:08,632   acc = 0.637713588283157
2021-08-08 22:49:08,632   eval_loss = 0.8226690230431495
2021-08-08 22:49:08,632   global_step = 4599
2021-08-08 22:49:46,792 ***** Running evaluation *****
2021-08-08 22:49:46,792   Epoch = 0 iter 4799 step
2021-08-08 22:49:46,792   Num examples = 9832
2021-08-08 22:49:46,792   Batch size = 32
2021-08-08 22:49:57,629 ***** Eval results *****
2021-08-08 22:49:57,629   acc = 0.5814686737184703
2021-08-08 22:49:57,629   att_loss = 0.0
2021-08-08 22:49:57,629   cls_loss = 0.07845226063731313
2021-08-08 22:49:57,629   eval_loss = 0.841418583671768
2021-08-08 22:49:57,629   global_step = 4799
2021-08-08 22:49:57,629   loss = 0.07845226063731313
2021-08-08 22:49:57,629   rep_loss = 0.0
2021-08-08 22:49:57,630 ***** Save model *****
2021-08-08 22:49:58,433 Writing example 0 of 9832
2021-08-08 22:49:58,433 *** Example ***
2021-08-08 22:49:58,434 guid: dev_matched-0
2021-08-08 22:49:58,434 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:49:58,434 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:49:58,434 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:49:58,434 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:49:58,434 label: contradiction
2021-08-08 22:49:58,434 label_id: 0
2021-08-08 22:50:03,064 ***** Running mm evaluation *****
2021-08-08 22:50:03,065   Num examples = 9832
2021-08-08 22:50:03,065   Batch size = 32
2021-08-08 22:50:14,029 ***** Eval results *****
2021-08-08 22:50:14,029   acc = 0.5814686737184703
2021-08-08 22:50:14,029   eval_loss = 0.841418583671768
2021-08-08 22:50:14,029   global_step = 4799
2021-08-08 22:50:52,248 ***** Running evaluation *****
2021-08-08 22:50:52,249   Epoch = 0 iter 4999 step
2021-08-08 22:50:52,249   Num examples = 9832
2021-08-08 22:50:52,249   Batch size = 32
2021-08-08 22:51:03,111 ***** Eval results *****
2021-08-08 22:51:03,111   acc = 0.6192026037428804
2021-08-08 22:51:03,111   att_loss = 0.0
2021-08-08 22:51:03,111   cls_loss = 0.07844818952346377
2021-08-08 22:51:03,111   eval_loss = 0.8302651672781288
2021-08-08 22:51:03,111   global_step = 4999
2021-08-08 22:51:03,111   loss = 0.07844818952346377
2021-08-08 22:51:03,112   rep_loss = 0.0
2021-08-08 22:51:03,112 ***** Save model *****
2021-08-08 22:51:04,100 Writing example 0 of 9832
2021-08-08 22:51:04,101 *** Example ***
2021-08-08 22:51:04,101 guid: dev_matched-0
2021-08-08 22:51:04,101 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:51:04,101 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:51:04,101 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:51:04,101 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:51:04,101 label: contradiction
2021-08-08 22:51:04,101 label_id: 0
2021-08-08 22:51:08,671 ***** Running mm evaluation *****
2021-08-08 22:51:08,671   Num examples = 9832
2021-08-08 22:51:08,671   Batch size = 32
2021-08-08 22:51:19,565 ***** Eval results *****
2021-08-08 22:51:19,566   acc = 0.6192026037428804
2021-08-08 22:51:19,566   eval_loss = 0.8302651672781288
2021-08-08 22:51:19,566   global_step = 4999
2021-08-08 22:51:57,747 ***** Running evaluation *****
2021-08-08 22:51:57,748   Epoch = 0 iter 5199 step
2021-08-08 22:51:57,748   Num examples = 9832
2021-08-08 22:51:57,748   Batch size = 32
2021-08-08 22:52:08,640 ***** Eval results *****
2021-08-08 22:52:08,640   acc = 0.6419853539462979
2021-08-08 22:52:08,640   att_loss = 0.0
2021-08-08 22:52:08,640   cls_loss = 0.07844536955722549
2021-08-08 22:52:08,640   eval_loss = 0.8274639841023977
2021-08-08 22:52:08,640   global_step = 5199
2021-08-08 22:52:08,640   loss = 0.07844536955722549
2021-08-08 22:52:08,640   rep_loss = 0.0
2021-08-08 22:52:08,640 ***** Save model *****
2021-08-08 22:52:10,826 Writing example 0 of 9832
2021-08-08 22:52:10,826 *** Example ***
2021-08-08 22:52:10,826 guid: dev_matched-0
2021-08-08 22:52:10,826 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:52:10,827 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:52:10,827 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:52:10,827 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:52:10,827 label: contradiction
2021-08-08 22:52:10,827 label_id: 0
2021-08-08 22:52:15,390 ***** Running mm evaluation *****
2021-08-08 22:52:15,390   Num examples = 9832
2021-08-08 22:52:15,390   Batch size = 32
2021-08-08 22:52:26,258 ***** Eval results *****
2021-08-08 22:52:26,258   acc = 0.6419853539462979
2021-08-08 22:52:26,258   eval_loss = 0.8274639841023977
2021-08-08 22:52:26,258   global_step = 5199
2021-08-08 22:53:06,120 ***** Running evaluation *****
2021-08-08 22:53:06,121   Epoch = 0 iter 5399 step
2021-08-08 22:53:06,121   Num examples = 9832
2021-08-08 22:53:06,121   Batch size = 32
2021-08-08 22:53:17,004 ***** Eval results *****
2021-08-08 22:53:17,004   acc = 0.5860455655004069
2021-08-08 22:53:17,004   att_loss = 0.0
2021-08-08 22:53:17,004   cls_loss = 0.07844285357686231
2021-08-08 22:53:17,004   eval_loss = 0.8385524490436951
2021-08-08 22:53:17,004   global_step = 5399
2021-08-08 22:53:17,005   loss = 0.07844285357686231
2021-08-08 22:53:17,005   rep_loss = 0.0
2021-08-08 22:53:17,005 ***** Save model *****
2021-08-08 22:53:18,138 Writing example 0 of 9832
2021-08-08 22:53:18,139 *** Example ***
2021-08-08 22:53:18,139 guid: dev_matched-0
2021-08-08 22:53:18,139 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:53:18,139 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:53:18,139 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:53:18,139 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:53:18,139 label: contradiction
2021-08-08 22:53:18,139 label_id: 0
2021-08-08 22:53:22,841 ***** Running mm evaluation *****
2021-08-08 22:53:22,841   Num examples = 9832
2021-08-08 22:53:22,841   Batch size = 32
2021-08-08 22:53:35,331 ***** Eval results *****
2021-08-08 22:53:35,332   acc = 0.5860455655004069
2021-08-08 22:53:35,332   eval_loss = 0.8385524490436951
2021-08-08 22:53:35,332   global_step = 5399
2021-08-08 22:54:13,586 ***** Running evaluation *****
2021-08-08 22:54:13,587   Epoch = 0 iter 5599 step
2021-08-08 22:54:13,587   Num examples = 9832
2021-08-08 22:54:13,587   Batch size = 32
2021-08-08 22:54:24,425 ***** Eval results *****
2021-08-08 22:54:24,425   acc = 0.5864524003254679
2021-08-08 22:54:24,425   att_loss = 0.0
2021-08-08 22:54:24,425   cls_loss = 0.07844061053694519
2021-08-08 22:54:24,425   eval_loss = 0.8399437015706842
2021-08-08 22:54:24,425   global_step = 5599
2021-08-08 22:54:24,425   loss = 0.07844061053694519
2021-08-08 22:54:24,425   rep_loss = 0.0
2021-08-08 22:54:24,425 ***** Save model *****
2021-08-08 22:54:26,591 Writing example 0 of 9832
2021-08-08 22:54:26,591 *** Example ***
2021-08-08 22:54:26,591 guid: dev_matched-0
2021-08-08 22:54:26,591 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:54:26,591 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:54:26,592 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:54:26,592 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:54:26,592 label: contradiction
2021-08-08 22:54:26,592 label_id: 0
2021-08-08 22:54:31,230 ***** Running mm evaluation *****
2021-08-08 22:54:31,230   Num examples = 9832
2021-08-08 22:54:31,230   Batch size = 32
2021-08-08 22:54:42,154 ***** Eval results *****
2021-08-08 22:54:42,154   acc = 0.5864524003254679
2021-08-08 22:54:42,154   eval_loss = 0.8399437015706842
2021-08-08 22:54:42,155   global_step = 5599
2021-08-08 22:55:20,378 ***** Running evaluation *****
2021-08-08 22:55:20,379   Epoch = 0 iter 5799 step
2021-08-08 22:55:20,379   Num examples = 9832
2021-08-08 22:55:20,379   Batch size = 32
2021-08-08 22:55:31,236 ***** Eval results *****
2021-08-08 22:55:31,236   acc = 0.5719080553295363
2021-08-08 22:55:31,236   att_loss = 0.0
2021-08-08 22:55:31,236   cls_loss = 0.07843767087389465
2021-08-08 22:55:31,236   eval_loss = 0.845243130217899
2021-08-08 22:55:31,236   global_step = 5799
2021-08-08 22:55:31,236   loss = 0.07843767087389465
2021-08-08 22:55:31,236   rep_loss = 0.0
2021-08-08 22:55:31,236 ***** Save model *****
2021-08-08 22:55:59,865 Writing example 0 of 9832
2021-08-08 22:55:59,865 *** Example ***
2021-08-08 22:55:59,865 guid: dev_matched-0
2021-08-08 22:55:59,865 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:55:59,866 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:55:59,866 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:55:59,866 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:55:59,866 label: contradiction
2021-08-08 22:55:59,866 label_id: 0
2021-08-08 22:56:04,447 ***** Running mm evaluation *****
2021-08-08 22:56:04,447   Num examples = 9832
2021-08-08 22:56:04,447   Batch size = 32
2021-08-08 22:56:15,336 ***** Eval results *****
2021-08-08 22:56:15,336   acc = 0.5719080553295363
2021-08-08 22:56:15,336   eval_loss = 0.845243130217899
2021-08-08 22:56:15,336   global_step = 5799
2021-08-08 22:56:53,579 ***** Running evaluation *****
2021-08-08 22:56:53,579   Epoch = 0 iter 5999 step
2021-08-08 22:56:53,579   Num examples = 9832
2021-08-08 22:56:53,579   Batch size = 32
2021-08-08 22:57:04,415 ***** Eval results *****
2021-08-08 22:57:04,415   acc = 0.5886899918633035
2021-08-08 22:57:04,415   att_loss = 0.0
2021-08-08 22:57:04,415   cls_loss = 0.07843251100732478
2021-08-08 22:57:04,415   eval_loss = 0.8450847022332154
2021-08-08 22:57:04,415   global_step = 5999
2021-08-08 22:57:04,416   loss = 0.07843251100732478
2021-08-08 22:57:04,416   rep_loss = 0.0
2021-08-08 22:57:04,416 ***** Save model *****
2021-08-08 22:57:05,698 Writing example 0 of 9832
2021-08-08 22:57:05,698 *** Example ***
2021-08-08 22:57:05,698 guid: dev_matched-0
2021-08-08 22:57:05,698 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:57:05,698 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:57:05,698 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:57:05,699 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:57:05,699 label: contradiction
2021-08-08 22:57:05,699 label_id: 0
2021-08-08 22:57:10,388 ***** Running mm evaluation *****
2021-08-08 22:57:10,388   Num examples = 9832
2021-08-08 22:57:10,388   Batch size = 32
2021-08-08 22:57:21,266 ***** Eval results *****
2021-08-08 22:57:21,266   acc = 0.5886899918633035
2021-08-08 22:57:21,266   eval_loss = 0.8450847022332154
2021-08-08 22:57:21,266   global_step = 5999
2021-08-08 22:57:59,568 ***** Running evaluation *****
2021-08-08 22:57:59,568   Epoch = 0 iter 6199 step
2021-08-08 22:57:59,568   Num examples = 9832
2021-08-08 22:57:59,568   Batch size = 32
2021-08-08 22:58:10,544 ***** Eval results *****
2021-08-08 22:58:10,544   acc = 0.5810618388934092
2021-08-08 22:58:10,545   att_loss = 0.0
2021-08-08 22:58:10,545   cls_loss = 0.07843104250648557
2021-08-08 22:58:10,545   eval_loss = 0.851701492225969
2021-08-08 22:58:10,545   global_step = 6199
2021-08-08 22:58:10,545   loss = 0.07843104250648557
2021-08-08 22:58:10,545   rep_loss = 0.0
2021-08-08 22:58:10,545 ***** Save model *****
2021-08-08 22:58:18,563 Writing example 0 of 9832
2021-08-08 22:58:18,564 *** Example ***
2021-08-08 22:58:18,564 guid: dev_matched-0
2021-08-08 22:58:18,564 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:58:18,564 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:58:18,564 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:58:18,564 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:58:18,564 label: contradiction
2021-08-08 22:58:18,564 label_id: 0
2021-08-08 22:58:23,126 ***** Running mm evaluation *****
2021-08-08 22:58:23,127   Num examples = 9832
2021-08-08 22:58:23,127   Batch size = 32
2021-08-08 22:58:33,983 ***** Eval results *****
2021-08-08 22:58:33,983   acc = 0.5810618388934092
2021-08-08 22:58:33,983   eval_loss = 0.851701492225969
2021-08-08 22:58:33,984   global_step = 6199
2021-08-08 22:59:12,182 ***** Running evaluation *****
2021-08-08 22:59:12,183   Epoch = 0 iter 6399 step
2021-08-08 22:59:12,183   Num examples = 9832
2021-08-08 22:59:12,183   Batch size = 32
2021-08-08 22:59:23,037 ***** Eval results *****
2021-08-08 22:59:23,037   acc = 0.5390561432058584
2021-08-08 22:59:23,037   att_loss = 0.0
2021-08-08 22:59:23,037   cls_loss = 0.07842967654694094
2021-08-08 22:59:23,037   eval_loss = 0.8598929728780474
2021-08-08 22:59:23,037   global_step = 6399
2021-08-08 22:59:23,037   loss = 0.07842967654694094
2021-08-08 22:59:23,037   rep_loss = 0.0
2021-08-08 22:59:23,037 ***** Save model *****
2021-08-08 22:59:24,123 Writing example 0 of 9832
2021-08-08 22:59:24,124 *** Example ***
2021-08-08 22:59:24,124 guid: dev_matched-0
2021-08-08 22:59:24,124 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 22:59:24,124 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:59:24,124 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:59:24,124 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 22:59:24,124 label: contradiction
2021-08-08 22:59:24,124 label_id: 0
2021-08-08 22:59:28,776 ***** Running mm evaluation *****
2021-08-08 22:59:28,776   Num examples = 9832
2021-08-08 22:59:28,776   Batch size = 32
2021-08-08 22:59:39,661 ***** Eval results *****
2021-08-08 22:59:39,661   acc = 0.5390561432058584
2021-08-08 22:59:39,661   eval_loss = 0.8598929728780474
2021-08-08 22:59:39,661   global_step = 6399
2021-08-08 23:00:17,982 ***** Running evaluation *****
2021-08-08 23:00:17,982   Epoch = 0 iter 6599 step
2021-08-08 23:00:17,982   Num examples = 9832
2021-08-08 23:00:17,982   Batch size = 32
2021-08-08 23:00:28,860 ***** Eval results *****
2021-08-08 23:00:28,860   acc = 0.5253254678600489
2021-08-08 23:00:28,860   att_loss = 0.0
2021-08-08 23:00:28,860   cls_loss = 0.0784270841854687
2021-08-08 23:00:28,860   eval_loss = 0.8641409258563797
2021-08-08 23:00:28,860   global_step = 6599
2021-08-08 23:00:28,860   loss = 0.0784270841854687
2021-08-08 23:00:28,860   rep_loss = 0.0
2021-08-08 23:00:28,861 ***** Save model *****
2021-08-08 23:00:30,129 Writing example 0 of 9832
2021-08-08 23:00:30,130 *** Example ***
2021-08-08 23:00:30,130 guid: dev_matched-0
2021-08-08 23:00:30,130 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:00:30,130 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:00:30,130 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:00:30,130 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:00:30,130 label: contradiction
2021-08-08 23:00:30,130 label_id: 0
2021-08-08 23:00:34,842 ***** Running mm evaluation *****
2021-08-08 23:00:34,842   Num examples = 9832
2021-08-08 23:00:34,842   Batch size = 32
2021-08-08 23:00:45,721 ***** Eval results *****
2021-08-08 23:00:45,721   acc = 0.5253254678600489
2021-08-08 23:00:45,721   eval_loss = 0.8641409258563797
2021-08-08 23:00:45,721   global_step = 6599
2021-08-08 23:01:23,842 ***** Running evaluation *****
2021-08-08 23:01:23,843   Epoch = 0 iter 6799 step
2021-08-08 23:01:23,843   Num examples = 9832
2021-08-08 23:01:23,843   Batch size = 32
2021-08-08 23:01:34,671 ***** Eval results *****
2021-08-08 23:01:34,671   acc = 0.5251220504475184
2021-08-08 23:01:34,671   att_loss = 0.0
2021-08-08 23:01:34,671   cls_loss = 0.07842419747000741
2021-08-08 23:01:34,671   eval_loss = 0.8631303950950697
2021-08-08 23:01:34,671   global_step = 6799
2021-08-08 23:01:34,671   loss = 0.07842419747000741
2021-08-08 23:01:34,671   rep_loss = 0.0
2021-08-08 23:01:34,672 ***** Save model *****
2021-08-08 23:01:35,720 Writing example 0 of 9832
2021-08-08 23:01:35,721 *** Example ***
2021-08-08 23:01:35,721 guid: dev_matched-0
2021-08-08 23:01:35,721 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:01:35,721 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:01:35,721 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:01:35,721 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:01:35,721 label: contradiction
2021-08-08 23:01:35,721 label_id: 0
2021-08-08 23:01:40,275 ***** Running mm evaluation *****
2021-08-08 23:01:40,275   Num examples = 9832
2021-08-08 23:01:40,275   Batch size = 32
2021-08-08 23:01:51,141 ***** Eval results *****
2021-08-08 23:01:51,142   acc = 0.5251220504475184
2021-08-08 23:01:51,142   eval_loss = 0.8631303950950697
2021-08-08 23:01:51,142   global_step = 6799
2021-08-08 23:02:29,288 ***** Running evaluation *****
2021-08-08 23:02:29,289   Epoch = 0 iter 6999 step
2021-08-08 23:02:29,289   Num examples = 9832
2021-08-08 23:02:29,289   Batch size = 32
2021-08-08 23:02:40,132 ***** Eval results *****
2021-08-08 23:02:40,132   acc = 0.5254271765663141
2021-08-08 23:02:40,132   att_loss = 0.0
2021-08-08 23:02:40,132   cls_loss = 0.07842190978948278
2021-08-08 23:02:40,132   eval_loss = 0.8675502759295625
2021-08-08 23:02:40,132   global_step = 6999
2021-08-08 23:02:40,132   loss = 0.07842190978948278
2021-08-08 23:02:40,132   rep_loss = 0.0
2021-08-08 23:02:40,133 ***** Save model *****
2021-08-08 23:02:41,308 Writing example 0 of 9832
2021-08-08 23:02:41,309 *** Example ***
2021-08-08 23:02:41,309 guid: dev_matched-0
2021-08-08 23:02:41,309 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:02:41,309 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:02:41,309 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:02:41,309 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:02:41,309 label: contradiction
2021-08-08 23:02:41,309 label_id: 0
2021-08-08 23:02:45,933 ***** Running mm evaluation *****
2021-08-08 23:02:45,933   Num examples = 9832
2021-08-08 23:02:45,933   Batch size = 32
2021-08-08 23:02:56,785 ***** Eval results *****
2021-08-08 23:02:56,785   acc = 0.5254271765663141
2021-08-08 23:02:56,785   eval_loss = 0.8675502759295625
2021-08-08 23:02:56,785   global_step = 6999
2021-08-08 23:03:35,169 ***** Running evaluation *****
2021-08-08 23:03:35,170   Epoch = 0 iter 7199 step
2021-08-08 23:03:35,170   Num examples = 9832
2021-08-08 23:03:35,170   Batch size = 32
2021-08-08 23:03:45,987 ***** Eval results *****
2021-08-08 23:03:45,987   acc = 0.5154597233523189
2021-08-08 23:03:45,987   att_loss = 0.0
2021-08-08 23:03:45,987   cls_loss = 0.07842148930260764
2021-08-08 23:03:45,987   eval_loss = 0.8691897831566922
2021-08-08 23:03:45,987   global_step = 7199
2021-08-08 23:03:45,987   loss = 0.07842148930260764
2021-08-08 23:03:45,987   rep_loss = 0.0
2021-08-08 23:03:45,988 ***** Save model *****
2021-08-08 23:03:47,011 Writing example 0 of 9832
2021-08-08 23:03:47,012 *** Example ***
2021-08-08 23:03:47,012 guid: dev_matched-0
2021-08-08 23:03:47,012 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:03:47,012 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:03:47,012 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:03:47,012 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:03:47,012 label: contradiction
2021-08-08 23:03:47,012 label_id: 0
2021-08-08 23:03:53,130 ***** Running mm evaluation *****
2021-08-08 23:03:53,130   Num examples = 9832
2021-08-08 23:03:53,130   Batch size = 32
2021-08-08 23:04:04,015 ***** Eval results *****
2021-08-08 23:04:04,015   acc = 0.5154597233523189
2021-08-08 23:04:04,015   eval_loss = 0.8691897831566922
2021-08-08 23:04:04,015   global_step = 7199
2021-08-08 23:04:42,305 ***** Running evaluation *****
2021-08-08 23:04:42,306   Epoch = 0 iter 7399 step
2021-08-08 23:04:42,306   Num examples = 9832
2021-08-08 23:04:42,306   Batch size = 32
2021-08-08 23:04:53,162 ***** Eval results *****
2021-08-08 23:04:53,162   acc = 0.5021358828315704
2021-08-08 23:04:53,162   att_loss = 0.0
2021-08-08 23:04:53,162   cls_loss = 0.07841847442641324
2021-08-08 23:04:53,162   eval_loss = 0.8743806020780043
2021-08-08 23:04:53,162   global_step = 7399
2021-08-08 23:04:53,162   loss = 0.07841847442641324
2021-08-08 23:04:53,162   rep_loss = 0.0
2021-08-08 23:04:53,162 ***** Save model *****
2021-08-08 23:04:55,326 Writing example 0 of 9832
2021-08-08 23:04:55,327 *** Example ***
2021-08-08 23:04:55,327 guid: dev_matched-0
2021-08-08 23:04:55,327 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:04:55,327 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:04:55,327 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:04:55,327 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:04:55,327 label: contradiction
2021-08-08 23:04:55,327 label_id: 0
2021-08-08 23:05:00,022 ***** Running mm evaluation *****
2021-08-08 23:05:00,022   Num examples = 9832
2021-08-08 23:05:00,022   Batch size = 32
2021-08-08 23:05:10,892 ***** Eval results *****
2021-08-08 23:05:10,893   acc = 0.5021358828315704
2021-08-08 23:05:10,893   eval_loss = 0.8743806020780043
2021-08-08 23:05:10,893   global_step = 7399
2021-08-08 23:05:47,376 ***** Running evaluation *****
2021-08-08 23:05:47,376   Epoch = 0 iter 7599 step
2021-08-08 23:05:47,376   Num examples = 9832
2021-08-08 23:05:47,376   Batch size = 32
2021-08-08 23:05:58,203 ***** Eval results *****
2021-08-08 23:05:58,203   acc = 0.5386493083807974
2021-08-08 23:05:58,203   att_loss = 0.0
2021-08-08 23:05:58,203   cls_loss = 0.07841566888117166
2021-08-08 23:05:58,203   eval_loss = 0.8604978481670479
2021-08-08 23:05:58,203   global_step = 7599
2021-08-08 23:05:58,203   loss = 0.07841566888117166
2021-08-08 23:05:58,203   rep_loss = 0.0
2021-08-08 23:05:58,204 ***** Save model *****
2021-08-08 23:05:59,034 Writing example 0 of 9832
2021-08-08 23:05:59,034 *** Example ***
2021-08-08 23:05:59,034 guid: dev_matched-0
2021-08-08 23:05:59,035 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:05:59,035 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:05:59,035 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:05:59,035 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:05:59,035 label: contradiction
2021-08-08 23:05:59,035 label_id: 0
2021-08-08 23:06:03,607 ***** Running mm evaluation *****
2021-08-08 23:06:03,608   Num examples = 9832
2021-08-08 23:06:03,608   Batch size = 32
2021-08-08 23:06:16,079 ***** Eval results *****
2021-08-08 23:06:16,079   acc = 0.5386493083807974
2021-08-08 23:06:16,079   eval_loss = 0.8604978481670479
2021-08-08 23:06:16,080   global_step = 7599
2021-08-08 23:06:52,555 ***** Running evaluation *****
2021-08-08 23:06:52,555   Epoch = 0 iter 7799 step
2021-08-08 23:06:52,555   Num examples = 9832
2021-08-08 23:06:52,555   Batch size = 32
2021-08-08 23:07:03,386 ***** Eval results *****
2021-08-08 23:07:03,386   acc = 0.5126118795768918
2021-08-08 23:07:03,386   att_loss = 0.0
2021-08-08 23:07:03,386   cls_loss = 0.07841382051587242
2021-08-08 23:07:03,386   eval_loss = 0.8695724763266452
2021-08-08 23:07:03,386   global_step = 7799
2021-08-08 23:07:03,386   loss = 0.07841382051587242
2021-08-08 23:07:03,386   rep_loss = 0.0
2021-08-08 23:07:03,387 ***** Save model *****
2021-08-08 23:07:05,301 Writing example 0 of 9832
2021-08-08 23:07:05,302 *** Example ***
2021-08-08 23:07:05,302 guid: dev_matched-0
2021-08-08 23:07:05,302 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:07:05,302 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:07:05,302 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:07:05,302 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:07:05,302 label: contradiction
2021-08-08 23:07:05,302 label_id: 0
2021-08-08 23:07:09,910 ***** Running mm evaluation *****
2021-08-08 23:07:09,910   Num examples = 9832
2021-08-08 23:07:09,910   Batch size = 32
2021-08-08 23:07:22,342 ***** Eval results *****
2021-08-08 23:07:22,342   acc = 0.5126118795768918
2021-08-08 23:07:22,342   eval_loss = 0.8695724763266452
2021-08-08 23:07:22,342   global_step = 7799
2021-08-08 23:07:58,817 ***** Running evaluation *****
2021-08-08 23:07:58,817   Epoch = 0 iter 7999 step
2021-08-08 23:07:58,817   Num examples = 9832
2021-08-08 23:07:58,817   Batch size = 32
2021-08-08 23:08:11,322 ***** Eval results *****
2021-08-08 23:08:11,322   acc = 0.5096623270951993
2021-08-08 23:08:11,322   att_loss = 0.0
2021-08-08 23:08:11,323   cls_loss = 0.07841339936415424
2021-08-08 23:08:11,323   eval_loss = 0.8703260959742906
2021-08-08 23:08:11,323   global_step = 7999
2021-08-08 23:08:11,323   loss = 0.07841339936415424
2021-08-08 23:08:11,323   rep_loss = 0.0
2021-08-08 23:08:11,323 ***** Save model *****
2021-08-08 23:08:16,352 Writing example 0 of 9832
2021-08-08 23:08:16,353 *** Example ***
2021-08-08 23:08:16,353 guid: dev_matched-0
2021-08-08 23:08:16,353 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:08:16,353 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:08:16,353 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:08:16,353 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:08:16,354 label: contradiction
2021-08-08 23:08:16,354 label_id: 0
2021-08-08 23:08:20,909 ***** Running mm evaluation *****
2021-08-08 23:08:20,910   Num examples = 9832
2021-08-08 23:08:20,910   Batch size = 32
2021-08-08 23:08:31,787 ***** Eval results *****
2021-08-08 23:08:31,787   acc = 0.5096623270951993
2021-08-08 23:08:31,787   eval_loss = 0.8703260959742906
2021-08-08 23:08:31,787   global_step = 7999
2021-08-08 23:09:08,305 ***** Running evaluation *****
2021-08-08 23:09:08,305   Epoch = 0 iter 8199 step
2021-08-08 23:09:08,305   Num examples = 9832
2021-08-08 23:09:08,305   Batch size = 32
2021-08-08 23:09:19,163 ***** Eval results *****
2021-08-08 23:09:19,163   acc = 0.5330553295362083
2021-08-08 23:09:19,163   att_loss = 0.0
2021-08-08 23:09:19,163   cls_loss = 0.07841202467661804
2021-08-08 23:09:19,163   eval_loss = 0.859864643448359
2021-08-08 23:09:19,163   global_step = 8199
2021-08-08 23:09:19,163   loss = 0.07841202467661804
2021-08-08 23:09:19,163   rep_loss = 0.0
2021-08-08 23:09:19,163 ***** Save model *****
2021-08-08 23:09:22,325 Writing example 0 of 9832
2021-08-08 23:09:22,326 *** Example ***
2021-08-08 23:09:22,326 guid: dev_matched-0
2021-08-08 23:09:22,326 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:09:22,326 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:09:22,326 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:09:22,326 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:09:22,326 label: contradiction
2021-08-08 23:09:22,326 label_id: 0
2021-08-08 23:09:26,937 ***** Running mm evaluation *****
2021-08-08 23:09:26,938   Num examples = 9832
2021-08-08 23:09:26,938   Batch size = 32
2021-08-08 23:09:39,419 ***** Eval results *****
2021-08-08 23:09:39,419   acc = 0.5330553295362083
2021-08-08 23:09:39,419   eval_loss = 0.859864643448359
2021-08-08 23:09:39,419   global_step = 8199
2021-08-08 23:10:15,958 ***** Running evaluation *****
2021-08-08 23:10:15,959   Epoch = 0 iter 8399 step
2021-08-08 23:10:15,959   Num examples = 9832
2021-08-08 23:10:15,959   Batch size = 32
2021-08-08 23:10:26,814 ***** Eval results *****
2021-08-08 23:10:26,814   acc = 0.5180024410089503
2021-08-08 23:10:26,814   att_loss = 0.0
2021-08-08 23:10:26,815   cls_loss = 0.07841047469150851
2021-08-08 23:10:26,815   eval_loss = 0.8630420939101802
2021-08-08 23:10:26,815   global_step = 8399
2021-08-08 23:10:26,815   loss = 0.07841047469150851
2021-08-08 23:10:26,815   rep_loss = 0.0
2021-08-08 23:10:26,815 ***** Save model *****
2021-08-08 23:10:28,399 Writing example 0 of 9832
2021-08-08 23:10:28,400 *** Example ***
2021-08-08 23:10:28,400 guid: dev_matched-0
2021-08-08 23:10:28,400 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:10:28,400 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:10:28,400 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:10:28,400 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:10:28,400 label: contradiction
2021-08-08 23:10:28,400 label_id: 0
2021-08-08 23:10:32,987 ***** Running mm evaluation *****
2021-08-08 23:10:32,987   Num examples = 9832
2021-08-08 23:10:32,987   Batch size = 32
2021-08-08 23:10:43,883 ***** Eval results *****
2021-08-08 23:10:43,883   acc = 0.5180024410089503
2021-08-08 23:10:43,883   eval_loss = 0.8630420939101802
2021-08-08 23:10:43,883   global_step = 8399
2021-08-08 23:11:22,168 ***** Running evaluation *****
2021-08-08 23:11:22,169   Epoch = 0 iter 8599 step
2021-08-08 23:11:22,169   Num examples = 9832
2021-08-08 23:11:22,169   Batch size = 32
2021-08-08 23:11:33,085 ***** Eval results *****
2021-08-08 23:11:33,085   acc = 0.519426362896664
2021-08-08 23:11:33,086   att_loss = 0.0
2021-08-08 23:11:33,086   cls_loss = 0.07840726967779256
2021-08-08 23:11:33,086   eval_loss = 0.8667146909933585
2021-08-08 23:11:33,086   global_step = 8599
2021-08-08 23:11:33,086   loss = 0.07840726967779256
2021-08-08 23:11:33,086   rep_loss = 0.0
2021-08-08 23:11:33,086 ***** Save model *****
2021-08-08 23:11:39,272 Writing example 0 of 9832
2021-08-08 23:11:39,273 *** Example ***
2021-08-08 23:11:39,273 guid: dev_matched-0
2021-08-08 23:11:39,273 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:11:39,273 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:11:39,273 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:11:39,273 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:11:39,273 label: contradiction
2021-08-08 23:11:39,273 label_id: 0
2021-08-08 23:11:43,908 ***** Running mm evaluation *****
2021-08-08 23:11:43,908   Num examples = 9832
2021-08-08 23:11:43,908   Batch size = 32
2021-08-08 23:11:54,787 ***** Eval results *****
2021-08-08 23:11:54,787   acc = 0.519426362896664
2021-08-08 23:11:54,787   eval_loss = 0.8667146909933585
2021-08-08 23:11:54,787   global_step = 8599
2021-08-08 23:12:32,919 ***** Running evaluation *****
2021-08-08 23:12:32,919   Epoch = 0 iter 8799 step
2021-08-08 23:12:32,919   Num examples = 9832
2021-08-08 23:12:32,919   Batch size = 32
2021-08-08 23:12:43,820 ***** Eval results *****
2021-08-08 23:12:43,820   acc = 0.5058991049633849
2021-08-08 23:12:43,820   att_loss = 0.0
2021-08-08 23:12:43,820   cls_loss = 0.07840672051863232
2021-08-08 23:12:43,820   eval_loss = 0.8721594920793136
2021-08-08 23:12:43,820   global_step = 8799
2021-08-08 23:12:43,820   loss = 0.07840672051863232
2021-08-08 23:12:43,820   rep_loss = 0.0
2021-08-08 23:12:43,821 ***** Save model *****
2021-08-08 23:12:45,259 Writing example 0 of 9832
2021-08-08 23:12:45,259 *** Example ***
2021-08-08 23:12:45,260 guid: dev_matched-0
2021-08-08 23:12:45,260 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:12:45,260 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:12:45,260 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:12:45,260 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:12:45,260 label: contradiction
2021-08-08 23:12:45,260 label_id: 0
2021-08-08 23:12:49,826 ***** Running mm evaluation *****
2021-08-08 23:12:49,826   Num examples = 9832
2021-08-08 23:12:49,826   Batch size = 32
2021-08-08 23:13:00,679 ***** Eval results *****
2021-08-08 23:13:00,679   acc = 0.5058991049633849
2021-08-08 23:13:00,679   eval_loss = 0.8721594920793136
2021-08-08 23:13:00,679   global_step = 8799
2021-08-08 23:13:38,790 ***** Running evaluation *****
2021-08-08 23:13:38,790   Epoch = 0 iter 8999 step
2021-08-08 23:13:38,791   Num examples = 9832
2021-08-08 23:13:38,791   Batch size = 32
2021-08-08 23:13:49,627 ***** Eval results *****
2021-08-08 23:13:49,627   acc = 0.5071196094385679
2021-08-08 23:13:49,627   att_loss = 0.0
2021-08-08 23:13:49,627   cls_loss = 0.0784046416720333
2021-08-08 23:13:49,627   eval_loss = 0.8773529231548309
2021-08-08 23:13:49,627   global_step = 8999
2021-08-08 23:13:49,627   loss = 0.0784046416720333
2021-08-08 23:13:49,627   rep_loss = 0.0
2021-08-08 23:13:49,628 ***** Save model *****
2021-08-08 23:13:50,805 Writing example 0 of 9832
2021-08-08 23:13:50,806 *** Example ***
2021-08-08 23:13:50,806 guid: dev_matched-0
2021-08-08 23:13:50,806 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:13:50,806 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:13:50,806 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:13:50,806 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:13:50,806 label: contradiction
2021-08-08 23:13:50,806 label_id: 0
2021-08-08 23:13:55,376 ***** Running mm evaluation *****
2021-08-08 23:13:55,376   Num examples = 9832
2021-08-08 23:13:55,376   Batch size = 32
2021-08-08 23:14:06,222 ***** Eval results *****
2021-08-08 23:14:06,222   acc = 0.5071196094385679
2021-08-08 23:14:06,222   eval_loss = 0.8773529231548309
2021-08-08 23:14:06,222   global_step = 8999
2021-08-08 23:14:46,245 ***** Running evaluation *****
2021-08-08 23:14:46,245   Epoch = 0 iter 9199 step
2021-08-08 23:14:46,245   Num examples = 9832
2021-08-08 23:14:46,246   Batch size = 32
2021-08-08 23:14:57,288 ***** Eval results *****
2021-08-08 23:14:57,289   acc = 0.5278681855166802
2021-08-08 23:14:57,289   att_loss = 0.0
2021-08-08 23:14:57,289   cls_loss = 0.07840383284628968
2021-08-08 23:14:57,289   eval_loss = 0.86316905528694
2021-08-08 23:14:57,289   global_step = 9199
2021-08-08 23:14:57,289   loss = 0.07840383284628968
2021-08-08 23:14:57,289   rep_loss = 0.0
2021-08-08 23:14:57,289 ***** Save model *****
2021-08-08 23:14:59,610 Writing example 0 of 9832
2021-08-08 23:14:59,611 *** Example ***
2021-08-08 23:14:59,611 guid: dev_matched-0
2021-08-08 23:14:59,611 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:14:59,611 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:14:59,611 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:14:59,611 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:14:59,611 label: contradiction
2021-08-08 23:14:59,611 label_id: 0
2021-08-08 23:15:04,164 ***** Running mm evaluation *****
2021-08-08 23:15:04,164   Num examples = 9832
2021-08-08 23:15:04,164   Batch size = 32
2021-08-08 23:15:15,066 ***** Eval results *****
2021-08-08 23:15:15,066   acc = 0.5278681855166802
2021-08-08 23:15:15,066   eval_loss = 0.86316905528694
2021-08-08 23:15:15,066   global_step = 9199
2021-08-08 23:15:53,435 ***** Running evaluation *****
2021-08-08 23:15:53,435   Epoch = 0 iter 9399 step
2021-08-08 23:15:53,435   Num examples = 9832
2021-08-08 23:15:53,435   Batch size = 32
2021-08-08 23:16:04,282 ***** Eval results *****
2021-08-08 23:16:04,283   acc = 0.525020341741253
2021-08-08 23:16:04,283   att_loss = 0.0
2021-08-08 23:16:04,283   cls_loss = 0.07840321377333535
2021-08-08 23:16:04,283   eval_loss = 0.8656880464646723
2021-08-08 23:16:04,283   global_step = 9399
2021-08-08 23:16:04,283   loss = 0.07840321377333535
2021-08-08 23:16:04,283   rep_loss = 0.0
2021-08-08 23:16:04,283 ***** Save model *****
2021-08-08 23:16:05,202 Writing example 0 of 9832
2021-08-08 23:16:05,203 *** Example ***
2021-08-08 23:16:05,203 guid: dev_matched-0
2021-08-08 23:16:05,203 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:16:05,203 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:16:05,203 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:16:05,203 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:16:05,203 label: contradiction
2021-08-08 23:16:05,203 label_id: 0
2021-08-08 23:16:11,289 ***** Running mm evaluation *****
2021-08-08 23:16:11,290   Num examples = 9832
2021-08-08 23:16:11,290   Batch size = 32
2021-08-08 23:16:22,132 ***** Eval results *****
2021-08-08 23:16:22,132   acc = 0.525020341741253
2021-08-08 23:16:22,132   eval_loss = 0.8656880464646723
2021-08-08 23:16:22,132   global_step = 9399
2021-08-08 23:16:58,555 ***** Running evaluation *****
2021-08-08 23:16:58,555   Epoch = 0 iter 9599 step
2021-08-08 23:16:58,555   Num examples = 9832
2021-08-08 23:16:58,556   Batch size = 32
2021-08-08 23:17:11,643 ***** Eval results *****
2021-08-08 23:17:11,643   acc = 0.4953213995117982
2021-08-08 23:17:11,643   att_loss = 0.0
2021-08-08 23:17:11,644   cls_loss = 0.07840100189309801
2021-08-08 23:17:11,644   eval_loss = 0.8775595949067698
2021-08-08 23:17:11,644   global_step = 9599
2021-08-08 23:17:11,644   loss = 0.07840100189309801
2021-08-08 23:17:11,644   rep_loss = 0.0
2021-08-08 23:17:11,876 ***** Save model *****
2021-08-08 23:18:12,786 Writing example 0 of 9832
2021-08-08 23:18:12,786 *** Example ***
2021-08-08 23:18:12,786 guid: dev_matched-0
2021-08-08 23:18:12,787 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:18:12,787 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:18:12,787 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:18:12,787 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:18:12,787 label: contradiction
2021-08-08 23:18:12,787 label_id: 0
2021-08-08 23:18:17,337 ***** Running mm evaluation *****
2021-08-08 23:18:17,337   Num examples = 9832
2021-08-08 23:18:17,337   Batch size = 32
2021-08-08 23:18:28,192 ***** Eval results *****
2021-08-08 23:18:28,192   acc = 0.4953213995117982
2021-08-08 23:18:28,192   eval_loss = 0.8775595949067698
2021-08-08 23:18:28,192   global_step = 9599
2021-08-08 23:19:06,273 ***** Running evaluation *****
2021-08-08 23:19:06,273   Epoch = 0 iter 9799 step
2021-08-08 23:19:06,273   Num examples = 9832
2021-08-08 23:19:06,274   Batch size = 32
2021-08-08 23:19:17,098 ***** Eval results *****
2021-08-08 23:19:17,098   acc = 0.5154597233523189
2021-08-08 23:19:17,098   att_loss = 0.0
2021-08-08 23:19:17,098   cls_loss = 0.07839962667178738
2021-08-08 23:19:17,098   eval_loss = 0.8767700466242704
2021-08-08 23:19:17,098   global_step = 9799
2021-08-08 23:19:17,098   loss = 0.07839962667178738
2021-08-08 23:19:17,098   rep_loss = 0.0
2021-08-08 23:19:17,099 ***** Save model *****
2021-08-08 23:19:18,159 Writing example 0 of 9832
2021-08-08 23:19:18,160 *** Example ***
2021-08-08 23:19:18,160 guid: dev_matched-0
2021-08-08 23:19:18,160 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:19:18,160 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:19:18,160 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:19:18,160 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:19:18,160 label: contradiction
2021-08-08 23:19:18,160 label_id: 0
2021-08-08 23:19:22,767 ***** Running mm evaluation *****
2021-08-08 23:19:22,768   Num examples = 9832
2021-08-08 23:19:22,768   Batch size = 32
2021-08-08 23:19:33,634 ***** Eval results *****
2021-08-08 23:19:33,634   acc = 0.5154597233523189
2021-08-08 23:19:33,634   eval_loss = 0.8767700466242704
2021-08-08 23:19:33,634   global_step = 9799
2021-08-08 23:20:13,596 ***** Running evaluation *****
2021-08-08 23:20:13,597   Epoch = 0 iter 9999 step
2021-08-08 23:20:13,597   Num examples = 9832
2021-08-08 23:20:13,597   Batch size = 32
2021-08-08 23:20:24,463 ***** Eval results *****
2021-08-08 23:20:24,463   acc = 0.5328519121236778
2021-08-08 23:20:24,463   att_loss = 0.0
2021-08-08 23:20:24,463   cls_loss = 0.07839920398446605
2021-08-08 23:20:24,463   eval_loss = 0.8688571182164279
2021-08-08 23:20:24,463   global_step = 9999
2021-08-08 23:20:24,463   loss = 0.07839920398446605
2021-08-08 23:20:24,463   rep_loss = 0.0
2021-08-08 23:20:24,463 ***** Save model *****
2021-08-08 23:20:27,695 Writing example 0 of 9832
2021-08-08 23:20:27,696 *** Example ***
2021-08-08 23:20:27,696 guid: dev_matched-0
2021-08-08 23:20:27,696 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:20:27,696 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:20:27,696 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:20:27,696 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:20:27,696 label: contradiction
2021-08-08 23:20:27,696 label_id: 0
2021-08-08 23:20:32,259 ***** Running mm evaluation *****
2021-08-08 23:20:32,259   Num examples = 9832
2021-08-08 23:20:32,259   Batch size = 32
2021-08-08 23:20:43,141 ***** Eval results *****
2021-08-08 23:20:43,141   acc = 0.5328519121236778
2021-08-08 23:20:43,141   eval_loss = 0.8688571182164279
2021-08-08 23:20:43,141   global_step = 9999
2021-08-08 23:21:21,213 ***** Running evaluation *****
2021-08-08 23:21:21,213   Epoch = 0 iter 10199 step
2021-08-08 23:21:21,213   Num examples = 9832
2021-08-08 23:21:21,213   Batch size = 32
2021-08-08 23:21:32,077 ***** Eval results *****
2021-08-08 23:21:32,078   acc = 0.49491456468673717
2021-08-08 23:21:32,078   att_loss = 0.0
2021-08-08 23:21:32,078   cls_loss = 0.07839837785383243
2021-08-08 23:21:32,078   eval_loss = 0.8866717639294538
2021-08-08 23:21:32,078   global_step = 10199
2021-08-08 23:21:32,078   loss = 0.07839837785383243
2021-08-08 23:21:32,078   rep_loss = 0.0
2021-08-08 23:21:32,079 ***** Save model *****
2021-08-08 23:21:33,483 Writing example 0 of 9832
2021-08-08 23:21:33,483 *** Example ***
2021-08-08 23:21:33,483 guid: dev_matched-0
2021-08-08 23:21:33,483 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:21:33,483 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:21:33,483 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:21:33,484 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:21:33,484 label: contradiction
2021-08-08 23:21:33,484 label_id: 0
2021-08-08 23:21:38,047 ***** Running mm evaluation *****
2021-08-08 23:21:38,047   Num examples = 9832
2021-08-08 23:21:38,047   Batch size = 32
2021-08-08 23:21:50,557 ***** Eval results *****
2021-08-08 23:21:50,557   acc = 0.49491456468673717
2021-08-08 23:21:50,557   eval_loss = 0.8866717639294538
2021-08-08 23:21:50,558   global_step = 10199
2021-08-08 23:22:26,932 ***** Running evaluation *****
2021-08-08 23:22:26,932   Epoch = 0 iter 10399 step
2021-08-08 23:22:26,932   Num examples = 9832
2021-08-08 23:22:26,932   Batch size = 32
2021-08-08 23:22:39,366 ***** Eval results *****
2021-08-08 23:22:39,367   acc = 0.4942026037428804
2021-08-08 23:22:39,367   att_loss = 0.0
2021-08-08 23:22:39,367   cls_loss = 0.0783965689541113
2021-08-08 23:22:39,367   eval_loss = 0.8796165769363379
2021-08-08 23:22:39,367   global_step = 10399
2021-08-08 23:22:39,367   loss = 0.0783965689541113
2021-08-08 23:22:39,367   rep_loss = 0.0
2021-08-08 23:22:39,367 ***** Save model *****
2021-08-08 23:22:40,386 Writing example 0 of 9832
2021-08-08 23:22:40,386 *** Example ***
2021-08-08 23:22:40,386 guid: dev_matched-0
2021-08-08 23:22:40,386 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:22:40,386 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:22:40,387 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:22:40,387 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:22:40,387 label: contradiction
2021-08-08 23:22:40,387 label_id: 0
2021-08-08 23:22:44,951 ***** Running mm evaluation *****
2021-08-08 23:22:44,951   Num examples = 9832
2021-08-08 23:22:44,951   Batch size = 32
2021-08-08 23:22:55,842 ***** Eval results *****
2021-08-08 23:22:55,843   acc = 0.4942026037428804
2021-08-08 23:22:55,843   eval_loss = 0.8796165769363379
2021-08-08 23:22:55,843   global_step = 10399
2021-08-08 23:23:35,600 ***** Running evaluation *****
2021-08-08 23:23:35,601   Epoch = 1 iter 10599 step
2021-08-08 23:23:35,601   Num examples = 9832
2021-08-08 23:23:35,601   Batch size = 32
2021-08-08 23:23:46,564 ***** Eval results *****
2021-08-08 23:23:46,565   acc = 0.4830146460537022
2021-08-08 23:23:46,565   att_loss = 0.0
2021-08-08 23:23:46,565   cls_loss = 0.07829454485605013
2021-08-08 23:23:46,565   eval_loss = 0.8869063469109597
2021-08-08 23:23:46,565   global_step = 10599
2021-08-08 23:23:46,565   loss = 0.07829454485605013
2021-08-08 23:23:46,565   rep_loss = 0.0
2021-08-08 23:23:46,566 ***** Save model *****
2021-08-08 23:23:47,740 Writing example 0 of 9832
2021-08-08 23:23:47,740 *** Example ***
2021-08-08 23:23:47,740 guid: dev_matched-0
2021-08-08 23:23:47,741 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:23:47,741 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:23:47,741 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:23:47,741 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:23:47,741 label: contradiction
2021-08-08 23:23:47,741 label_id: 0
2021-08-08 23:23:52,303 ***** Running mm evaluation *****
2021-08-08 23:23:52,303   Num examples = 9832
2021-08-08 23:23:52,304   Batch size = 32
2021-08-08 23:24:03,221 ***** Eval results *****
2021-08-08 23:24:03,221   acc = 0.4830146460537022
2021-08-08 23:24:03,221   eval_loss = 0.8869063469109597
2021-08-08 23:24:03,221   global_step = 10599
2021-08-08 23:24:41,432 ***** Running evaluation *****
2021-08-08 23:24:41,433   Epoch = 1 iter 10799 step
2021-08-08 23:24:41,433   Num examples = 9832
2021-08-08 23:24:41,433   Batch size = 32
2021-08-08 23:24:52,315 ***** Eval results *****
2021-08-08 23:24:52,315   acc = 0.5011187957689178
2021-08-08 23:24:52,315   att_loss = 0.0
2021-08-08 23:24:52,315   cls_loss = 0.07827769152903824
2021-08-08 23:24:52,315   eval_loss = 0.8809965750226727
2021-08-08 23:24:52,315   global_step = 10799
2021-08-08 23:24:52,315   loss = 0.07827769152903824
2021-08-08 23:24:52,315   rep_loss = 0.0
2021-08-08 23:24:52,316 ***** Save model *****
2021-08-08 23:24:53,139 Writing example 0 of 9832
2021-08-08 23:24:53,140 *** Example ***
2021-08-08 23:24:53,140 guid: dev_matched-0
2021-08-08 23:24:53,140 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:24:53,140 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:24:53,140 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:24:53,140 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:24:53,141 label: contradiction
2021-08-08 23:24:53,141 label_id: 0
2021-08-08 23:24:57,702 ***** Running mm evaluation *****
2021-08-08 23:24:57,702   Num examples = 9832
2021-08-08 23:24:57,702   Batch size = 32
2021-08-08 23:25:10,178 ***** Eval results *****
2021-08-08 23:25:10,178   acc = 0.5011187957689178
2021-08-08 23:25:10,178   eval_loss = 0.8809965750226727
2021-08-08 23:25:10,178   global_step = 10799
2021-08-08 23:25:46,628 ***** Running evaluation *****
2021-08-08 23:25:46,629   Epoch = 1 iter 10999 step
2021-08-08 23:25:46,629   Num examples = 9832
2021-08-08 23:25:46,629   Batch size = 32
2021-08-08 23:25:57,510 ***** Eval results *****
2021-08-08 23:25:57,510   acc = 0.48098047192839705
2021-08-08 23:25:57,510   att_loss = 0.0
2021-08-08 23:25:57,510   cls_loss = 0.07828050039756476
2021-08-08 23:25:57,510   eval_loss = 0.8890497523855854
2021-08-08 23:25:57,510   global_step = 10999
2021-08-08 23:25:57,510   loss = 0.07828050039756476
2021-08-08 23:25:57,510   rep_loss = 0.0
2021-08-08 23:25:57,510 ***** Save model *****
2021-08-08 23:26:00,428 Writing example 0 of 9832
2021-08-08 23:26:00,429 *** Example ***
2021-08-08 23:26:00,429 guid: dev_matched-0
2021-08-08 23:26:00,429 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:26:00,429 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:26:00,429 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:26:00,429 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:26:00,430 label: contradiction
2021-08-08 23:26:00,430 label_id: 0
2021-08-08 23:26:05,090 ***** Running mm evaluation *****
2021-08-08 23:26:05,090   Num examples = 9832
2021-08-08 23:26:05,090   Batch size = 32
2021-08-08 23:26:17,606 ***** Eval results *****
2021-08-08 23:26:17,607   acc = 0.48098047192839705
2021-08-08 23:26:17,607   eval_loss = 0.8890497523855854
2021-08-08 23:26:17,607   global_step = 10999
2021-08-08 23:26:54,066 ***** Running evaluation *****
2021-08-08 23:26:54,067   Epoch = 1 iter 11199 step
2021-08-08 23:26:54,067   Num examples = 9832
2021-08-08 23:26:54,067   Batch size = 32
2021-08-08 23:27:04,910 ***** Eval results *****
2021-08-08 23:27:04,911   acc = 0.5103742880390562
2021-08-08 23:27:04,911   att_loss = 0.0
2021-08-08 23:27:04,911   cls_loss = 0.07828027126134961
2021-08-08 23:27:04,911   eval_loss = 0.8759879400203754
2021-08-08 23:27:04,911   global_step = 11199
2021-08-08 23:27:04,911   loss = 0.07828027126134961
2021-08-08 23:27:04,911   rep_loss = 0.0
2021-08-08 23:27:04,911 ***** Save model *****
2021-08-08 23:27:07,207 Writing example 0 of 9832
2021-08-08 23:27:07,208 *** Example ***
2021-08-08 23:27:07,208 guid: dev_matched-0
2021-08-08 23:27:07,208 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:27:07,208 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:27:07,208 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:27:07,208 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:27:07,208 label: contradiction
2021-08-08 23:27:07,208 label_id: 0
2021-08-08 23:27:11,761 ***** Running mm evaluation *****
2021-08-08 23:27:11,761   Num examples = 9832
2021-08-08 23:27:11,761   Batch size = 32
2021-08-08 23:27:24,107 ***** Eval results *****
2021-08-08 23:27:24,108   acc = 0.5103742880390562
2021-08-08 23:27:24,108   eval_loss = 0.8759879400203754
2021-08-08 23:27:24,108   global_step = 11199
2021-08-08 23:28:00,585 ***** Running evaluation *****
2021-08-08 23:28:00,585   Epoch = 1 iter 11399 step
2021-08-08 23:28:00,585   Num examples = 9832
2021-08-08 23:28:00,585   Batch size = 32
2021-08-08 23:28:11,399 ***** Eval results *****
2021-08-08 23:28:11,399   acc = 0.48474369406021156
2021-08-08 23:28:11,399   att_loss = 0.0
2021-08-08 23:28:11,399   cls_loss = 0.07827681454270219
2021-08-08 23:28:11,399   eval_loss = 0.8866769574679337
2021-08-08 23:28:11,399   global_step = 11399
2021-08-08 23:28:11,399   loss = 0.07827681454270219
2021-08-08 23:28:11,399   rep_loss = 0.0
2021-08-08 23:28:11,399 ***** Save model *****
2021-08-08 23:28:13,932 Writing example 0 of 9832
2021-08-08 23:28:13,933 *** Example ***
2021-08-08 23:28:13,933 guid: dev_matched-0
2021-08-08 23:28:13,933 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:28:13,933 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:28:13,934 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:28:13,934 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:28:13,934 label: contradiction
2021-08-08 23:28:13,934 label_id: 0
2021-08-08 23:28:18,498 ***** Running mm evaluation *****
2021-08-08 23:28:18,498   Num examples = 9832
2021-08-08 23:28:18,498   Batch size = 32
2021-08-08 23:28:29,367 ***** Eval results *****
2021-08-08 23:28:29,367   acc = 0.48474369406021156
2021-08-08 23:28:29,367   eval_loss = 0.8866769574679337
2021-08-08 23:28:29,367   global_step = 11399
2021-08-08 23:29:07,609 ***** Running evaluation *****
2021-08-08 23:29:07,609   Epoch = 1 iter 11599 step
2021-08-08 23:29:07,609   Num examples = 9832
2021-08-08 23:29:07,609   Batch size = 32
2021-08-08 23:29:19,998 ***** Eval results *****
2021-08-08 23:29:19,998   acc = 0.4906427990235964
2021-08-08 23:29:19,998   att_loss = 0.0
2021-08-08 23:29:19,998   cls_loss = 0.07827364355186044
2021-08-08 23:29:19,998   eval_loss = 0.8804995191174668
2021-08-08 23:29:19,998   global_step = 11599
2021-08-08 23:29:19,998   loss = 0.07827364355186044
2021-08-08 23:29:19,998   rep_loss = 0.0
2021-08-08 23:29:19,999 ***** Save model *****
2021-08-08 23:29:20,807 Writing example 0 of 9832
2021-08-08 23:29:20,808 *** Example ***
2021-08-08 23:29:20,808 guid: dev_matched-0
2021-08-08 23:29:20,808 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:29:20,808 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:29:20,808 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:29:20,808 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:29:20,808 label: contradiction
2021-08-08 23:29:20,808 label_id: 0
2021-08-08 23:29:25,381 ***** Running mm evaluation *****
2021-08-08 23:29:25,381   Num examples = 9832
2021-08-08 23:29:25,381   Batch size = 32
2021-08-08 23:29:36,234 ***** Eval results *****
2021-08-08 23:29:36,234   acc = 0.4906427990235964
2021-08-08 23:29:36,234   eval_loss = 0.8804995191174668
2021-08-08 23:29:36,234   global_step = 11599
2021-08-08 23:30:12,796 ***** Running evaluation *****
2021-08-08 23:30:12,796   Epoch = 1 iter 11799 step
2021-08-08 23:30:12,796   Num examples = 9832
2021-08-08 23:30:12,796   Batch size = 32
2021-08-08 23:30:25,329 ***** Eval results *****
2021-08-08 23:30:25,330   acc = 0.480166802278275
2021-08-08 23:30:25,330   att_loss = 0.0
2021-08-08 23:30:25,330   cls_loss = 0.07827217042469659
2021-08-08 23:30:25,330   eval_loss = 0.8883084544500748
2021-08-08 23:30:25,330   global_step = 11799
2021-08-08 23:30:25,330   loss = 0.07827217042469659
2021-08-08 23:30:25,330   rep_loss = 0.0
2021-08-08 23:30:25,330 ***** Save model *****
2021-08-08 23:30:26,184 Writing example 0 of 9832
2021-08-08 23:30:26,184 *** Example ***
2021-08-08 23:30:26,184 guid: dev_matched-0
2021-08-08 23:30:26,184 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:30:26,185 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:30:26,185 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:30:26,185 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:30:26,185 label: contradiction
2021-08-08 23:30:26,185 label_id: 0
2021-08-08 23:30:30,764 ***** Running mm evaluation *****
2021-08-08 23:30:30,764   Num examples = 9832
2021-08-08 23:30:30,764   Batch size = 32
2021-08-08 23:30:41,614 ***** Eval results *****
2021-08-08 23:30:41,614   acc = 0.480166802278275
2021-08-08 23:30:41,614   eval_loss = 0.8883084544500748
2021-08-08 23:30:41,614   global_step = 11799
2021-08-08 23:31:19,603 ***** Running evaluation *****
2021-08-08 23:31:19,604   Epoch = 1 iter 11999 step
2021-08-08 23:31:19,604   Num examples = 9832
2021-08-08 23:31:19,604   Batch size = 32
2021-08-08 23:31:30,476 ***** Eval results *****
2021-08-08 23:31:30,477   acc = 0.495626525630594
2021-08-08 23:31:30,477   att_loss = 0.0
2021-08-08 23:31:30,477   cls_loss = 0.0782742610413279
2021-08-08 23:31:30,477   eval_loss = 0.8794880424227033
2021-08-08 23:31:30,477   global_step = 11999
2021-08-08 23:31:30,477   loss = 0.0782742610413279
2021-08-08 23:31:30,477   rep_loss = 0.0
2021-08-08 23:31:30,477 ***** Save model *****
2021-08-08 23:31:31,340 Writing example 0 of 9832
2021-08-08 23:31:31,340 *** Example ***
2021-08-08 23:31:31,341 guid: dev_matched-0
2021-08-08 23:31:31,341 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:31:31,341 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:31:31,341 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:31:31,341 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:31:31,341 label: contradiction
2021-08-08 23:31:31,341 label_id: 0
2021-08-08 23:31:35,897 ***** Running mm evaluation *****
2021-08-08 23:31:35,897   Num examples = 9832
2021-08-08 23:31:35,897   Batch size = 32
2021-08-08 23:31:46,723 ***** Eval results *****
2021-08-08 23:31:46,724   acc = 0.495626525630594
2021-08-08 23:31:46,724   eval_loss = 0.8794880424227033
2021-08-08 23:31:46,724   global_step = 11999
2021-08-08 23:32:23,225 ***** Running evaluation *****
2021-08-08 23:32:23,225   Epoch = 1 iter 12199 step
2021-08-08 23:32:23,225   Num examples = 9832
2021-08-08 23:32:23,225   Batch size = 32
2021-08-08 23:32:35,775 ***** Eval results *****
2021-08-08 23:32:35,775   acc = 0.48626932465419037
2021-08-08 23:32:35,775   att_loss = 0.0
2021-08-08 23:32:35,775   cls_loss = 0.07827098563036378
2021-08-08 23:32:35,775   eval_loss = 0.8841200736048934
2021-08-08 23:32:35,775   global_step = 12199
2021-08-08 23:32:35,775   loss = 0.07827098563036378
2021-08-08 23:32:35,775   rep_loss = 0.0
2021-08-08 23:32:35,776 ***** Save model *****
2021-08-08 23:32:36,569 Writing example 0 of 9832
2021-08-08 23:32:36,570 *** Example ***
2021-08-08 23:32:36,570 guid: dev_matched-0
2021-08-08 23:32:36,570 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:32:36,570 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:32:36,570 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:32:36,570 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:32:36,570 label: contradiction
2021-08-08 23:32:36,570 label_id: 0
2021-08-08 23:32:41,128 ***** Running mm evaluation *****
2021-08-08 23:32:41,128   Num examples = 9832
2021-08-08 23:32:41,128   Batch size = 32
2021-08-08 23:32:52,003 ***** Eval results *****
2021-08-08 23:32:52,003   acc = 0.48626932465419037
2021-08-08 23:32:52,003   eval_loss = 0.8841200736048934
2021-08-08 23:32:52,003   global_step = 12199
2021-08-08 23:33:30,210 ***** Running evaluation *****
2021-08-08 23:33:30,211   Epoch = 1 iter 12399 step
2021-08-08 23:33:30,211   Num examples = 9832
2021-08-08 23:33:30,211   Batch size = 32
2021-08-08 23:33:41,031 ***** Eval results *****
2021-08-08 23:33:41,031   acc = 0.491659886086249
2021-08-08 23:33:41,031   att_loss = 0.0
2021-08-08 23:33:41,032   cls_loss = 0.07827523368673651
2021-08-08 23:33:41,032   eval_loss = 0.8767774339233126
2021-08-08 23:33:41,032   global_step = 12399
2021-08-08 23:33:41,032   loss = 0.07827523368673651
2021-08-08 23:33:41,032   rep_loss = 0.0
2021-08-08 23:33:41,032 ***** Save model *****
2021-08-08 23:33:43,561 Writing example 0 of 9832
2021-08-08 23:33:43,561 *** Example ***
2021-08-08 23:33:43,562 guid: dev_matched-0
2021-08-08 23:33:43,562 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:33:43,562 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:33:43,562 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:33:43,562 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:33:43,562 label: contradiction
2021-08-08 23:33:43,562 label_id: 0
2021-08-08 23:33:48,192 ***** Running mm evaluation *****
2021-08-08 23:33:48,192   Num examples = 9832
2021-08-08 23:33:48,192   Batch size = 32
2021-08-08 23:33:59,088 ***** Eval results *****
2021-08-08 23:33:59,088   acc = 0.491659886086249
2021-08-08 23:33:59,088   eval_loss = 0.8767774339233126
2021-08-08 23:33:59,088   global_step = 12399
2021-08-08 23:34:37,180 ***** Running evaluation *****
2021-08-08 23:34:37,181   Epoch = 1 iter 12599 step
2021-08-08 23:34:37,181   Num examples = 9832
2021-08-08 23:34:37,181   Batch size = 32
2021-08-08 23:34:48,033 ***** Eval results *****
2021-08-08 23:34:48,033   acc = 0.5152563059397884
2021-08-08 23:34:48,033   att_loss = 0.0
2021-08-08 23:34:48,033   cls_loss = 0.07827987575954899
2021-08-08 23:34:48,033   eval_loss = 0.8718095148925658
2021-08-08 23:34:48,033   global_step = 12599
2021-08-08 23:34:48,033   loss = 0.07827987575954899
2021-08-08 23:34:48,033   rep_loss = 0.0
2021-08-08 23:34:48,034 ***** Save model *****
2021-08-08 23:34:49,147 Writing example 0 of 9832
2021-08-08 23:34:49,147 *** Example ***
2021-08-08 23:34:49,147 guid: dev_matched-0
2021-08-08 23:34:49,147 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:34:49,148 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:34:49,148 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:34:49,148 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:34:49,148 label: contradiction
2021-08-08 23:34:49,148 label_id: 0
2021-08-08 23:34:53,714 ***** Running mm evaluation *****
2021-08-08 23:34:53,714   Num examples = 9832
2021-08-08 23:34:53,714   Batch size = 32
2021-08-08 23:35:04,608 ***** Eval results *****
2021-08-08 23:35:04,608   acc = 0.5152563059397884
2021-08-08 23:35:04,608   eval_loss = 0.8718095148925658
2021-08-08 23:35:04,608   global_step = 12599
2021-08-08 23:35:42,751 ***** Running evaluation *****
2021-08-08 23:35:42,751   Epoch = 1 iter 12799 step
2021-08-08 23:35:42,751   Num examples = 9832
2021-08-08 23:35:42,751   Batch size = 32
2021-08-08 23:35:53,552 ***** Eval results *****
2021-08-08 23:35:53,552   acc = 0.5067127746135069
2021-08-08 23:35:53,552   att_loss = 0.0
2021-08-08 23:35:53,552   cls_loss = 0.07828016955788356
2021-08-08 23:35:53,552   eval_loss = 0.8809964250434529
2021-08-08 23:35:53,552   global_step = 12799
2021-08-08 23:35:53,552   loss = 0.07828016955788356
2021-08-08 23:35:53,552   rep_loss = 0.0
2021-08-08 23:35:53,553 ***** Save model *****
2021-08-08 23:35:55,120 Writing example 0 of 9832
2021-08-08 23:35:55,121 *** Example ***
2021-08-08 23:35:55,121 guid: dev_matched-0
2021-08-08 23:35:55,121 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:35:55,121 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:35:55,121 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:35:55,121 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:35:55,121 label: contradiction
2021-08-08 23:35:55,121 label_id: 0
2021-08-08 23:35:59,687 ***** Running mm evaluation *****
2021-08-08 23:35:59,687   Num examples = 9832
2021-08-08 23:35:59,687   Batch size = 32
2021-08-08 23:36:10,559 ***** Eval results *****
2021-08-08 23:36:10,559   acc = 0.5067127746135069
2021-08-08 23:36:10,559   eval_loss = 0.8809964250434529
2021-08-08 23:36:10,559   global_step = 12799
2021-08-08 23:36:50,416 ***** Running evaluation *****
2021-08-08 23:36:50,417   Epoch = 1 iter 12999 step
2021-08-08 23:36:50,417   Num examples = 9832
2021-08-08 23:36:50,417   Batch size = 32
2021-08-08 23:37:01,264 ***** Eval results *****
2021-08-08 23:37:01,264   acc = 0.48810008136696503
2021-08-08 23:37:01,264   att_loss = 0.0
2021-08-08 23:37:01,264   cls_loss = 0.07828011122136934
2021-08-08 23:37:01,264   eval_loss = 0.8877712803614604
2021-08-08 23:37:01,264   global_step = 12999
2021-08-08 23:37:01,264   loss = 0.07828011122136934
2021-08-08 23:37:01,264   rep_loss = 0.0
2021-08-08 23:37:01,265 ***** Save model *****
2021-08-08 23:37:09,674 Writing example 0 of 9832
2021-08-08 23:37:09,675 *** Example ***
2021-08-08 23:37:09,675 guid: dev_matched-0
2021-08-08 23:37:09,675 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:37:09,675 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:37:09,675 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:37:09,675 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:37:09,675 label: contradiction
2021-08-08 23:37:09,675 label_id: 0
2021-08-08 23:37:14,231 ***** Running mm evaluation *****
2021-08-08 23:37:14,231   Num examples = 9832
2021-08-08 23:37:14,231   Batch size = 32
2021-08-08 23:37:26,776 ***** Eval results *****
2021-08-08 23:37:26,776   acc = 0.48810008136696503
2021-08-08 23:37:26,776   eval_loss = 0.8877712803614604
2021-08-08 23:37:26,776   global_step = 12999
2021-08-08 23:38:04,934 ***** Running evaluation *****
2021-08-08 23:38:04,935   Epoch = 1 iter 13199 step
2021-08-08 23:38:04,935   Num examples = 9832
2021-08-08 23:38:04,935   Batch size = 32
2021-08-08 23:38:15,772 ***** Eval results *****
2021-08-08 23:38:15,772   acc = 0.4827095199349064
2021-08-08 23:38:15,772   att_loss = 0.0
2021-08-08 23:38:15,772   cls_loss = 0.07828106138370526
2021-08-08 23:38:15,773   eval_loss = 0.8855495849600086
2021-08-08 23:38:15,773   global_step = 13199
2021-08-08 23:38:15,773   loss = 0.07828106138370526
2021-08-08 23:38:15,773   rep_loss = 0.0
2021-08-08 23:38:15,773 ***** Save model *****
2021-08-08 23:38:17,087 Writing example 0 of 9832
2021-08-08 23:38:17,088 *** Example ***
2021-08-08 23:38:17,088 guid: dev_matched-0
2021-08-08 23:38:17,088 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:38:17,088 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:38:17,088 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:38:17,088 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:38:17,088 label: contradiction
2021-08-08 23:38:17,088 label_id: 0
2021-08-08 23:38:21,687 ***** Running mm evaluation *****
2021-08-08 23:38:21,687   Num examples = 9832
2021-08-08 23:38:21,687   Batch size = 32
2021-08-08 23:38:32,647 ***** Eval results *****
2021-08-08 23:38:32,648   acc = 0.4827095199349064
2021-08-08 23:38:32,648   eval_loss = 0.8855495849600086
2021-08-08 23:38:32,648   global_step = 13199
2021-08-08 23:39:10,922 ***** Running evaluation *****
2021-08-08 23:39:10,922   Epoch = 1 iter 13399 step
2021-08-08 23:39:10,922   Num examples = 9832
2021-08-08 23:39:10,922   Batch size = 32
2021-08-08 23:39:21,751 ***** Eval results *****
2021-08-08 23:39:21,751   acc = 0.4968470301057771
2021-08-08 23:39:21,751   att_loss = 0.0
2021-08-08 23:39:21,751   cls_loss = 0.07828051575830185
2021-08-08 23:39:21,751   eval_loss = 0.8825703271023639
2021-08-08 23:39:21,751   global_step = 13399
2021-08-08 23:39:21,751   loss = 0.07828051575830185
2021-08-08 23:39:21,751   rep_loss = 0.0
2021-08-08 23:39:21,751 ***** Save model *****
2021-08-08 23:39:22,721 Writing example 0 of 9832
2021-08-08 23:39:22,722 *** Example ***
2021-08-08 23:39:22,722 guid: dev_matched-0
2021-08-08 23:39:22,722 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:39:22,722 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:39:22,722 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:39:22,722 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:39:22,722 label: contradiction
2021-08-08 23:39:22,722 label_id: 0
2021-08-08 23:39:27,329 ***** Running mm evaluation *****
2021-08-08 23:39:27,329   Num examples = 9832
2021-08-08 23:39:27,329   Batch size = 32
2021-08-08 23:39:38,186 ***** Eval results *****
2021-08-08 23:39:38,186   acc = 0.4968470301057771
2021-08-08 23:39:38,186   eval_loss = 0.8825703271023639
2021-08-08 23:39:38,186   global_step = 13399
2021-08-08 23:40:16,359 ***** Running evaluation *****
2021-08-08 23:40:16,359   Epoch = 1 iter 13599 step
2021-08-08 23:40:16,359   Num examples = 9832
2021-08-08 23:40:16,359   Batch size = 32
2021-08-08 23:40:27,192 ***** Eval results *****
2021-08-08 23:40:27,192   acc = 0.49491456468673717
2021-08-08 23:40:27,192   att_loss = 0.0
2021-08-08 23:40:27,192   cls_loss = 0.07828215906596316
2021-08-08 23:40:27,192   eval_loss = 0.8842571499672803
2021-08-08 23:40:27,192   global_step = 13599
2021-08-08 23:40:27,192   loss = 0.07828215906596316
2021-08-08 23:40:27,192   rep_loss = 0.0
2021-08-08 23:40:27,192 ***** Save model *****
2021-08-08 23:40:29,985 Writing example 0 of 9832
2021-08-08 23:40:29,986 *** Example ***
2021-08-08 23:40:29,986 guid: dev_matched-0
2021-08-08 23:40:29,986 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:40:29,986 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:40:29,986 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:40:29,986 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:40:29,986 label: contradiction
2021-08-08 23:40:29,986 label_id: 0
2021-08-08 23:40:34,568 ***** Running mm evaluation *****
2021-08-08 23:40:34,568   Num examples = 9832
2021-08-08 23:40:34,569   Batch size = 32
2021-08-08 23:40:45,614 ***** Eval results *****
2021-08-08 23:40:45,614   acc = 0.49491456468673717
2021-08-08 23:40:45,614   eval_loss = 0.8842571499672803
2021-08-08 23:40:45,614   global_step = 13599
2021-08-08 23:41:25,595 ***** Running evaluation *****
2021-08-08 23:41:25,595   Epoch = 1 iter 13799 step
2021-08-08 23:41:25,595   Num examples = 9832
2021-08-08 23:41:25,595   Batch size = 32
2021-08-08 23:41:36,447 ***** Eval results *****
2021-08-08 23:41:36,447   acc = 0.48626932465419037
2021-08-08 23:41:36,447   att_loss = 0.0
2021-08-08 23:41:36,447   cls_loss = 0.07828231464431744
2021-08-08 23:41:36,447   eval_loss = 0.8851648936798047
2021-08-08 23:41:36,447   global_step = 13799
2021-08-08 23:41:36,447   loss = 0.07828231464431744
2021-08-08 23:41:36,448   rep_loss = 0.0
2021-08-08 23:41:36,448 ***** Save model *****
2021-08-08 23:41:45,375 Writing example 0 of 9832
2021-08-08 23:41:45,376 *** Example ***
2021-08-08 23:41:45,376 guid: dev_matched-0
2021-08-08 23:41:45,376 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:41:45,376 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:41:45,376 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:41:45,376 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:41:45,376 label: contradiction
2021-08-08 23:41:45,376 label_id: 0
2021-08-08 23:41:49,930 ***** Running mm evaluation *****
2021-08-08 23:41:49,930   Num examples = 9832
2021-08-08 23:41:49,930   Batch size = 32
2021-08-08 23:42:00,781 ***** Eval results *****
2021-08-08 23:42:00,781   acc = 0.48626932465419037
2021-08-08 23:42:00,781   eval_loss = 0.8851648936798047
2021-08-08 23:42:00,781   global_step = 13799
2021-08-08 23:42:38,897 ***** Running evaluation *****
2021-08-08 23:42:38,898   Epoch = 1 iter 13999 step
2021-08-08 23:42:38,898   Num examples = 9832
2021-08-08 23:42:38,898   Batch size = 32
2021-08-08 23:42:49,721 ***** Eval results *****
2021-08-08 23:42:49,721   acc = 0.48545565500406834
2021-08-08 23:42:49,722   att_loss = 0.0
2021-08-08 23:42:49,722   cls_loss = 0.07828183436698036
2021-08-08 23:42:49,722   eval_loss = 0.8873914847900342
2021-08-08 23:42:49,722   global_step = 13999
2021-08-08 23:42:49,722   loss = 0.07828183436698036
2021-08-08 23:42:49,722   rep_loss = 0.0
2021-08-08 23:42:49,722 ***** Save model *****
2021-08-08 23:42:50,602 Writing example 0 of 9832
2021-08-08 23:42:50,602 *** Example ***
2021-08-08 23:42:50,602 guid: dev_matched-0
2021-08-08 23:42:50,602 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:42:50,602 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:42:50,602 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:42:50,603 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:42:50,603 label: contradiction
2021-08-08 23:42:50,603 label_id: 0
2021-08-08 23:42:56,798 ***** Running mm evaluation *****
2021-08-08 23:42:56,798   Num examples = 9832
2021-08-08 23:42:56,798   Batch size = 32
2021-08-08 23:43:07,660 ***** Eval results *****
2021-08-08 23:43:07,661   acc = 0.48545565500406834
2021-08-08 23:43:07,661   eval_loss = 0.8873914847900342
2021-08-08 23:43:07,661   global_step = 13999
2021-08-08 23:43:43,998 ***** Running evaluation *****
2021-08-08 23:43:43,999   Epoch = 1 iter 14199 step
2021-08-08 23:43:43,999   Num examples = 9832
2021-08-08 23:43:43,999   Batch size = 32
2021-08-08 23:43:56,385 ***** Eval results *****
2021-08-08 23:43:56,385   acc = 0.487693246541904
2021-08-08 23:43:56,385   att_loss = 0.0
2021-08-08 23:43:56,385   cls_loss = 0.07827949992456974
2021-08-08 23:43:56,385   eval_loss = 0.8806434260173277
2021-08-08 23:43:56,385   global_step = 14199
2021-08-08 23:43:56,385   loss = 0.07827949992456974
2021-08-08 23:43:56,386   rep_loss = 0.0
2021-08-08 23:43:56,386 ***** Save model *****
2021-08-08 23:44:01,630 Writing example 0 of 9832
2021-08-08 23:44:01,631 *** Example ***
2021-08-08 23:44:01,631 guid: dev_matched-0
2021-08-08 23:44:01,631 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:44:01,631 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:44:01,631 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:44:01,631 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:44:01,631 label: contradiction
2021-08-08 23:44:01,631 label_id: 0
2021-08-08 23:44:06,196 ***** Running mm evaluation *****
2021-08-08 23:44:06,196   Num examples = 9832
2021-08-08 23:44:06,196   Batch size = 32
2021-08-08 23:44:17,042 ***** Eval results *****
2021-08-08 23:44:17,042   acc = 0.487693246541904
2021-08-08 23:44:17,042   eval_loss = 0.8806434260173277
2021-08-08 23:44:17,042   global_step = 14199
2021-08-08 23:44:55,198 ***** Running evaluation *****
2021-08-08 23:44:55,198   Epoch = 1 iter 14399 step
2021-08-08 23:44:55,198   Num examples = 9832
2021-08-08 23:44:55,198   Batch size = 32
2021-08-08 23:45:06,028 ***** Eval results *****
2021-08-08 23:45:06,028   acc = 0.48179414157851913
2021-08-08 23:45:06,028   att_loss = 0.0
2021-08-08 23:45:06,028   cls_loss = 0.07828130884968344
2021-08-08 23:45:06,028   eval_loss = 0.8896144200842102
2021-08-08 23:45:06,028   global_step = 14399
2021-08-08 23:45:06,028   loss = 0.07828130884968344
2021-08-08 23:45:06,028   rep_loss = 0.0
2021-08-08 23:45:06,029 ***** Save model *****
2021-08-08 23:46:51,837 Writing example 0 of 9832
2021-08-08 23:46:51,838 *** Example ***
2021-08-08 23:46:51,838 guid: dev_matched-0
2021-08-08 23:46:51,838 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:46:51,838 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:46:51,838 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:46:51,838 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:46:51,838 label: contradiction
2021-08-08 23:46:51,838 label_id: 0
2021-08-08 23:46:56,399 ***** Running mm evaluation *****
2021-08-08 23:46:56,399   Num examples = 9832
2021-08-08 23:46:56,399   Batch size = 32
2021-08-08 23:47:07,241 ***** Eval results *****
2021-08-08 23:47:07,241   acc = 0.48179414157851913
2021-08-08 23:47:07,242   eval_loss = 0.8896144200842102
2021-08-08 23:47:07,242   global_step = 14399
2021-08-08 23:47:43,657 ***** Running evaluation *****
2021-08-08 23:47:43,657   Epoch = 1 iter 14599 step
2021-08-08 23:47:43,657   Num examples = 9832
2021-08-08 23:47:43,657   Batch size = 32
2021-08-08 23:47:56,072 ***** Eval results *****
2021-08-08 23:47:56,072   acc = 0.49847436940602113
2021-08-08 23:47:56,072   att_loss = 0.0
2021-08-08 23:47:56,072   cls_loss = 0.07828380644709818
2021-08-08 23:47:56,072   eval_loss = 0.8825803753230479
2021-08-08 23:47:56,072   global_step = 14599
2021-08-08 23:47:56,072   loss = 0.07828380644709818
2021-08-08 23:47:56,072   rep_loss = 0.0
2021-08-08 23:47:56,072 ***** Save model *****
2021-08-08 23:47:57,887 Writing example 0 of 9832
2021-08-08 23:47:57,888 *** Example ***
2021-08-08 23:47:57,888 guid: dev_matched-0
2021-08-08 23:47:57,888 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:47:57,888 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:47:57,888 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:47:57,888 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:47:57,888 label: contradiction
2021-08-08 23:47:57,888 label_id: 0
2021-08-08 23:48:02,451 ***** Running mm evaluation *****
2021-08-08 23:48:02,452   Num examples = 9832
2021-08-08 23:48:02,452   Batch size = 32
2021-08-08 23:48:13,297 ***** Eval results *****
2021-08-08 23:48:13,297   acc = 0.49847436940602113
2021-08-08 23:48:13,297   eval_loss = 0.8825803753230479
2021-08-08 23:48:13,297   global_step = 14599
2021-08-08 23:48:51,275 ***** Running evaluation *****
2021-08-08 23:48:51,276   Epoch = 1 iter 14799 step
2021-08-08 23:48:51,276   Num examples = 9832
2021-08-08 23:48:51,276   Batch size = 32
2021-08-08 23:49:02,082 ***** Eval results *****
2021-08-08 23:49:02,083   acc = 0.4927786818551668
2021-08-08 23:49:02,083   att_loss = 0.0
2021-08-08 23:49:02,083   cls_loss = 0.07828372693680644
2021-08-08 23:49:02,083   eval_loss = 0.8841133382800338
2021-08-08 23:49:02,083   global_step = 14799
2021-08-08 23:49:02,083   loss = 0.07828372693680644
2021-08-08 23:49:02,083   rep_loss = 0.0
2021-08-08 23:49:02,083 ***** Save model *****
2021-08-08 23:49:02,991 Writing example 0 of 9832
2021-08-08 23:49:02,992 *** Example ***
2021-08-08 23:49:02,992 guid: dev_matched-0
2021-08-08 23:49:02,992 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:49:02,992 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:49:02,992 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:49:02,992 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:49:02,992 label: contradiction
2021-08-08 23:49:02,992 label_id: 0
2021-08-08 23:49:07,545 ***** Running mm evaluation *****
2021-08-08 23:49:07,546   Num examples = 9832
2021-08-08 23:49:07,546   Batch size = 32
2021-08-08 23:49:18,407 ***** Eval results *****
2021-08-08 23:49:18,408   acc = 0.4927786818551668
2021-08-08 23:49:18,408   eval_loss = 0.8841133382800338
2021-08-08 23:49:18,408   global_step = 14799
2021-08-08 23:49:56,566 ***** Running evaluation *****
2021-08-08 23:49:56,567   Epoch = 1 iter 14999 step
2021-08-08 23:49:56,567   Num examples = 9832
2021-08-08 23:49:56,567   Batch size = 32
2021-08-08 23:50:07,435 ***** Eval results *****
2021-08-08 23:50:07,435   acc = 0.4825061025223759
2021-08-08 23:50:07,435   att_loss = 0.0
2021-08-08 23:50:07,435   cls_loss = 0.07828392377796657
2021-08-08 23:50:07,435   eval_loss = 0.890498450630671
2021-08-08 23:50:07,435   global_step = 14999
2021-08-08 23:50:07,435   loss = 0.07828392377796657
2021-08-08 23:50:07,435   rep_loss = 0.0
2021-08-08 23:50:07,435 ***** Save model *****
2021-08-08 23:50:09,295 Writing example 0 of 9832
2021-08-08 23:50:09,296 *** Example ***
2021-08-08 23:50:09,296 guid: dev_matched-0
2021-08-08 23:50:09,296 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:50:09,296 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:50:09,296 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:50:09,296 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:50:09,296 label: contradiction
2021-08-08 23:50:09,296 label_id: 0
2021-08-08 23:50:13,867 ***** Running mm evaluation *****
2021-08-08 23:50:13,867   Num examples = 9832
2021-08-08 23:50:13,867   Batch size = 32
2021-08-08 23:50:24,712 ***** Eval results *****
2021-08-08 23:50:24,712   acc = 0.4825061025223759
2021-08-08 23:50:24,712   eval_loss = 0.890498450630671
2021-08-08 23:50:24,712   global_step = 14999
2021-08-08 23:51:02,718 ***** Running evaluation *****
2021-08-08 23:51:02,719   Epoch = 1 iter 15199 step
2021-08-08 23:51:02,719   Num examples = 9832
2021-08-08 23:51:02,719   Batch size = 32
2021-08-08 23:51:13,530 ***** Eval results *****
2021-08-08 23:51:13,530   acc = 0.475386493083808
2021-08-08 23:51:13,530   att_loss = 0.0
2021-08-08 23:51:13,530   cls_loss = 0.07828408228091772
2021-08-08 23:51:13,530   eval_loss = 0.8971551619953924
2021-08-08 23:51:13,530   global_step = 15199
2021-08-08 23:51:13,530   loss = 0.07828408228091772
2021-08-08 23:51:13,530   rep_loss = 0.0
2021-08-08 23:51:13,531 ***** Save model *****
2021-08-08 23:51:17,863 Writing example 0 of 9832
2021-08-08 23:51:17,864 *** Example ***
2021-08-08 23:51:17,864 guid: dev_matched-0
2021-08-08 23:51:17,864 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:51:17,864 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:51:17,864 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:51:17,865 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:51:17,865 label: contradiction
2021-08-08 23:51:17,865 label_id: 0
2021-08-08 23:51:22,460 ***** Running mm evaluation *****
2021-08-08 23:51:22,460   Num examples = 9832
2021-08-08 23:51:22,460   Batch size = 32
2021-08-08 23:51:33,352 ***** Eval results *****
2021-08-08 23:51:33,352   acc = 0.475386493083808
2021-08-08 23:51:33,353   eval_loss = 0.8971551619953924
2021-08-08 23:51:33,353   global_step = 15199
2021-08-08 23:52:11,565 ***** Running evaluation *****
2021-08-08 23:52:11,565   Epoch = 1 iter 15399 step
2021-08-08 23:52:11,565   Num examples = 9832
2021-08-08 23:52:11,565   Batch size = 32
2021-08-08 23:52:22,388 ***** Eval results *****
2021-08-08 23:52:22,388   acc = 0.47620016273393
2021-08-08 23:52:22,388   att_loss = 0.0
2021-08-08 23:52:22,388   cls_loss = 0.07828401256439155
2021-08-08 23:52:22,388   eval_loss = 0.8999313160583571
2021-08-08 23:52:22,388   global_step = 15399
2021-08-08 23:52:22,388   loss = 0.07828401256439155
2021-08-08 23:52:22,388   rep_loss = 0.0
2021-08-08 23:52:22,389 ***** Save model *****
2021-08-08 23:52:24,881 Writing example 0 of 9832
2021-08-08 23:52:24,881 *** Example ***
2021-08-08 23:52:24,881 guid: dev_matched-0
2021-08-08 23:52:24,882 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:52:24,882 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:52:24,882 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:52:24,882 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:52:24,882 label: contradiction
2021-08-08 23:52:24,882 label_id: 0
2021-08-08 23:52:29,454 ***** Running mm evaluation *****
2021-08-08 23:52:29,454   Num examples = 9832
2021-08-08 23:52:29,454   Batch size = 32
2021-08-08 23:52:40,344 ***** Eval results *****
2021-08-08 23:52:40,344   acc = 0.47620016273393
2021-08-08 23:52:40,344   eval_loss = 0.8999313160583571
2021-08-08 23:52:40,344   global_step = 15399
2021-08-08 23:53:18,613 ***** Running evaluation *****
2021-08-08 23:53:18,613   Epoch = 1 iter 15599 step
2021-08-08 23:53:18,613   Num examples = 9832
2021-08-08 23:53:18,613   Batch size = 32
2021-08-08 23:53:29,487 ***** Eval results *****
2021-08-08 23:53:29,487   acc = 0.47406427990235966
2021-08-08 23:53:29,487   att_loss = 0.0
2021-08-08 23:53:29,487   cls_loss = 0.07828456606147603
2021-08-08 23:53:29,487   eval_loss = 0.8970254959611149
2021-08-08 23:53:29,487   global_step = 15599
2021-08-08 23:53:29,487   loss = 0.07828456606147603
2021-08-08 23:53:29,487   rep_loss = 0.0
2021-08-08 23:53:29,488 ***** Save model *****
2021-08-08 23:53:51,480 Writing example 0 of 9832
2021-08-08 23:53:51,481 *** Example ***
2021-08-08 23:53:51,481 guid: dev_matched-0
2021-08-08 23:53:51,481 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:53:51,481 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:53:51,481 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:53:51,481 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:53:51,481 label: contradiction
2021-08-08 23:53:51,482 label_id: 0
2021-08-08 23:53:56,059 ***** Running mm evaluation *****
2021-08-08 23:53:56,059   Num examples = 9832
2021-08-08 23:53:56,059   Batch size = 32
2021-08-08 23:54:08,475 ***** Eval results *****
2021-08-08 23:54:08,476   acc = 0.47406427990235966
2021-08-08 23:54:08,476   eval_loss = 0.8970254959611149
2021-08-08 23:54:08,476   global_step = 15599
2021-08-08 23:54:46,601 ***** Running evaluation *****
2021-08-08 23:54:46,602   Epoch = 1 iter 15799 step
2021-08-08 23:54:46,602   Num examples = 9832
2021-08-08 23:54:46,602   Batch size = 32
2021-08-08 23:54:57,424 ***** Eval results *****
2021-08-08 23:54:57,424   acc = 0.44629780309194467
2021-08-08 23:54:57,424   att_loss = 0.0
2021-08-08 23:54:57,424   cls_loss = 0.07828498955654062
2021-08-08 23:54:57,424   eval_loss = 0.9090227194420704
2021-08-08 23:54:57,424   global_step = 15799
2021-08-08 23:54:57,424   loss = 0.07828498955654062
2021-08-08 23:54:57,424   rep_loss = 0.0
2021-08-08 23:54:57,424 ***** Save model *****
2021-08-08 23:55:03,594 Writing example 0 of 9832
2021-08-08 23:55:03,594 *** Example ***
2021-08-08 23:55:03,594 guid: dev_matched-0
2021-08-08 23:55:03,594 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:55:03,594 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:55:03,594 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:55:03,595 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:55:03,595 label: contradiction
2021-08-08 23:55:03,595 label_id: 0
2021-08-08 23:55:08,150 ***** Running mm evaluation *****
2021-08-08 23:55:08,150   Num examples = 9832
2021-08-08 23:55:08,150   Batch size = 32
2021-08-08 23:55:19,007 ***** Eval results *****
2021-08-08 23:55:19,007   acc = 0.44629780309194467
2021-08-08 23:55:19,007   eval_loss = 0.9090227194420704
2021-08-08 23:55:19,007   global_step = 15799
2021-08-08 23:55:57,125 ***** Running evaluation *****
2021-08-08 23:55:57,125   Epoch = 1 iter 15999 step
2021-08-08 23:55:57,125   Num examples = 9832
2021-08-08 23:55:57,125   Batch size = 32
2021-08-08 23:56:07,944 ***** Eval results *****
2021-08-08 23:56:07,945   acc = 0.46745321399511797
2021-08-08 23:56:07,945   att_loss = 0.0
2021-08-08 23:56:07,945   cls_loss = 0.07828607511723175
2021-08-08 23:56:07,945   eval_loss = 0.8984361703132654
2021-08-08 23:56:07,945   global_step = 15999
2021-08-08 23:56:07,945   loss = 0.07828607511723175
2021-08-08 23:56:07,945   rep_loss = 0.0
2021-08-08 23:56:07,945 ***** Save model *****
2021-08-08 23:56:09,041 Writing example 0 of 9832
2021-08-08 23:56:09,042 *** Example ***
2021-08-08 23:56:09,042 guid: dev_matched-0
2021-08-08 23:56:09,042 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:56:09,042 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:56:09,042 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:56:09,042 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:56:09,042 label: contradiction
2021-08-08 23:56:09,042 label_id: 0
2021-08-08 23:56:13,614 ***** Running mm evaluation *****
2021-08-08 23:56:13,614   Num examples = 9832
2021-08-08 23:56:13,614   Batch size = 32
2021-08-08 23:56:24,455 ***** Eval results *****
2021-08-08 23:56:24,455   acc = 0.46745321399511797
2021-08-08 23:56:24,455   eval_loss = 0.8984361703132654
2021-08-08 23:56:24,455   global_step = 15999
2021-08-08 23:57:00,834 ***** Running evaluation *****
2021-08-08 23:57:00,835   Epoch = 1 iter 16199 step
2021-08-08 23:57:00,835   Num examples = 9832
2021-08-08 23:57:00,835   Batch size = 32
2021-08-08 23:57:13,308 ***** Eval results *****
2021-08-08 23:57:13,308   acc = 0.47243694060211555
2021-08-08 23:57:13,309   att_loss = 0.0
2021-08-08 23:57:13,309   cls_loss = 0.07828627855968745
2021-08-08 23:57:13,309   eval_loss = 0.9019019220556531
2021-08-08 23:57:13,309   global_step = 16199
2021-08-08 23:57:13,309   loss = 0.07828627855968745
2021-08-08 23:57:13,309   rep_loss = 0.0
2021-08-08 23:57:13,309 ***** Save model *****
2021-08-08 23:57:14,706 Writing example 0 of 9832
2021-08-08 23:57:14,707 *** Example ***
2021-08-08 23:57:14,707 guid: dev_matched-0
2021-08-08 23:57:14,707 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:57:14,707 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:57:14,707 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:57:14,707 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:57:14,707 label: contradiction
2021-08-08 23:57:14,707 label_id: 0
2021-08-08 23:57:19,266 ***** Running mm evaluation *****
2021-08-08 23:57:19,267   Num examples = 9832
2021-08-08 23:57:19,267   Batch size = 32
2021-08-08 23:57:30,101 ***** Eval results *****
2021-08-08 23:57:30,101   acc = 0.47243694060211555
2021-08-08 23:57:30,101   eval_loss = 0.9019019220556531
2021-08-08 23:57:30,102   global_step = 16199
2021-08-08 23:58:08,184 ***** Running evaluation *****
2021-08-08 23:58:08,184   Epoch = 1 iter 16399 step
2021-08-08 23:58:08,184   Num examples = 9832
2021-08-08 23:58:08,184   Batch size = 32
2021-08-08 23:58:19,035 ***** Eval results *****
2021-08-08 23:58:19,035   acc = 0.463079739625712
2021-08-08 23:58:19,035   att_loss = 0.0
2021-08-08 23:58:19,035   cls_loss = 0.07828629874702571
2021-08-08 23:58:19,035   eval_loss = 0.9071877621985102
2021-08-08 23:58:19,035   global_step = 16399
2021-08-08 23:58:19,035   loss = 0.07828629874702571
2021-08-08 23:58:19,035   rep_loss = 0.0
2021-08-08 23:58:19,036 ***** Save model *****
2021-08-08 23:58:19,886 Writing example 0 of 9832
2021-08-08 23:58:19,887 *** Example ***
2021-08-08 23:58:19,887 guid: dev_matched-0
2021-08-08 23:58:19,887 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:58:19,887 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:58:19,887 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:58:19,887 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:58:19,887 label: contradiction
2021-08-08 23:58:19,887 label_id: 0
2021-08-08 23:58:24,508 ***** Running mm evaluation *****
2021-08-08 23:58:24,508   Num examples = 9832
2021-08-08 23:58:24,508   Batch size = 32
2021-08-08 23:58:35,395 ***** Eval results *****
2021-08-08 23:58:35,395   acc = 0.463079739625712
2021-08-08 23:58:35,395   eval_loss = 0.9071877621985102
2021-08-08 23:58:35,395   global_step = 16399
2021-08-08 23:59:13,564 ***** Running evaluation *****
2021-08-08 23:59:13,564   Epoch = 1 iter 16599 step
2021-08-08 23:59:13,564   Num examples = 9832
2021-08-08 23:59:13,565   Batch size = 32
2021-08-08 23:59:24,422 ***** Eval results *****
2021-08-08 23:59:24,422   acc = 0.45931651749389746
2021-08-08 23:59:24,422   att_loss = 0.0
2021-08-08 23:59:24,422   cls_loss = 0.07828421426655453
2021-08-08 23:59:24,422   eval_loss = 0.9055638677114016
2021-08-08 23:59:24,422   global_step = 16599
2021-08-08 23:59:24,422   loss = 0.07828421426655453
2021-08-08 23:59:24,422   rep_loss = 0.0
2021-08-08 23:59:24,423 ***** Save model *****
2021-08-08 23:59:50,250 Writing example 0 of 9832
2021-08-08 23:59:50,250 *** Example ***
2021-08-08 23:59:50,251 guid: dev_matched-0
2021-08-08 23:59:50,251 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-08 23:59:50,251 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:59:50,251 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:59:50,251 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-08 23:59:50,251 label: contradiction
2021-08-08 23:59:50,251 label_id: 0
2021-08-08 23:59:54,811 ***** Running mm evaluation *****
2021-08-08 23:59:54,811   Num examples = 9832
2021-08-08 23:59:54,811   Batch size = 32
2021-08-09 00:00:05,659 ***** Eval results *****
2021-08-09 00:00:05,659   acc = 0.45931651749389746
2021-08-09 00:00:05,659   eval_loss = 0.9055638677114016
2021-08-09 00:00:05,659   global_step = 16599
2021-08-09 00:00:43,719 ***** Running evaluation *****
2021-08-09 00:00:43,720   Epoch = 1 iter 16799 step
2021-08-09 00:00:43,720   Num examples = 9832
2021-08-09 00:00:43,720   Batch size = 32
2021-08-09 00:00:54,549 ***** Eval results *****
2021-08-09 00:00:54,549   acc = 0.46175752644426366
2021-08-09 00:00:54,549   att_loss = 0.0
2021-08-09 00:00:54,549   cls_loss = 0.07828366076887291
2021-08-09 00:00:54,549   eval_loss = 0.9076740571430751
2021-08-09 00:00:54,549   global_step = 16799
2021-08-09 00:00:54,549   loss = 0.07828366076887291
2021-08-09 00:00:54,549   rep_loss = 0.0
2021-08-09 00:00:54,549 ***** Save model *****
2021-08-09 00:00:55,949 Writing example 0 of 9832
2021-08-09 00:00:55,949 *** Example ***
2021-08-09 00:00:55,950 guid: dev_matched-0
2021-08-09 00:00:55,950 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:00:55,950 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:00:55,950 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:00:55,950 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:00:55,950 label: contradiction
2021-08-09 00:00:55,950 label_id: 0
2021-08-09 00:01:00,639 ***** Running mm evaluation *****
2021-08-09 00:01:00,639   Num examples = 9832
2021-08-09 00:01:00,639   Batch size = 32
2021-08-09 00:01:11,505 ***** Eval results *****
2021-08-09 00:01:11,505   acc = 0.46175752644426366
2021-08-09 00:01:11,505   eval_loss = 0.9076740571430751
2021-08-09 00:01:11,505   global_step = 16799
2021-08-09 00:01:49,684 ***** Running evaluation *****
2021-08-09 00:01:49,684   Epoch = 1 iter 16999 step
2021-08-09 00:01:49,684   Num examples = 9832
2021-08-09 00:01:49,684   Batch size = 32
2021-08-09 00:02:00,533 ***** Eval results *****
2021-08-09 00:02:00,533   acc = 0.4600284784377543
2021-08-09 00:02:00,534   att_loss = 0.0
2021-08-09 00:02:00,534   cls_loss = 0.07828372302265685
2021-08-09 00:02:00,534   eval_loss = 0.9087413801001264
2021-08-09 00:02:00,534   global_step = 16999
2021-08-09 00:02:00,534   loss = 0.07828372302265685
2021-08-09 00:02:00,534   rep_loss = 0.0
2021-08-09 00:02:00,534 ***** Save model *****
2021-08-09 00:02:01,360 Writing example 0 of 9832
2021-08-09 00:02:01,361 *** Example ***
2021-08-09 00:02:01,361 guid: dev_matched-0
2021-08-09 00:02:01,361 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:02:01,361 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:02:01,361 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:02:01,362 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:02:01,362 label: contradiction
2021-08-09 00:02:01,362 label_id: 0
2021-08-09 00:02:05,923 ***** Running mm evaluation *****
2021-08-09 00:02:05,923   Num examples = 9832
2021-08-09 00:02:05,923   Batch size = 32
2021-08-09 00:02:16,776 ***** Eval results *****
2021-08-09 00:02:16,777   acc = 0.4600284784377543
2021-08-09 00:02:16,777   eval_loss = 0.9087413801001264
2021-08-09 00:02:16,777   global_step = 16999
2021-08-09 00:02:54,882 ***** Running evaluation *****
2021-08-09 00:02:54,882   Epoch = 1 iter 17199 step
2021-08-09 00:02:54,882   Num examples = 9832
2021-08-09 00:02:54,882   Batch size = 32
2021-08-09 00:03:05,724 ***** Eval results *****
2021-08-09 00:03:05,724   acc = 0.443246541903987
2021-08-09 00:03:05,724   att_loss = 0.0
2021-08-09 00:03:05,724   cls_loss = 0.07828407645006488
2021-08-09 00:03:05,725   eval_loss = 0.919260359816737
2021-08-09 00:03:05,725   global_step = 17199
2021-08-09 00:03:05,725   loss = 0.07828407645006488
2021-08-09 00:03:05,725   rep_loss = 0.0
2021-08-09 00:03:05,726 ***** Save model *****
2021-08-09 00:03:06,600 Writing example 0 of 9832
2021-08-09 00:03:06,600 *** Example ***
2021-08-09 00:03:06,600 guid: dev_matched-0
2021-08-09 00:03:06,600 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:03:06,600 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:03:06,601 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:03:06,601 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:03:06,601 label: contradiction
2021-08-09 00:03:06,601 label_id: 0
2021-08-09 00:03:12,698 ***** Running mm evaluation *****
2021-08-09 00:03:12,698   Num examples = 9832
2021-08-09 00:03:12,698   Batch size = 32
2021-08-09 00:03:23,735 ***** Eval results *****
2021-08-09 00:03:23,735   acc = 0.443246541903987
2021-08-09 00:03:23,735   eval_loss = 0.919260359816737
2021-08-09 00:03:23,735   global_step = 17199
2021-08-09 00:04:01,966 ***** Running evaluation *****
2021-08-09 00:04:01,966   Epoch = 1 iter 17399 step
2021-08-09 00:04:01,966   Num examples = 9832
2021-08-09 00:04:01,966   Batch size = 32
2021-08-09 00:04:12,821 ***** Eval results *****
2021-08-09 00:04:12,821   acc = 0.45514646053702196
2021-08-09 00:04:12,821   att_loss = 0.0
2021-08-09 00:04:12,821   cls_loss = 0.07828471620404531
2021-08-09 00:04:12,821   eval_loss = 0.904088257001592
2021-08-09 00:04:12,821   global_step = 17399
2021-08-09 00:04:12,821   loss = 0.07828471620404531
2021-08-09 00:04:12,822   rep_loss = 0.0
2021-08-09 00:04:12,822 ***** Save model *****
2021-08-09 00:04:14,513 Writing example 0 of 9832
2021-08-09 00:04:14,513 *** Example ***
2021-08-09 00:04:14,514 guid: dev_matched-0
2021-08-09 00:04:14,514 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:04:14,514 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:04:14,514 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:04:14,514 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:04:14,514 label: contradiction
2021-08-09 00:04:14,514 label_id: 0
2021-08-09 00:04:19,095 ***** Running mm evaluation *****
2021-08-09 00:04:19,095   Num examples = 9832
2021-08-09 00:04:19,095   Batch size = 32
2021-08-09 00:04:29,954 ***** Eval results *****
2021-08-09 00:04:29,955   acc = 0.45514646053702196
2021-08-09 00:04:29,955   eval_loss = 0.904088257001592
2021-08-09 00:04:29,955   global_step = 17399
2021-08-09 00:05:06,269 ***** Running evaluation *****
2021-08-09 00:05:06,269   Epoch = 1 iter 17599 step
2021-08-09 00:05:06,269   Num examples = 9832
2021-08-09 00:05:06,269   Batch size = 32
2021-08-09 00:05:17,083 ***** Eval results *****
2021-08-09 00:05:17,083   acc = 0.4565703824247356
2021-08-09 00:05:17,083   att_loss = 0.0
2021-08-09 00:05:17,083   cls_loss = 0.07828521397999562
2021-08-09 00:05:17,083   eval_loss = 0.904436291038216
2021-08-09 00:05:17,083   global_step = 17599
2021-08-09 00:05:17,083   loss = 0.07828521397999562
2021-08-09 00:05:17,083   rep_loss = 0.0
2021-08-09 00:05:17,084 ***** Save model *****
2021-08-09 00:05:19,086 Writing example 0 of 9832
2021-08-09 00:05:19,086 *** Example ***
2021-08-09 00:05:19,086 guid: dev_matched-0
2021-08-09 00:05:19,086 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:05:19,087 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:05:19,087 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:05:19,087 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:05:19,087 label: contradiction
2021-08-09 00:05:19,087 label_id: 0
2021-08-09 00:05:23,702 ***** Running mm evaluation *****
2021-08-09 00:05:23,703   Num examples = 9832
2021-08-09 00:05:23,703   Batch size = 32
2021-08-09 00:05:36,128 ***** Eval results *****
2021-08-09 00:05:36,129   acc = 0.4565703824247356
2021-08-09 00:05:36,129   eval_loss = 0.904436291038216
2021-08-09 00:05:36,129   global_step = 17599
2021-08-09 00:06:12,531 ***** Running evaluation *****
2021-08-09 00:06:12,531   Epoch = 1 iter 17799 step
2021-08-09 00:06:12,531   Num examples = 9832
2021-08-09 00:06:12,531   Batch size = 32
2021-08-09 00:06:23,329 ***** Eval results *****
2021-08-09 00:06:23,329   acc = 0.4553498779495525
2021-08-09 00:06:23,329   att_loss = 0.0
2021-08-09 00:06:23,329   cls_loss = 0.07828549398805214
2021-08-09 00:06:23,329   eval_loss = 0.9025713882663033
2021-08-09 00:06:23,329   global_step = 17799
2021-08-09 00:06:23,329   loss = 0.07828549398805214
2021-08-09 00:06:23,329   rep_loss = 0.0
2021-08-09 00:06:23,329 ***** Save model *****
2021-08-09 00:06:25,439 Writing example 0 of 9832
2021-08-09 00:06:25,440 *** Example ***
2021-08-09 00:06:25,440 guid: dev_matched-0
2021-08-09 00:06:25,440 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:06:25,440 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:06:25,440 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:06:25,440 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:06:25,440 label: contradiction
2021-08-09 00:06:25,440 label_id: 0
2021-08-09 00:06:30,051 ***** Running mm evaluation *****
2021-08-09 00:06:30,051   Num examples = 9832
2021-08-09 00:06:30,051   Batch size = 32
2021-08-09 00:06:42,665 ***** Eval results *****
2021-08-09 00:06:42,665   acc = 0.4553498779495525
2021-08-09 00:06:42,666   eval_loss = 0.9025713882663033
2021-08-09 00:06:42,666   global_step = 17799
2021-08-09 00:07:19,227 ***** Running evaluation *****
2021-08-09 00:07:19,228   Epoch = 1 iter 17999 step
2021-08-09 00:07:19,228   Num examples = 9832
2021-08-09 00:07:19,228   Batch size = 32
2021-08-09 00:07:31,668 ***** Eval results *****
2021-08-09 00:07:31,668   acc = 0.439686737184703
2021-08-09 00:07:31,668   att_loss = 0.0
2021-08-09 00:07:31,668   cls_loss = 0.0782857602246398
2021-08-09 00:07:31,668   eval_loss = 0.9142638860971897
2021-08-09 00:07:31,668   global_step = 17999
2021-08-09 00:07:31,668   loss = 0.0782857602246398
2021-08-09 00:07:31,668   rep_loss = 0.0
2021-08-09 00:07:31,669 ***** Save model *****
2021-08-09 00:07:32,901 Writing example 0 of 9832
2021-08-09 00:07:32,902 *** Example ***
2021-08-09 00:07:32,902 guid: dev_matched-0
2021-08-09 00:07:32,902 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:07:32,902 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:07:32,902 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:07:32,902 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:07:32,902 label: contradiction
2021-08-09 00:07:32,902 label_id: 0
2021-08-09 00:07:37,460 ***** Running mm evaluation *****
2021-08-09 00:07:37,461   Num examples = 9832
2021-08-09 00:07:37,461   Batch size = 32
2021-08-09 00:07:48,314 ***** Eval results *****
2021-08-09 00:07:48,314   acc = 0.439686737184703
2021-08-09 00:07:48,314   eval_loss = 0.9142638860971897
2021-08-09 00:07:48,314   global_step = 17999
2021-08-09 00:08:24,716 ***** Running evaluation *****
2021-08-09 00:08:24,716   Epoch = 1 iter 18199 step
2021-08-09 00:08:24,717   Num examples = 9832
2021-08-09 00:08:24,717   Batch size = 32
2021-08-09 00:08:36,015 ***** Eval results *****
2021-08-09 00:08:36,015   acc = 0.4382628152969894
2021-08-09 00:08:36,015   att_loss = 0.0
2021-08-09 00:08:36,015   cls_loss = 0.07828481693816368
2021-08-09 00:08:36,015   eval_loss = 0.9198011365029719
2021-08-09 00:08:36,015   global_step = 18199
2021-08-09 00:08:36,015   loss = 0.07828481693816368
2021-08-09 00:08:36,015   rep_loss = 0.0
2021-08-09 00:08:36,155 ***** Save model *****
2021-08-09 00:08:39,771 Writing example 0 of 9832
2021-08-09 00:08:39,772 *** Example ***
2021-08-09 00:08:39,772 guid: dev_matched-0
2021-08-09 00:08:39,772 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:08:39,772 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:08:39,772 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:08:39,772 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:08:39,773 label: contradiction
2021-08-09 00:08:39,773 label_id: 0
2021-08-09 00:08:44,368 ***** Running mm evaluation *****
2021-08-09 00:08:44,369   Num examples = 9832
2021-08-09 00:08:44,369   Batch size = 32
2021-08-09 00:08:55,200 ***** Eval results *****
2021-08-09 00:08:55,201   acc = 0.4382628152969894
2021-08-09 00:08:55,201   eval_loss = 0.9198011365029719
2021-08-09 00:08:55,201   global_step = 18199
2021-08-09 00:09:33,271 ***** Running evaluation *****
2021-08-09 00:09:33,271   Epoch = 1 iter 18399 step
2021-08-09 00:09:33,271   Num examples = 9832
2021-08-09 00:09:33,271   Batch size = 32
2021-08-09 00:09:44,104 ***** Eval results *****
2021-08-09 00:09:44,104   acc = 0.43287225386493083
2021-08-09 00:09:44,104   att_loss = 0.0
2021-08-09 00:09:44,104   cls_loss = 0.07828553854468304
2021-08-09 00:09:44,105   eval_loss = 0.9256613051349466
2021-08-09 00:09:44,105   global_step = 18399
2021-08-09 00:09:44,105   loss = 0.07828553854468304
2021-08-09 00:09:44,105   rep_loss = 0.0
2021-08-09 00:09:44,105 ***** Save model *****
2021-08-09 00:09:45,474 Writing example 0 of 9832
2021-08-09 00:09:45,475 *** Example ***
2021-08-09 00:09:45,475 guid: dev_matched-0
2021-08-09 00:09:45,475 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:09:45,475 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:09:45,475 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:09:45,475 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:09:45,475 label: contradiction
2021-08-09 00:09:45,476 label_id: 0
2021-08-09 00:09:50,039 ***** Running mm evaluation *****
2021-08-09 00:09:50,039   Num examples = 9832
2021-08-09 00:09:50,039   Batch size = 32
2021-08-09 00:10:02,590 ***** Eval results *****
2021-08-09 00:10:02,590   acc = 0.43287225386493083
2021-08-09 00:10:02,590   eval_loss = 0.9256613051349466
2021-08-09 00:10:02,590   global_step = 18399
2021-08-09 00:10:40,813 ***** Running evaluation *****
2021-08-09 00:10:40,813   Epoch = 1 iter 18599 step
2021-08-09 00:10:40,813   Num examples = 9832
2021-08-09 00:10:40,813   Batch size = 32
2021-08-09 00:10:51,627 ***** Eval results *****
2021-08-09 00:10:51,627   acc = 0.44629780309194467
2021-08-09 00:10:51,627   att_loss = 0.0
2021-08-09 00:10:51,627   cls_loss = 0.07828468214007016
2021-08-09 00:10:51,627   eval_loss = 0.9127938331334622
2021-08-09 00:10:51,627   global_step = 18599
2021-08-09 00:10:51,627   loss = 0.07828468214007016
2021-08-09 00:10:51,627   rep_loss = 0.0
2021-08-09 00:10:51,628 ***** Save model *****
2021-08-09 00:10:53,130 Writing example 0 of 9832
2021-08-09 00:10:53,130 *** Example ***
2021-08-09 00:10:53,130 guid: dev_matched-0
2021-08-09 00:10:53,130 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:10:53,131 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:10:53,131 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:10:53,131 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:10:53,131 label: contradiction
2021-08-09 00:10:53,131 label_id: 0
2021-08-09 00:10:57,796 ***** Running mm evaluation *****
2021-08-09 00:10:57,796   Num examples = 9832
2021-08-09 00:10:57,796   Batch size = 32
2021-08-09 00:11:08,678 ***** Eval results *****
2021-08-09 00:11:08,678   acc = 0.44629780309194467
2021-08-09 00:11:08,678   eval_loss = 0.9127938331334622
2021-08-09 00:11:08,678   global_step = 18599
2021-08-09 00:11:46,927 ***** Running evaluation *****
2021-08-09 00:11:46,927   Epoch = 1 iter 18799 step
2021-08-09 00:11:46,927   Num examples = 9832
2021-08-09 00:11:46,927   Batch size = 32
2021-08-09 00:11:57,759 ***** Eval results *****
2021-08-09 00:11:57,759   acc = 0.4497558991049634
2021-08-09 00:11:57,759   att_loss = 0.0
2021-08-09 00:11:57,759   cls_loss = 0.07828388969299366
2021-08-09 00:11:57,759   eval_loss = 0.9136482705543567
2021-08-09 00:11:57,759   global_step = 18799
2021-08-09 00:11:57,759   loss = 0.07828388969299366
2021-08-09 00:11:57,759   rep_loss = 0.0
2021-08-09 00:11:57,760 ***** Save model *****
2021-08-09 00:11:58,818 Writing example 0 of 9832
2021-08-09 00:11:58,818 *** Example ***
2021-08-09 00:11:58,819 guid: dev_matched-0
2021-08-09 00:11:58,819 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:11:58,819 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:11:58,819 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:11:58,819 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:11:58,819 label: contradiction
2021-08-09 00:11:58,819 label_id: 0
2021-08-09 00:12:03,371 ***** Running mm evaluation *****
2021-08-09 00:12:03,371   Num examples = 9832
2021-08-09 00:12:03,371   Batch size = 32
2021-08-09 00:12:14,949 ***** Eval results *****
2021-08-09 00:12:14,949   acc = 0.4497558991049634
2021-08-09 00:12:14,949   eval_loss = 0.9136482705543567
2021-08-09 00:12:14,949   global_step = 18799
2021-08-09 00:12:53,048 ***** Running evaluation *****
2021-08-09 00:12:53,048   Epoch = 1 iter 18999 step
2021-08-09 00:12:53,048   Num examples = 9832
2021-08-09 00:12:53,048   Batch size = 32
2021-08-09 00:13:03,872 ***** Eval results *****
2021-08-09 00:13:03,872   acc = 0.45829943043124494
2021-08-09 00:13:03,872   att_loss = 0.0
2021-08-09 00:13:03,872   cls_loss = 0.0782837659231434
2021-08-09 00:13:03,872   eval_loss = 0.9135810531579055
2021-08-09 00:13:03,872   global_step = 18999
2021-08-09 00:13:03,873   loss = 0.0782837659231434
2021-08-09 00:13:03,873   rep_loss = 0.0
2021-08-09 00:13:03,873 ***** Save model *****
2021-08-09 00:13:04,915 Writing example 0 of 9832
2021-08-09 00:13:04,915 *** Example ***
2021-08-09 00:13:04,915 guid: dev_matched-0
2021-08-09 00:13:04,916 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:13:04,916 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:13:04,916 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:13:04,916 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:13:04,916 label: contradiction
2021-08-09 00:13:04,916 label_id: 0
2021-08-09 00:13:09,482 ***** Running mm evaluation *****
2021-08-09 00:13:09,482   Num examples = 9832
2021-08-09 00:13:09,482   Batch size = 32
2021-08-09 00:13:20,329 ***** Eval results *****
2021-08-09 00:13:20,330   acc = 0.45829943043124494
2021-08-09 00:13:20,330   eval_loss = 0.9135810531579055
2021-08-09 00:13:20,330   global_step = 18999
2021-08-09 00:13:58,574 ***** Running evaluation *****
2021-08-09 00:13:58,575   Epoch = 1 iter 19199 step
2021-08-09 00:13:58,575   Num examples = 9832
2021-08-09 00:13:58,575   Batch size = 32
2021-08-09 00:14:09,428 ***** Eval results *****
2021-08-09 00:14:09,428   acc = 0.447213181448332
2021-08-09 00:14:09,428   att_loss = 0.0
2021-08-09 00:14:09,428   cls_loss = 0.07828372707447544
2021-08-09 00:14:09,428   eval_loss = 0.9142419121095112
2021-08-09 00:14:09,428   global_step = 19199
2021-08-09 00:14:09,428   loss = 0.07828372707447544
2021-08-09 00:14:09,428   rep_loss = 0.0
2021-08-09 00:14:09,429 ***** Save model *****
2021-08-09 00:14:10,434 Writing example 0 of 9832
2021-08-09 00:14:10,435 *** Example ***
2021-08-09 00:14:10,435 guid: dev_matched-0
2021-08-09 00:14:10,435 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:14:10,435 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:14:10,435 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:14:10,435 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:14:10,435 label: contradiction
2021-08-09 00:14:10,435 label_id: 0
2021-08-09 00:14:14,991 ***** Running mm evaluation *****
2021-08-09 00:14:14,992   Num examples = 9832
2021-08-09 00:14:14,992   Batch size = 32
2021-08-09 00:14:25,874 ***** Eval results *****
2021-08-09 00:14:25,874   acc = 0.447213181448332
2021-08-09 00:14:25,875   eval_loss = 0.9142419121095112
2021-08-09 00:14:25,875   global_step = 19199
2021-08-09 00:15:04,114 ***** Running evaluation *****
2021-08-09 00:15:04,114   Epoch = 1 iter 19399 step
2021-08-09 00:15:04,114   Num examples = 9832
2021-08-09 00:15:04,115   Batch size = 32
2021-08-09 00:15:14,971 ***** Eval results *****
2021-08-09 00:15:14,971   acc = 0.4481285598047193
2021-08-09 00:15:14,971   att_loss = 0.0
2021-08-09 00:15:14,971   cls_loss = 0.07828390252194249
2021-08-09 00:15:14,971   eval_loss = 0.9191942075630287
2021-08-09 00:15:14,971   global_step = 19399
2021-08-09 00:15:14,971   loss = 0.07828390252194249
2021-08-09 00:15:14,971   rep_loss = 0.0
2021-08-09 00:15:14,971 ***** Save model *****
2021-08-09 00:15:15,989 Writing example 0 of 9832
2021-08-09 00:15:15,989 *** Example ***
2021-08-09 00:15:15,989 guid: dev_matched-0
2021-08-09 00:15:15,989 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:15:15,990 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:15:15,990 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:15:15,990 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:15:15,990 label: contradiction
2021-08-09 00:15:15,990 label_id: 0
2021-08-09 00:15:22,062 ***** Running mm evaluation *****
2021-08-09 00:15:22,062   Num examples = 9832
2021-08-09 00:15:22,063   Batch size = 32
2021-08-09 00:15:32,943 ***** Eval results *****
2021-08-09 00:15:32,943   acc = 0.4481285598047193
2021-08-09 00:15:32,943   eval_loss = 0.9191942075630287
2021-08-09 00:15:32,943   global_step = 19399
2021-08-09 00:16:11,056 ***** Running evaluation *****
2021-08-09 00:16:11,057   Epoch = 1 iter 19599 step
2021-08-09 00:16:11,057   Num examples = 9832
2021-08-09 00:16:11,057   Batch size = 32
2021-08-09 00:16:21,885 ***** Eval results *****
2021-08-09 00:16:21,885   acc = 0.4467046379170057
2021-08-09 00:16:21,885   att_loss = 0.0
2021-08-09 00:16:21,885   cls_loss = 0.07828448038846929
2021-08-09 00:16:21,886   eval_loss = 0.911938081507559
2021-08-09 00:16:21,886   global_step = 19599
2021-08-09 00:16:21,886   loss = 0.07828448038846929
2021-08-09 00:16:21,886   rep_loss = 0.0
2021-08-09 00:16:21,886 ***** Save model *****
2021-08-09 00:16:23,089 Writing example 0 of 9832
2021-08-09 00:16:23,090 *** Example ***
2021-08-09 00:16:23,090 guid: dev_matched-0
2021-08-09 00:16:23,090 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:16:23,090 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:16:23,090 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:16:23,090 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:16:23,090 label: contradiction
2021-08-09 00:16:23,090 label_id: 0
2021-08-09 00:16:27,730 ***** Running mm evaluation *****
2021-08-09 00:16:27,731   Num examples = 9832
2021-08-09 00:16:27,731   Batch size = 32
2021-08-09 00:16:38,577 ***** Eval results *****
2021-08-09 00:16:38,578   acc = 0.4467046379170057
2021-08-09 00:16:38,578   eval_loss = 0.911938081507559
2021-08-09 00:16:38,578   global_step = 19599
2021-08-09 00:17:14,947 ***** Running evaluation *****
2021-08-09 00:17:14,948   Epoch = 1 iter 19799 step
2021-08-09 00:17:14,948   Num examples = 9832
2021-08-09 00:17:14,948   Batch size = 32
2021-08-09 00:17:25,765 ***** Eval results *****
2021-08-09 00:17:25,765   acc = 0.44650122050447516
2021-08-09 00:17:25,765   att_loss = 0.0
2021-08-09 00:17:25,765   cls_loss = 0.07828507618400751
2021-08-09 00:17:25,765   eval_loss = 0.9155723752526493
2021-08-09 00:17:25,765   global_step = 19799
2021-08-09 00:17:25,765   loss = 0.07828507618400751
2021-08-09 00:17:25,765   rep_loss = 0.0
2021-08-09 00:17:25,765 ***** Save model *****
2021-08-09 00:17:40,867 Writing example 0 of 9832
2021-08-09 00:17:40,868 *** Example ***
2021-08-09 00:17:40,868 guid: dev_matched-0
2021-08-09 00:17:40,868 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:17:40,868 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:17:40,868 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:17:40,868 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:17:40,868 label: contradiction
2021-08-09 00:17:40,868 label_id: 0
2021-08-09 00:17:45,421 ***** Running mm evaluation *****
2021-08-09 00:17:45,421   Num examples = 9832
2021-08-09 00:17:45,421   Batch size = 32
2021-08-09 00:17:57,904 ***** Eval results *****
2021-08-09 00:17:57,904   acc = 0.44650122050447516
2021-08-09 00:17:57,904   eval_loss = 0.9155723752526493
2021-08-09 00:17:57,904   global_step = 19799
2021-08-09 00:18:34,339 ***** Running evaluation *****
2021-08-09 00:18:34,339   Epoch = 1 iter 19999 step
2021-08-09 00:18:34,339   Num examples = 9832
2021-08-09 00:18:34,339   Batch size = 32
2021-08-09 00:18:45,152 ***** Eval results *****
2021-08-09 00:18:45,152   acc = 0.439686737184703
2021-08-09 00:18:45,152   att_loss = 0.0
2021-08-09 00:18:45,152   cls_loss = 0.07828522354657161
2021-08-09 00:18:45,153   eval_loss = 0.9202285656681308
2021-08-09 00:18:45,153   global_step = 19999
2021-08-09 00:18:45,153   loss = 0.07828522354657161
2021-08-09 00:18:45,153   rep_loss = 0.0
2021-08-09 00:18:45,153 ***** Save model *****
2021-08-09 00:18:46,309 Writing example 0 of 9832
2021-08-09 00:18:46,310 *** Example ***
2021-08-09 00:18:46,310 guid: dev_matched-0
2021-08-09 00:18:46,310 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:18:46,310 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:18:46,310 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:18:46,310 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:18:46,310 label: contradiction
2021-08-09 00:18:46,310 label_id: 0
2021-08-09 00:18:50,867 ***** Running mm evaluation *****
2021-08-09 00:18:50,867   Num examples = 9832
2021-08-09 00:18:50,867   Batch size = 32
2021-08-09 00:19:03,379 ***** Eval results *****
2021-08-09 00:19:03,379   acc = 0.439686737184703
2021-08-09 00:19:03,379   eval_loss = 0.9202285656681308
2021-08-09 00:19:03,380   global_step = 19999
2021-08-09 00:19:39,705 ***** Running evaluation *****
2021-08-09 00:19:39,705   Epoch = 1 iter 20199 step
2021-08-09 00:19:39,706   Num examples = 9832
2021-08-09 00:19:39,706   Batch size = 32
2021-08-09 00:19:52,583 ***** Eval results *****
2021-08-09 00:19:52,583   acc = 0.4406021155410903
2021-08-09 00:19:52,583   att_loss = 0.0
2021-08-09 00:19:52,583   cls_loss = 0.07828424011457419
2021-08-09 00:19:52,583   eval_loss = 0.9211728692828834
2021-08-09 00:19:52,583   global_step = 20199
2021-08-09 00:19:52,583   loss = 0.07828424011457419
2021-08-09 00:19:52,583   rep_loss = 0.0
2021-08-09 00:19:52,644 ***** Save model *****
2021-08-09 00:19:54,226 Writing example 0 of 9832
2021-08-09 00:19:54,226 *** Example ***
2021-08-09 00:19:54,227 guid: dev_matched-0
2021-08-09 00:19:54,227 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:19:54,227 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:19:54,227 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:19:54,227 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:19:54,227 label: contradiction
2021-08-09 00:19:54,227 label_id: 0
2021-08-09 00:19:58,804 ***** Running mm evaluation *****
2021-08-09 00:19:58,804   Num examples = 9832
2021-08-09 00:19:58,804   Batch size = 32
2021-08-09 00:20:09,756 ***** Eval results *****
2021-08-09 00:20:09,756   acc = 0.4406021155410903
2021-08-09 00:20:09,756   eval_loss = 0.9211728692828834
2021-08-09 00:20:09,756   global_step = 20199
2021-08-09 00:20:46,188 ***** Running evaluation *****
2021-08-09 00:20:46,188   Epoch = 1 iter 20399 step
2021-08-09 00:20:46,188   Num examples = 9832
2021-08-09 00:20:46,188   Batch size = 32
2021-08-09 00:20:57,053 ***** Eval results *****
2021-08-09 00:20:57,053   acc = 0.43683889340927584
2021-08-09 00:20:57,053   att_loss = 0.0
2021-08-09 00:20:57,053   cls_loss = 0.07828375871337703
2021-08-09 00:20:57,053   eval_loss = 0.9274560636901236
2021-08-09 00:20:57,053   global_step = 20399
2021-08-09 00:20:57,053   loss = 0.07828375871337703
2021-08-09 00:20:57,053   rep_loss = 0.0
2021-08-09 00:20:57,054 ***** Save model *****
2021-08-09 00:20:57,906 Writing example 0 of 9832
2021-08-09 00:20:57,907 *** Example ***
2021-08-09 00:20:57,907 guid: dev_matched-0
2021-08-09 00:20:57,907 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:20:57,907 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:20:57,907 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:20:57,907 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:20:57,907 label: contradiction
2021-08-09 00:20:57,907 label_id: 0
2021-08-09 00:21:02,560 ***** Running mm evaluation *****
2021-08-09 00:21:02,560   Num examples = 9832
2021-08-09 00:21:02,560   Batch size = 32
2021-08-09 00:21:15,082 ***** Eval results *****
2021-08-09 00:21:15,082   acc = 0.43683889340927584
2021-08-09 00:21:15,082   eval_loss = 0.9274560636901236
2021-08-09 00:21:15,082   global_step = 20399
2021-08-09 00:21:51,464 ***** Running evaluation *****
2021-08-09 00:21:51,465   Epoch = 1 iter 20599 step
2021-08-09 00:21:51,465   Num examples = 9832
2021-08-09 00:21:51,465   Batch size = 32
2021-08-09 00:22:02,306 ***** Eval results *****
2021-08-09 00:22:02,306   acc = 0.4387713588283157
2021-08-09 00:22:02,306   att_loss = 0.0
2021-08-09 00:22:02,306   cls_loss = 0.07828357457238347
2021-08-09 00:22:02,307   eval_loss = 0.9208914771095499
2021-08-09 00:22:02,307   global_step = 20599
2021-08-09 00:22:02,307   loss = 0.07828357457238347
2021-08-09 00:22:02,307   rep_loss = 0.0
2021-08-09 00:22:02,307 ***** Save model *****
2021-08-09 00:22:04,618 Writing example 0 of 9832
2021-08-09 00:22:04,619 *** Example ***
2021-08-09 00:22:04,619 guid: dev_matched-0
2021-08-09 00:22:04,619 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:22:04,619 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:22:04,619 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:22:04,619 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:22:04,619 label: contradiction
2021-08-09 00:22:04,619 label_id: 0
2021-08-09 00:22:09,171 ***** Running mm evaluation *****
2021-08-09 00:22:09,172   Num examples = 9832
2021-08-09 00:22:09,172   Batch size = 32
2021-08-09 00:22:20,047 ***** Eval results *****
2021-08-09 00:22:20,047   acc = 0.4387713588283157
2021-08-09 00:22:20,047   eval_loss = 0.9208914771095499
2021-08-09 00:22:20,047   global_step = 20599
2021-08-09 00:22:58,247 ***** Running evaluation *****
2021-08-09 00:22:58,247   Epoch = 1 iter 20799 step
2021-08-09 00:22:58,248   Num examples = 9832
2021-08-09 00:22:58,248   Batch size = 32
2021-08-09 00:23:09,091 ***** Eval results *****
2021-08-09 00:23:09,091   acc = 0.42727827502034177
2021-08-09 00:23:09,092   att_loss = 0.0
2021-08-09 00:23:09,092   cls_loss = 0.07828325756693161
2021-08-09 00:23:09,092   eval_loss = 0.925774450813021
2021-08-09 00:23:09,092   global_step = 20799
2021-08-09 00:23:09,092   loss = 0.07828325756693161
2021-08-09 00:23:09,092   rep_loss = 0.0
2021-08-09 00:23:09,093 ***** Save model *****
2021-08-09 00:23:16,814 Writing example 0 of 9832
2021-08-09 00:23:16,815 *** Example ***
2021-08-09 00:23:16,815 guid: dev_matched-0
2021-08-09 00:23:16,815 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:23:16,815 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:23:16,815 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:23:16,815 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:23:16,815 label: contradiction
2021-08-09 00:23:16,815 label_id: 0
2021-08-09 00:23:21,367 ***** Running mm evaluation *****
2021-08-09 00:23:21,367   Num examples = 9832
2021-08-09 00:23:21,367   Batch size = 32
2021-08-09 00:23:32,246 ***** Eval results *****
2021-08-09 00:23:32,246   acc = 0.42727827502034177
2021-08-09 00:23:32,246   eval_loss = 0.925774450813021
2021-08-09 00:23:32,246   global_step = 20799
2021-08-09 00:24:10,502 ***** Running evaluation *****
2021-08-09 00:24:10,502   Epoch = 1 iter 20999 step
2021-08-09 00:24:10,502   Num examples = 9832
2021-08-09 00:24:10,502   Batch size = 32
2021-08-09 00:24:21,335 ***** Eval results *****
2021-08-09 00:24:21,335   acc = 0.419446704637917
2021-08-09 00:24:21,335   att_loss = 0.0
2021-08-09 00:24:21,335   cls_loss = 0.07828376739946719
2021-08-09 00:24:21,335   eval_loss = 0.9303164383420697
2021-08-09 00:24:21,335   global_step = 20999
2021-08-09 00:24:21,335   loss = 0.07828376739946719
2021-08-09 00:24:21,335   rep_loss = 0.0
2021-08-09 00:24:21,336 ***** Save model *****
2021-08-09 00:24:22,658 Writing example 0 of 9832
2021-08-09 00:24:22,659 *** Example ***
2021-08-09 00:24:22,659 guid: dev_matched-0
2021-08-09 00:24:22,659 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:24:22,659 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:24:22,659 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:24:22,659 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:24:22,659 label: contradiction
2021-08-09 00:24:22,660 label_id: 0
2021-08-09 00:24:27,286 ***** Running mm evaluation *****
2021-08-09 00:24:27,286   Num examples = 9832
2021-08-09 00:24:27,286   Batch size = 32
2021-08-09 00:24:38,110 ***** Eval results *****
2021-08-09 00:24:38,110   acc = 0.419446704637917
2021-08-09 00:24:38,110   eval_loss = 0.9303164383420697
2021-08-09 00:24:38,110   global_step = 20999
2021-08-09 00:25:14,552 ***** Running evaluation *****
2021-08-09 00:25:14,553   Epoch = 2 iter 21199 step
2021-08-09 00:25:14,553   Num examples = 9832
2021-08-09 00:25:14,553   Batch size = 32
2021-08-09 00:25:25,371 ***** Eval results *****
2021-08-09 00:25:25,372   acc = 0.4254475183075671
2021-08-09 00:25:25,372   att_loss = 0.0
2021-08-09 00:25:25,372   cls_loss = 0.07824751916858885
2021-08-09 00:25:25,372   eval_loss = 0.9377604752004921
2021-08-09 00:25:25,372   global_step = 21199
2021-08-09 00:25:25,372   loss = 0.07824751916858885
2021-08-09 00:25:25,372   rep_loss = 0.0
2021-08-09 00:25:25,372 ***** Save model *****
2021-08-09 00:25:28,703 Writing example 0 of 9832
2021-08-09 00:25:28,704 *** Example ***
2021-08-09 00:25:28,704 guid: dev_matched-0
2021-08-09 00:25:28,704 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:25:28,704 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:25:28,704 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:25:28,704 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:25:28,704 label: contradiction
2021-08-09 00:25:28,704 label_id: 0
2021-08-09 00:25:33,267 ***** Running mm evaluation *****
2021-08-09 00:25:33,267   Num examples = 9832
2021-08-09 00:25:33,267   Batch size = 32
2021-08-09 00:25:45,705 ***** Eval results *****
2021-08-09 00:25:45,705   acc = 0.4254475183075671
2021-08-09 00:25:45,705   eval_loss = 0.9377604752004921
2021-08-09 00:25:45,705   global_step = 21199
2021-08-09 00:26:22,210 ***** Running evaluation *****
2021-08-09 00:26:22,210   Epoch = 2 iter 21399 step
2021-08-09 00:26:22,210   Num examples = 9832
2021-08-09 00:26:22,210   Batch size = 32
2021-08-09 00:26:33,066 ***** Eval results *****
2021-08-09 00:26:33,066   acc = 0.41405614320585843
2021-08-09 00:26:33,066   att_loss = 0.0
2021-08-09 00:26:33,066   cls_loss = 0.0782503003504739
2021-08-09 00:26:33,066   eval_loss = 0.9476356732767898
2021-08-09 00:26:33,066   global_step = 21399
2021-08-09 00:26:33,066   loss = 0.0782503003504739
2021-08-09 00:26:33,066   rep_loss = 0.0
2021-08-09 00:26:33,067 ***** Save model *****
2021-08-09 00:26:34,098 Writing example 0 of 9832
2021-08-09 00:26:34,098 *** Example ***
2021-08-09 00:26:34,098 guid: dev_matched-0
2021-08-09 00:26:34,098 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:26:34,098 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:26:34,099 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:26:34,099 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:26:34,099 label: contradiction
2021-08-09 00:26:34,099 label_id: 0
2021-08-09 00:26:38,730 ***** Running mm evaluation *****
2021-08-09 00:26:38,730   Num examples = 9832
2021-08-09 00:26:38,730   Batch size = 32
2021-08-09 00:26:51,121 ***** Eval results *****
2021-08-09 00:26:51,121   acc = 0.41405614320585843
2021-08-09 00:26:51,121   eval_loss = 0.9476356732767898
2021-08-09 00:26:51,121   global_step = 21399
2021-08-09 00:27:27,603 ***** Running evaluation *****
2021-08-09 00:27:27,603   Epoch = 2 iter 21599 step
2021-08-09 00:27:27,603   Num examples = 9832
2021-08-09 00:27:27,603   Batch size = 32
2021-08-09 00:27:40,194 ***** Eval results *****
2021-08-09 00:27:40,194   acc = 0.4141578519121237
2021-08-09 00:27:40,195   att_loss = 0.0
2021-08-09 00:27:40,195   cls_loss = 0.07826310762456644
2021-08-09 00:27:40,195   eval_loss = 0.9423957546423007
2021-08-09 00:27:40,195   global_step = 21599
2021-08-09 00:27:40,195   loss = 0.07826310762456644
2021-08-09 00:27:40,195   rep_loss = 0.0
2021-08-09 00:27:40,196 ***** Save model *****
2021-08-09 00:27:45,620 Writing example 0 of 9832
2021-08-09 00:27:45,621 *** Example ***
2021-08-09 00:27:45,621 guid: dev_matched-0
2021-08-09 00:27:45,621 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:27:45,621 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:27:45,621 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:27:45,621 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:27:45,622 label: contradiction
2021-08-09 00:27:45,622 label_id: 0
2021-08-09 00:27:50,181 ***** Running mm evaluation *****
2021-08-09 00:27:50,181   Num examples = 9832
2021-08-09 00:27:50,181   Batch size = 32
2021-08-09 00:28:01,107 ***** Eval results *****
2021-08-09 00:28:01,107   acc = 0.4141578519121237
2021-08-09 00:28:01,107   eval_loss = 0.9423957546423007
2021-08-09 00:28:01,107   global_step = 21599
2021-08-09 00:28:38,535 ***** Running evaluation *****
2021-08-09 00:28:38,535   Epoch = 2 iter 21799 step
2021-08-09 00:28:38,535   Num examples = 9832
2021-08-09 00:28:38,535   Batch size = 32
2021-08-09 00:28:49,425 ***** Eval results *****
2021-08-09 00:28:49,425   acc = 0.4267697314890155
2021-08-09 00:28:49,425   att_loss = 0.0
2021-08-09 00:28:49,425   cls_loss = 0.07826317494012872
2021-08-09 00:28:49,426   eval_loss = 0.9320567691480959
2021-08-09 00:28:49,426   global_step = 21799
2021-08-09 00:28:49,426   loss = 0.07826317494012872
2021-08-09 00:28:49,426   rep_loss = 0.0
2021-08-09 00:28:49,426 ***** Save model *****
2021-08-09 00:28:52,533 Writing example 0 of 9832
2021-08-09 00:28:52,534 *** Example ***
2021-08-09 00:28:52,534 guid: dev_matched-0
2021-08-09 00:28:52,534 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:28:52,534 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:28:52,534 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:28:52,534 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:28:52,534 label: contradiction
2021-08-09 00:28:52,534 label_id: 0
2021-08-09 00:28:57,154 ***** Running mm evaluation *****
2021-08-09 00:28:57,154   Num examples = 9832
2021-08-09 00:28:57,155   Batch size = 32
2021-08-09 00:29:10,654 ***** Eval results *****
2021-08-09 00:29:10,655   acc = 0.4267697314890155
2021-08-09 00:29:10,655   eval_loss = 0.9320567691480959
2021-08-09 00:29:10,655   global_step = 21799
2021-08-09 00:29:47,050 ***** Running evaluation *****
2021-08-09 00:29:47,050   Epoch = 2 iter 21999 step
2021-08-09 00:29:47,050   Num examples = 9832
2021-08-09 00:29:47,050   Batch size = 32
2021-08-09 00:29:57,854 ***** Eval results *****
2021-08-09 00:29:57,855   acc = 0.42331163547599676
2021-08-09 00:29:57,855   att_loss = 0.0
2021-08-09 00:29:57,855   cls_loss = 0.07825728491506474
2021-08-09 00:29:57,855   eval_loss = 0.9356200110602688
2021-08-09 00:29:57,855   global_step = 21999
2021-08-09 00:29:57,855   loss = 0.07825728491506474
2021-08-09 00:29:57,855   rep_loss = 0.0
2021-08-09 00:29:57,855 ***** Save model *****
2021-08-09 00:29:58,858 Writing example 0 of 9832
2021-08-09 00:29:58,858 *** Example ***
2021-08-09 00:29:58,859 guid: dev_matched-0
2021-08-09 00:29:58,859 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:29:58,859 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:29:58,859 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:29:58,859 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:29:58,859 label: contradiction
2021-08-09 00:29:58,859 label_id: 0
2021-08-09 00:30:03,410 ***** Running mm evaluation *****
2021-08-09 00:30:03,410   Num examples = 9832
2021-08-09 00:30:03,410   Batch size = 32
2021-08-09 00:30:15,975 ***** Eval results *****
2021-08-09 00:30:15,975   acc = 0.42331163547599676
2021-08-09 00:30:15,975   eval_loss = 0.9356200110602688
2021-08-09 00:30:15,975   global_step = 21999
2021-08-09 00:30:54,070 ***** Running evaluation *****
2021-08-09 00:30:54,071   Epoch = 2 iter 22199 step
2021-08-09 00:30:54,071   Num examples = 9832
2021-08-09 00:30:54,071   Batch size = 32
2021-08-09 00:31:04,904 ***** Eval results *****
2021-08-09 00:31:04,904   acc = 0.4299227013832384
2021-08-09 00:31:04,904   att_loss = 0.0
2021-08-09 00:31:04,904   cls_loss = 0.0782530414769303
2021-08-09 00:31:04,905   eval_loss = 0.9317436628527456
2021-08-09 00:31:04,905   global_step = 22199
2021-08-09 00:31:04,905   loss = 0.0782530414769303
2021-08-09 00:31:04,905   rep_loss = 0.0
2021-08-09 00:31:04,905 ***** Save model *****
2021-08-09 00:31:06,487 Writing example 0 of 9832
2021-08-09 00:31:06,487 *** Example ***
2021-08-09 00:31:06,487 guid: dev_matched-0
2021-08-09 00:31:06,487 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:31:06,487 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:31:06,488 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:31:06,488 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:31:06,488 label: contradiction
2021-08-09 00:31:06,488 label_id: 0
2021-08-09 00:31:11,055 ***** Running mm evaluation *****
2021-08-09 00:31:11,056   Num examples = 9832
2021-08-09 00:31:11,056   Batch size = 32
2021-08-09 00:31:21,895 ***** Eval results *****
2021-08-09 00:31:21,896   acc = 0.4299227013832384
2021-08-09 00:31:21,896   eval_loss = 0.9317436628527456
2021-08-09 00:31:21,896   global_step = 22199
2021-08-09 00:32:00,095 ***** Running evaluation *****
2021-08-09 00:32:00,095   Epoch = 2 iter 22399 step
2021-08-09 00:32:00,095   Num examples = 9832
2021-08-09 00:32:00,095   Batch size = 32
2021-08-09 00:32:10,923 ***** Eval results *****
2021-08-09 00:32:10,924   acc = 0.4292107404393816
2021-08-09 00:32:10,924   att_loss = 0.0
2021-08-09 00:32:10,924   cls_loss = 0.07825382090128316
2021-08-09 00:32:10,924   eval_loss = 0.930817302171286
2021-08-09 00:32:10,924   global_step = 22399
2021-08-09 00:32:10,924   loss = 0.07825382090128316
2021-08-09 00:32:10,924   rep_loss = 0.0
2021-08-09 00:32:10,924 ***** Save model *****
2021-08-09 00:32:12,910 Writing example 0 of 9832
2021-08-09 00:32:12,910 *** Example ***
2021-08-09 00:32:12,911 guid: dev_matched-0
2021-08-09 00:32:12,911 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:32:12,911 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:32:12,911 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:32:12,911 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:32:12,911 label: contradiction
2021-08-09 00:32:12,911 label_id: 0
2021-08-09 00:32:17,457 ***** Running mm evaluation *****
2021-08-09 00:32:17,457   Num examples = 9832
2021-08-09 00:32:17,457   Batch size = 32
2021-08-09 00:32:28,323 ***** Eval results *****
2021-08-09 00:32:28,323   acc = 0.4292107404393816
2021-08-09 00:32:28,323   eval_loss = 0.930817302171286
2021-08-09 00:32:28,323   global_step = 22399
2021-08-09 00:33:06,471 ***** Running evaluation *****
2021-08-09 00:33:06,472   Epoch = 2 iter 22599 step
2021-08-09 00:33:06,472   Num examples = 9832
2021-08-09 00:33:06,472   Batch size = 32
2021-08-09 00:33:17,335 ***** Eval results *****
2021-08-09 00:33:17,335   acc = 0.42605777054515864
2021-08-09 00:33:17,336   att_loss = 0.0
2021-08-09 00:33:17,336   cls_loss = 0.07825171911561139
2021-08-09 00:33:17,336   eval_loss = 0.9365592082212497
2021-08-09 00:33:17,336   global_step = 22599
2021-08-09 00:33:17,336   loss = 0.07825171911561139
2021-08-09 00:33:17,336   rep_loss = 0.0
2021-08-09 00:33:17,336 ***** Save model *****
2021-08-09 00:33:18,410 Writing example 0 of 9832
2021-08-09 00:33:18,411 *** Example ***
2021-08-09 00:33:18,411 guid: dev_matched-0
2021-08-09 00:33:18,411 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:33:18,411 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:33:18,411 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:33:18,412 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:33:18,412 label: contradiction
2021-08-09 00:33:18,412 label_id: 0
2021-08-09 00:33:24,635 ***** Running mm evaluation *****
2021-08-09 00:33:24,635   Num examples = 9832
2021-08-09 00:33:24,635   Batch size = 32
2021-08-09 00:33:35,431 ***** Eval results *****
2021-08-09 00:33:35,431   acc = 0.42605777054515864
2021-08-09 00:33:35,431   eval_loss = 0.9365592082212497
2021-08-09 00:33:35,431   global_step = 22599
2021-08-09 00:34:11,790 ***** Running evaluation *****
2021-08-09 00:34:11,791   Epoch = 2 iter 22799 step
2021-08-09 00:34:11,791   Num examples = 9832
2021-08-09 00:34:11,791   Batch size = 32
2021-08-09 00:34:22,659 ***** Eval results *****
2021-08-09 00:34:22,659   acc = 0.4264646053702197
2021-08-09 00:34:22,659   att_loss = 0.0
2021-08-09 00:34:22,659   cls_loss = 0.07824860825404653
2021-08-09 00:34:22,659   eval_loss = 0.9377121590561681
2021-08-09 00:34:22,659   global_step = 22799
2021-08-09 00:34:22,659   loss = 0.07824860825404653
2021-08-09 00:34:22,659   rep_loss = 0.0
2021-08-09 00:34:22,660 ***** Save model *****
2021-08-09 00:34:23,902 Writing example 0 of 9832
2021-08-09 00:34:23,903 *** Example ***
2021-08-09 00:34:23,903 guid: dev_matched-0
2021-08-09 00:34:23,903 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:34:23,903 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:34:23,903 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:34:23,903 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:34:23,903 label: contradiction
2021-08-09 00:34:23,903 label_id: 0
2021-08-09 00:34:28,600 ***** Running mm evaluation *****
2021-08-09 00:34:28,600   Num examples = 9832
2021-08-09 00:34:28,600   Batch size = 32
2021-08-09 00:34:41,072 ***** Eval results *****
2021-08-09 00:34:41,072   acc = 0.4264646053702197
2021-08-09 00:34:41,073   eval_loss = 0.9377121590561681
2021-08-09 00:34:41,073   global_step = 22799
2021-08-09 00:35:19,280 ***** Running evaluation *****
2021-08-09 00:35:19,281   Epoch = 2 iter 22999 step
2021-08-09 00:35:19,281   Num examples = 9832
2021-08-09 00:35:19,281   Batch size = 32
2021-08-09 00:35:30,126 ***** Eval results *****
2021-08-09 00:35:30,127   acc = 0.41985353946297804
2021-08-09 00:35:30,127   att_loss = 0.0
2021-08-09 00:35:30,127   cls_loss = 0.07825174536122832
2021-08-09 00:35:30,127   eval_loss = 0.9488505682387909
2021-08-09 00:35:30,127   global_step = 22999
2021-08-09 00:35:30,127   loss = 0.07825174536122832
2021-08-09 00:35:30,127   rep_loss = 0.0
2021-08-09 00:35:30,127 ***** Save model *****
2021-08-09 00:35:31,695 Writing example 0 of 9832
2021-08-09 00:35:31,695 *** Example ***
2021-08-09 00:35:31,695 guid: dev_matched-0
2021-08-09 00:35:31,696 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:35:31,696 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:35:31,696 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:35:31,696 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:35:31,696 label: contradiction
2021-08-09 00:35:31,696 label_id: 0
2021-08-09 00:35:36,255 ***** Running mm evaluation *****
2021-08-09 00:35:36,255   Num examples = 9832
2021-08-09 00:35:36,255   Batch size = 32
2021-08-09 00:35:48,675 ***** Eval results *****
2021-08-09 00:35:48,675   acc = 0.41985353946297804
2021-08-09 00:35:48,675   eval_loss = 0.9488505682387909
2021-08-09 00:35:48,675   global_step = 22999
2021-08-09 00:36:25,159 ***** Running evaluation *****
2021-08-09 00:36:25,160   Epoch = 2 iter 23199 step
2021-08-09 00:36:25,160   Num examples = 9832
2021-08-09 00:36:25,160   Batch size = 32
2021-08-09 00:36:37,612 ***** Eval results *****
2021-08-09 00:36:37,612   acc = 0.4171074043938161
2021-08-09 00:36:37,612   att_loss = 0.0
2021-08-09 00:36:37,613   cls_loss = 0.07825452700710966
2021-08-09 00:36:37,613   eval_loss = 0.935098565437577
2021-08-09 00:36:37,613   global_step = 23199
2021-08-09 00:36:37,613   loss = 0.07825452700710966
2021-08-09 00:36:37,613   rep_loss = 0.0
2021-08-09 00:36:37,613 ***** Save model *****
2021-08-09 00:36:38,552 Writing example 0 of 9832
2021-08-09 00:36:38,553 *** Example ***
2021-08-09 00:36:38,553 guid: dev_matched-0
2021-08-09 00:36:38,553 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:36:38,553 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:36:38,553 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:36:38,553 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:36:38,553 label: contradiction
2021-08-09 00:36:38,553 label_id: 0
2021-08-09 00:36:43,110 ***** Running mm evaluation *****
2021-08-09 00:36:43,110   Num examples = 9832
2021-08-09 00:36:43,110   Batch size = 32
2021-08-09 00:36:53,993 ***** Eval results *****
2021-08-09 00:36:53,994   acc = 0.4171074043938161
2021-08-09 00:36:53,994   eval_loss = 0.935098565437577
2021-08-09 00:36:53,994   global_step = 23199
2021-08-09 00:37:30,475 ***** Running evaluation *****
2021-08-09 00:37:30,475   Epoch = 2 iter 23399 step
2021-08-09 00:37:30,475   Num examples = 9832
2021-08-09 00:37:30,476   Batch size = 32
2021-08-09 00:37:41,324 ***** Eval results *****
2021-08-09 00:37:41,324   acc = 0.41720911310008135
2021-08-09 00:37:41,324   att_loss = 0.0
2021-08-09 00:37:41,324   cls_loss = 0.07825185384050937
2021-08-09 00:37:41,324   eval_loss = 0.9430350378736273
2021-08-09 00:37:41,324   global_step = 23399
2021-08-09 00:37:41,324   loss = 0.07825185384050937
2021-08-09 00:37:41,324   rep_loss = 0.0
2021-08-09 00:37:41,325 ***** Save model *****
2021-08-09 00:37:42,128 Writing example 0 of 9832
2021-08-09 00:37:42,129 *** Example ***
2021-08-09 00:37:42,129 guid: dev_matched-0
2021-08-09 00:37:42,129 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:37:42,129 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:37:42,129 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:37:42,129 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:37:42,129 label: contradiction
2021-08-09 00:37:42,129 label_id: 0
2021-08-09 00:37:46,738 ***** Running mm evaluation *****
2021-08-09 00:37:46,738   Num examples = 9832
2021-08-09 00:37:46,738   Batch size = 32
2021-08-09 00:37:59,145 ***** Eval results *****
2021-08-09 00:37:59,145   acc = 0.41720911310008135
2021-08-09 00:37:59,145   eval_loss = 0.9430350378736273
2021-08-09 00:37:59,145   global_step = 23399
2021-08-09 00:38:35,532 ***** Running evaluation *****
2021-08-09 00:38:35,533   Epoch = 2 iter 23599 step
2021-08-09 00:38:35,533   Num examples = 9832
2021-08-09 00:38:35,533   Batch size = 32
2021-08-09 00:38:48,012 ***** Eval results *****
2021-08-09 00:38:48,012   acc = 0.42931244914564687
2021-08-09 00:38:48,012   att_loss = 0.0
2021-08-09 00:38:48,012   cls_loss = 0.07825389713989442
2021-08-09 00:38:48,012   eval_loss = 0.9352748382401157
2021-08-09 00:38:48,012   global_step = 23599
2021-08-09 00:38:48,012   loss = 0.07825389713989442
2021-08-09 00:38:48,012   rep_loss = 0.0
2021-08-09 00:38:48,013 ***** Save model *****
2021-08-09 00:38:49,371 Writing example 0 of 9832
2021-08-09 00:38:49,372 *** Example ***
2021-08-09 00:38:49,372 guid: dev_matched-0
2021-08-09 00:38:49,372 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:38:49,372 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:38:49,372 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:38:49,372 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:38:49,372 label: contradiction
2021-08-09 00:38:49,372 label_id: 0
2021-08-09 00:38:53,924 ***** Running mm evaluation *****
2021-08-09 00:38:53,924   Num examples = 9832
2021-08-09 00:38:53,924   Batch size = 32
2021-08-09 00:39:04,894 ***** Eval results *****
2021-08-09 00:39:04,894   acc = 0.42931244914564687
2021-08-09 00:39:04,895   eval_loss = 0.9352748382401157
2021-08-09 00:39:04,895   global_step = 23599
2021-08-09 00:39:43,021 ***** Running evaluation *****
2021-08-09 00:39:43,021   Epoch = 2 iter 23799 step
2021-08-09 00:39:43,021   Num examples = 9832
2021-08-09 00:39:43,021   Batch size = 32
2021-08-09 00:39:53,930 ***** Eval results *****
2021-08-09 00:39:53,930   acc = 0.43083807973962573
2021-08-09 00:39:53,930   att_loss = 0.0
2021-08-09 00:39:53,930   cls_loss = 0.07825686967024202
2021-08-09 00:39:53,930   eval_loss = 0.9365675130060741
2021-08-09 00:39:53,930   global_step = 23799
2021-08-09 00:39:53,930   loss = 0.07825686967024202
2021-08-09 00:39:53,930   rep_loss = 0.0
2021-08-09 00:39:53,963 ***** Save model *****
2021-08-09 00:39:55,945 Writing example 0 of 9832
2021-08-09 00:39:55,946 *** Example ***
2021-08-09 00:39:55,946 guid: dev_matched-0
2021-08-09 00:39:55,946 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:39:55,946 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:39:55,946 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:39:55,946 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:39:55,946 label: contradiction
2021-08-09 00:39:55,946 label_id: 0
2021-08-09 00:40:00,666 ***** Running mm evaluation *****
2021-08-09 00:40:00,667   Num examples = 9832
2021-08-09 00:40:00,667   Batch size = 32
2021-08-09 00:40:11,549 ***** Eval results *****
2021-08-09 00:40:11,549   acc = 0.43083807973962573
2021-08-09 00:40:11,549   eval_loss = 0.9365675130060741
2021-08-09 00:40:11,549   global_step = 23799
2021-08-09 00:40:47,993 ***** Running evaluation *****
2021-08-09 00:40:47,993   Epoch = 2 iter 23999 step
2021-08-09 00:40:47,993   Num examples = 9832
2021-08-09 00:40:47,994   Batch size = 32
2021-08-09 00:40:58,859 ***** Eval results *****
2021-08-09 00:40:58,859   acc = 0.41385272579332794
2021-08-09 00:40:58,859   att_loss = 0.0
2021-08-09 00:40:58,859   cls_loss = 0.07825425836865296
2021-08-09 00:40:58,859   eval_loss = 0.942384145670123
2021-08-09 00:40:58,859   global_step = 23999
2021-08-09 00:40:58,859   loss = 0.07825425836865296
2021-08-09 00:40:58,859   rep_loss = 0.0
2021-08-09 00:40:58,860 ***** Save model *****
2021-08-09 00:40:59,882 Writing example 0 of 9832
2021-08-09 00:40:59,882 *** Example ***
2021-08-09 00:40:59,882 guid: dev_matched-0
2021-08-09 00:40:59,883 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:40:59,883 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:40:59,883 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:40:59,883 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:40:59,883 label: contradiction
2021-08-09 00:40:59,883 label_id: 0
2021-08-09 00:41:06,070 ***** Running mm evaluation *****
2021-08-09 00:41:06,071   Num examples = 9832
2021-08-09 00:41:06,071   Batch size = 32
2021-08-09 00:41:16,929 ***** Eval results *****
2021-08-09 00:41:16,929   acc = 0.41385272579332794
2021-08-09 00:41:16,929   eval_loss = 0.942384145670123
2021-08-09 00:41:16,929   global_step = 23999
2021-08-09 00:41:53,366 ***** Running evaluation *****
2021-08-09 00:41:53,366   Epoch = 2 iter 24199 step
2021-08-09 00:41:53,367   Num examples = 9832
2021-08-09 00:41:53,367   Batch size = 32
2021-08-09 00:42:04,244 ***** Eval results *****
2021-08-09 00:42:04,244   acc = 0.4282953620829943
2021-08-09 00:42:04,244   att_loss = 0.0
2021-08-09 00:42:04,244   cls_loss = 0.0782558958638798
2021-08-09 00:42:04,244   eval_loss = 0.9395456579211471
2021-08-09 00:42:04,244   global_step = 24199
2021-08-09 00:42:04,244   loss = 0.0782558958638798
2021-08-09 00:42:04,244   rep_loss = 0.0
2021-08-09 00:42:04,256 ***** Save model *****
2021-08-09 00:42:08,591 Writing example 0 of 9832
2021-08-09 00:42:08,592 *** Example ***
2021-08-09 00:42:08,592 guid: dev_matched-0
2021-08-09 00:42:08,592 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:42:08,592 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:42:08,592 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:42:08,592 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:42:08,592 label: contradiction
2021-08-09 00:42:08,592 label_id: 0
2021-08-09 00:42:13,177 ***** Running mm evaluation *****
2021-08-09 00:42:13,177   Num examples = 9832
2021-08-09 00:42:13,177   Batch size = 32
2021-08-09 00:42:25,741 ***** Eval results *****
2021-08-09 00:42:25,742   acc = 0.4282953620829943
2021-08-09 00:42:25,742   eval_loss = 0.9395456579211471
2021-08-09 00:42:25,742   global_step = 24199
2021-08-09 00:43:03,928 ***** Running evaluation *****
2021-08-09 00:43:03,928   Epoch = 2 iter 24399 step
2021-08-09 00:43:03,928   Num examples = 9832
2021-08-09 00:43:03,929   Batch size = 32
2021-08-09 00:43:16,319 ***** Eval results *****
2021-08-09 00:43:16,319   acc = 0.4259560618388934
2021-08-09 00:43:16,319   att_loss = 0.0
2021-08-09 00:43:16,319   cls_loss = 0.07825637500414784
2021-08-09 00:43:16,319   eval_loss = 0.9394734699230689
2021-08-09 00:43:16,319   global_step = 24399
2021-08-09 00:43:16,319   loss = 0.07825637500414784
2021-08-09 00:43:16,319   rep_loss = 0.0
2021-08-09 00:43:16,320 ***** Save model *****
2021-08-09 00:43:17,229 Writing example 0 of 9832
2021-08-09 00:43:17,230 *** Example ***
2021-08-09 00:43:17,230 guid: dev_matched-0
2021-08-09 00:43:17,230 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:43:17,230 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:43:17,230 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:43:17,230 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:43:17,231 label: contradiction
2021-08-09 00:43:17,231 label_id: 0
2021-08-09 00:43:21,791 ***** Running mm evaluation *****
2021-08-09 00:43:21,791   Num examples = 9832
2021-08-09 00:43:21,791   Batch size = 32
2021-08-09 00:43:32,722 ***** Eval results *****
2021-08-09 00:43:32,722   acc = 0.4259560618388934
2021-08-09 00:43:32,722   eval_loss = 0.9394734699230689
2021-08-09 00:43:32,722   global_step = 24399
2021-08-09 00:44:11,061 ***** Running evaluation *****
2021-08-09 00:44:11,061   Epoch = 2 iter 24599 step
2021-08-09 00:44:11,061   Num examples = 9832
2021-08-09 00:44:11,061   Batch size = 32
2021-08-09 00:44:21,921 ***** Eval results *****
2021-08-09 00:44:21,921   acc = 0.4292107404393816
2021-08-09 00:44:21,921   att_loss = 0.0
2021-08-09 00:44:21,922   cls_loss = 0.07825554453121047
2021-08-09 00:44:21,922   eval_loss = 0.9290815088656041
2021-08-09 00:44:21,922   global_step = 24599
2021-08-09 00:44:21,922   loss = 0.07825554453121047
2021-08-09 00:44:21,922   rep_loss = 0.0
2021-08-09 00:44:22,678 ***** Save model *****
2021-08-09 00:44:33,059 Writing example 0 of 9832
2021-08-09 00:44:33,060 *** Example ***
2021-08-09 00:44:33,060 guid: dev_matched-0
2021-08-09 00:44:33,060 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:44:33,060 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:44:33,060 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:44:33,060 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:44:33,060 label: contradiction
2021-08-09 00:44:33,060 label_id: 0
2021-08-09 00:44:37,613 ***** Running mm evaluation *****
2021-08-09 00:44:37,614   Num examples = 9832
2021-08-09 00:44:37,614   Batch size = 32
2021-08-09 00:44:48,497 ***** Eval results *****
2021-08-09 00:44:48,497   acc = 0.4292107404393816
2021-08-09 00:44:48,497   eval_loss = 0.9290815088656041
2021-08-09 00:44:48,497   global_step = 24599
2021-08-09 00:45:26,574 ***** Running evaluation *****
2021-08-09 00:45:26,574   Epoch = 2 iter 24799 step
2021-08-09 00:45:26,574   Num examples = 9832
2021-08-09 00:45:26,574   Batch size = 32
2021-08-09 00:45:37,364 ***** Eval results *****
2021-08-09 00:45:37,364   acc = 0.4262611879576892
2021-08-09 00:45:37,364   att_loss = 0.0
2021-08-09 00:45:37,364   cls_loss = 0.07825802609902947
2021-08-09 00:45:37,364   eval_loss = 0.9276605056864875
2021-08-09 00:45:37,364   global_step = 24799
2021-08-09 00:45:37,364   loss = 0.07825802609902947
2021-08-09 00:45:37,364   rep_loss = 0.0
2021-08-09 00:45:37,366 ***** Save model *****
2021-08-09 00:45:39,861 Writing example 0 of 9832
2021-08-09 00:45:39,862 *** Example ***
2021-08-09 00:45:39,862 guid: dev_matched-0
2021-08-09 00:45:39,862 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:45:39,862 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:45:39,862 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:45:39,862 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:45:39,862 label: contradiction
2021-08-09 00:45:39,862 label_id: 0
2021-08-09 00:45:44,425 ***** Running mm evaluation *****
2021-08-09 00:45:44,425   Num examples = 9832
2021-08-09 00:45:44,425   Batch size = 32
2021-08-09 00:45:55,283 ***** Eval results *****
2021-08-09 00:45:55,283   acc = 0.4262611879576892
2021-08-09 00:45:55,283   eval_loss = 0.9276605056864875
2021-08-09 00:45:55,283   global_step = 24799
2021-08-09 00:46:33,336 ***** Running evaluation *****
2021-08-09 00:46:33,336   Epoch = 2 iter 24999 step
2021-08-09 00:46:33,336   Num examples = 9832
2021-08-09 00:46:33,336   Batch size = 32
2021-08-09 00:46:44,126 ***** Eval results *****
2021-08-09 00:46:44,126   acc = 0.4374491456468674
2021-08-09 00:46:44,126   att_loss = 0.0
2021-08-09 00:46:44,126   cls_loss = 0.07825852387514927
2021-08-09 00:46:44,126   eval_loss = 0.9302539161660455
2021-08-09 00:46:44,126   global_step = 24999
2021-08-09 00:46:44,127   loss = 0.07825852387514927
2021-08-09 00:46:44,127   rep_loss = 0.0
2021-08-09 00:46:44,127 ***** Save model *****
2021-08-09 00:46:44,925 Writing example 0 of 9832
2021-08-09 00:46:44,925 *** Example ***
2021-08-09 00:46:44,926 guid: dev_matched-0
2021-08-09 00:46:44,926 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:46:44,926 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:46:44,926 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:46:44,926 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:46:44,926 label: contradiction
2021-08-09 00:46:44,926 label_id: 0
2021-08-09 00:46:49,474 ***** Running mm evaluation *****
2021-08-09 00:46:49,475   Num examples = 9832
2021-08-09 00:46:49,475   Batch size = 32
2021-08-09 00:47:00,273 ***** Eval results *****
2021-08-09 00:47:00,273   acc = 0.4374491456468674
2021-08-09 00:47:00,273   eval_loss = 0.9302539161660455
2021-08-09 00:47:00,273   global_step = 24999
2021-08-09 00:47:38,192 ***** Running evaluation *****
2021-08-09 00:47:38,192   Epoch = 2 iter 25199 step
2021-08-09 00:47:38,192   Num examples = 9832
2021-08-09 00:47:38,192   Batch size = 32
2021-08-09 00:47:50,578 ***** Eval results *****
2021-08-09 00:47:50,578   acc = 0.4429414157851912
2021-08-09 00:47:50,578   att_loss = 0.0
2021-08-09 00:47:50,578   cls_loss = 0.07825869083260395
2021-08-09 00:47:50,578   eval_loss = 0.9311486923849428
2021-08-09 00:47:50,578   global_step = 25199
2021-08-09 00:47:50,578   loss = 0.07825869083260395
2021-08-09 00:47:50,578   rep_loss = 0.0
2021-08-09 00:47:50,579 ***** Save model *****
2021-08-09 00:47:51,922 Writing example 0 of 9832
2021-08-09 00:47:51,922 *** Example ***
2021-08-09 00:47:51,922 guid: dev_matched-0
2021-08-09 00:47:51,922 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:47:51,923 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:47:51,923 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:47:51,923 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:47:51,923 label: contradiction
2021-08-09 00:47:51,923 label_id: 0
2021-08-09 00:47:56,502 ***** Running mm evaluation *****
2021-08-09 00:47:56,502   Num examples = 9832
2021-08-09 00:47:56,502   Batch size = 32
2021-08-09 00:48:07,323 ***** Eval results *****
2021-08-09 00:48:07,323   acc = 0.4429414157851912
2021-08-09 00:48:07,323   eval_loss = 0.9311486923849428
2021-08-09 00:48:07,323   global_step = 25199
2021-08-09 00:48:45,130 ***** Running evaluation *****
2021-08-09 00:48:45,130   Epoch = 2 iter 25399 step
2021-08-09 00:48:45,130   Num examples = 9832
2021-08-09 00:48:45,130   Batch size = 32
2021-08-09 00:48:55,885 ***** Eval results *****
2021-08-09 00:48:55,885   acc = 0.42615947925142394
2021-08-09 00:48:55,885   att_loss = 0.0
2021-08-09 00:48:55,885   cls_loss = 0.07825725860592954
2021-08-09 00:48:55,885   eval_loss = 0.9431087049570951
2021-08-09 00:48:55,885   global_step = 25399
2021-08-09 00:48:55,885   loss = 0.07825725860592954
2021-08-09 00:48:55,885   rep_loss = 0.0
2021-08-09 00:48:55,886 ***** Save model *****
2021-08-09 00:48:57,733 Writing example 0 of 9832
2021-08-09 00:48:57,734 *** Example ***
2021-08-09 00:48:57,734 guid: dev_matched-0
2021-08-09 00:48:57,734 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:48:57,734 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:48:57,734 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:48:57,734 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:48:57,734 label: contradiction
2021-08-09 00:48:57,734 label_id: 0
2021-08-09 00:49:02,285 ***** Running mm evaluation *****
2021-08-09 00:49:02,285   Num examples = 9832
2021-08-09 00:49:02,285   Batch size = 32
2021-08-09 00:49:13,063 ***** Eval results *****
2021-08-09 00:49:13,063   acc = 0.42615947925142394
2021-08-09 00:49:13,063   eval_loss = 0.9431087049570951
2021-08-09 00:49:13,063   global_step = 25399
2021-08-09 00:49:50,992 ***** Running evaluation *****
2021-08-09 00:49:50,992   Epoch = 2 iter 25599 step
2021-08-09 00:49:50,992   Num examples = 9832
2021-08-09 00:49:50,992   Batch size = 32
2021-08-09 00:50:01,750 ***** Eval results *****
2021-08-09 00:50:01,750   acc = 0.43236371033360455
2021-08-09 00:50:01,750   att_loss = 0.0
2021-08-09 00:50:01,750   cls_loss = 0.0782564025253156
2021-08-09 00:50:01,750   eval_loss = 0.9506848529948817
2021-08-09 00:50:01,750   global_step = 25599
2021-08-09 00:50:01,750   loss = 0.0782564025253156
2021-08-09 00:50:01,750   rep_loss = 0.0
2021-08-09 00:50:01,750 ***** Save model *****
2021-08-09 00:50:02,671 Writing example 0 of 9832
2021-08-09 00:50:02,672 *** Example ***
2021-08-09 00:50:02,672 guid: dev_matched-0
2021-08-09 00:50:02,672 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:50:02,672 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:50:02,672 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:50:02,672 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:50:02,672 label: contradiction
2021-08-09 00:50:02,672 label_id: 0
2021-08-09 00:50:07,247 ***** Running mm evaluation *****
2021-08-09 00:50:07,247   Num examples = 9832
2021-08-09 00:50:07,247   Batch size = 32
2021-08-09 00:50:18,026 ***** Eval results *****
2021-08-09 00:50:18,026   acc = 0.43236371033360455
2021-08-09 00:50:18,027   eval_loss = 0.9506848529948817
2021-08-09 00:50:18,027   global_step = 25599
2021-08-09 00:50:55,968 ***** Running evaluation *****
2021-08-09 00:50:55,968   Epoch = 2 iter 25799 step
2021-08-09 00:50:55,968   Num examples = 9832
2021-08-09 00:50:55,969   Batch size = 32
2021-08-09 00:51:06,719 ***** Eval results *****
2021-08-09 00:51:06,720   acc = 0.43704231082180633
2021-08-09 00:51:06,720   att_loss = 0.0
2021-08-09 00:51:06,720   cls_loss = 0.07825767844600688
2021-08-09 00:51:06,720   eval_loss = 0.9472719678631076
2021-08-09 00:51:06,720   global_step = 25799
2021-08-09 00:51:06,720   loss = 0.07825767844600688
2021-08-09 00:51:06,720   rep_loss = 0.0
2021-08-09 00:51:06,720 ***** Save model *****
2021-08-09 00:51:08,289 Writing example 0 of 9832
2021-08-09 00:51:08,290 *** Example ***
2021-08-09 00:51:08,290 guid: dev_matched-0
2021-08-09 00:51:08,290 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:51:08,290 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:51:08,290 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:51:08,290 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:51:08,290 label: contradiction
2021-08-09 00:51:08,290 label_id: 0
2021-08-09 00:51:12,858 ***** Running mm evaluation *****
2021-08-09 00:51:12,858   Num examples = 9832
2021-08-09 00:51:12,858   Batch size = 32
2021-08-09 00:51:23,650 ***** Eval results *****
2021-08-09 00:51:23,650   acc = 0.43704231082180633
2021-08-09 00:51:23,650   eval_loss = 0.9472719678631076
2021-08-09 00:51:23,650   global_step = 25799
2021-08-09 00:52:01,649 ***** Running evaluation *****
2021-08-09 00:52:01,650   Epoch = 2 iter 25999 step
2021-08-09 00:52:01,650   Num examples = 9832
2021-08-09 00:52:01,650   Batch size = 32
2021-08-09 00:52:12,349 ***** Eval results *****
2021-08-09 00:52:12,349   acc = 0.43449959316517495
2021-08-09 00:52:12,349   att_loss = 0.0
2021-08-09 00:52:12,349   cls_loss = 0.07825715674998912
2021-08-09 00:52:12,349   eval_loss = 0.9466221820224415
2021-08-09 00:52:12,349   global_step = 25999
2021-08-09 00:52:12,349   loss = 0.07825715674998912
2021-08-09 00:52:12,350   rep_loss = 0.0
2021-08-09 00:52:12,350 ***** Save model *****
2021-08-09 00:52:15,584 Writing example 0 of 9832
2021-08-09 00:52:15,585 *** Example ***
2021-08-09 00:52:15,585 guid: dev_matched-0
2021-08-09 00:52:15,585 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:52:15,585 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:52:15,585 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:52:15,585 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:52:15,585 label: contradiction
2021-08-09 00:52:15,585 label_id: 0
2021-08-09 00:52:20,212 ***** Running mm evaluation *****
2021-08-09 00:52:20,212   Num examples = 9832
2021-08-09 00:52:20,212   Batch size = 32
2021-08-09 00:52:30,998 ***** Eval results *****
2021-08-09 00:52:30,999   acc = 0.43449959316517495
2021-08-09 00:52:30,999   eval_loss = 0.9466221820224415
2021-08-09 00:52:30,999   global_step = 25999
2021-08-09 00:53:07,100 ***** Running evaluation *****
2021-08-09 00:53:07,100   Epoch = 2 iter 26199 step
2021-08-09 00:53:07,100   Num examples = 9832
2021-08-09 00:53:07,100   Batch size = 32
2021-08-09 00:53:19,504 ***** Eval results *****
2021-08-09 00:53:19,504   acc = 0.42188771358828314
2021-08-09 00:53:19,505   att_loss = 0.0
2021-08-09 00:53:19,505   cls_loss = 0.07825616202312644
2021-08-09 00:53:19,505   eval_loss = 0.9473788513765706
2021-08-09 00:53:19,505   global_step = 26199
2021-08-09 00:53:19,505   loss = 0.07825616202312644
2021-08-09 00:53:19,505   rep_loss = 0.0
2021-08-09 00:53:19,505 ***** Save model *****
2021-08-09 00:53:20,920 Writing example 0 of 9832
2021-08-09 00:53:20,921 *** Example ***
2021-08-09 00:53:20,921 guid: dev_matched-0
2021-08-09 00:53:20,921 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 00:53:20,921 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:53:20,921 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:53:20,921 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 00:53:20,921 label: contradiction
2021-08-09 00:53:20,921 label_id: 0
2021-08-09 00:53:25,478 ***** Running mm evaluation *****
2021-08-09 00:53:25,478   Num examples = 9832
2021-08-09 00:53:25,478   Batch size = 32
2021-08-09 00:53:36,269 ***** Eval results *****
2021-08-09 00:53:36,269   acc = 0.42188771358828314
2021-08-09 00:53:36,269   eval_loss = 0.9473788513765706
2021-08-09 00:53:36,269   global_step = 26199
2021-08-09 01:28:40,935 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-09 01:28:41,018 device: cuda n_gpu: 2
2021-08-09 01:28:50,721 Writing example 0 of 505555
2021-08-09 01:28:50,723 *** Example ***
2021-08-09 01:28:50,723 guid: aug-0
2021-08-09 01:28:50,723 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-09 01:28:50,723 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:28:50,723 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:28:50,723 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:28:50,723 label: neutral
2021-08-09 01:28:50,723 label_id: 2
2021-08-09 01:28:55,443 Writing example 10000 of 505555
2021-08-09 01:29:00,257 Writing example 20000 of 505555
2021-08-09 01:29:04,846 Writing example 30000 of 505555
2021-08-09 01:29:09,580 Writing example 40000 of 505555
2021-08-09 01:29:14,417 Writing example 50000 of 505555
2021-08-09 01:29:18,947 Writing example 60000 of 505555
2021-08-09 01:29:23,737 Writing example 70000 of 505555
2021-08-09 01:29:28,402 Writing example 80000 of 505555
2021-08-09 01:29:33,406 Writing example 90000 of 505555
2021-08-09 01:29:38,029 Writing example 100000 of 505555
2021-08-09 01:29:42,653 Writing example 110000 of 505555
2021-08-09 01:29:47,052 Writing example 120000 of 505555
2021-08-09 01:29:51,855 Writing example 130000 of 505555
2021-08-09 01:29:57,210 Writing example 140000 of 505555
2021-08-09 01:30:01,683 Writing example 150000 of 505555
2021-08-09 01:30:06,526 Writing example 160000 of 505555
2021-08-09 01:30:11,236 Writing example 170000 of 505555
2021-08-09 01:30:16,056 Writing example 180000 of 505555
2021-08-09 01:30:20,704 Writing example 190000 of 505555
2021-08-09 01:30:25,313 Writing example 200000 of 505555
2021-08-09 01:30:30,756 Writing example 210000 of 505555
2021-08-09 01:30:35,252 Writing example 220000 of 505555
2021-08-09 01:30:40,031 Writing example 230000 of 505555
2021-08-09 01:30:44,603 Writing example 240000 of 505555
2021-08-09 01:30:49,275 Writing example 250000 of 505555
2021-08-09 01:30:53,882 Writing example 260000 of 505555
2021-08-09 01:30:58,404 Writing example 270000 of 505555
2021-08-09 01:31:02,948 Writing example 280000 of 505555
2021-08-09 01:31:08,635 Writing example 290000 of 505555
2021-08-09 01:31:13,347 Writing example 300000 of 505555
2021-08-09 01:31:17,908 Writing example 310000 of 505555
2021-08-09 01:31:22,360 Writing example 320000 of 505555
2021-08-09 01:31:27,076 Writing example 330000 of 505555
2021-08-09 01:31:31,772 Writing example 340000 of 505555
2021-08-09 01:31:36,336 Writing example 350000 of 505555
2021-08-09 01:31:40,864 Writing example 360000 of 505555
2021-08-09 01:31:45,293 Writing example 370000 of 505555
2021-08-09 01:31:49,875 Writing example 380000 of 505555
2021-08-09 01:31:56,107 Writing example 390000 of 505555
2021-08-09 01:32:00,752 Writing example 400000 of 505555
2021-08-09 01:32:05,343 Writing example 410000 of 505555
2021-08-09 01:32:09,827 Writing example 420000 of 505555
2021-08-09 01:32:14,251 Writing example 430000 of 505555
2021-08-09 01:32:18,789 Writing example 440000 of 505555
2021-08-09 01:32:23,482 Writing example 450000 of 505555
2021-08-09 01:32:28,115 Writing example 460000 of 505555
2021-08-09 01:32:32,715 Writing example 470000 of 505555
2021-08-09 01:32:37,390 Writing example 480000 of 505555
2021-08-09 01:32:42,015 Writing example 490000 of 505555
2021-08-09 01:32:46,808 Writing example 500000 of 505555
2021-08-09 01:32:54,636 Writing example 0 of 9815
2021-08-09 01:32:54,636 *** Example ***
2021-08-09 01:32:54,636 guid: dev_matched-0
2021-08-09 01:32:54,636 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-09 01:32:54,636 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:32:54,636 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:32:54,636 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:32:54,636 label: neutral
2021-08-09 01:32:54,636 label_id: 2
2021-08-09 01:32:59,057 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-09 01:33:01,566 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-09 01:33:12,170 loading model...
2021-08-09 01:33:12,203 done!
2021-08-09 01:33:12,203 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-09 01:33:12,203 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-09 01:33:19,438 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-09 01:33:20,930 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity/pytorch_model.bin
2021-08-09 01:33:23,817 loading model...
2021-08-09 01:33:23,830 done!
2021-08-09 01:33:23,898 ***** Running training *****
2021-08-09 01:33:23,898   Num examples = 505555
2021-08-09 01:33:23,898   Batch size = 32
2021-08-09 01:33:23,898   Num steps = 47394
2021-08-09 01:33:23,899 n: module.bert.embeddings.word_embeddings.weight
2021-08-09 01:33:23,899 n: module.bert.embeddings.position_embeddings.weight
2021-08-09 01:33:23,899 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-09 01:33:23,899 n: module.bert.embeddings.LayerNorm.weight
2021-08-09 01:33:23,899 n: module.bert.embeddings.LayerNorm.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-09 01:33:23,900 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-09 01:33:23,901 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-09 01:33:23,902 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-09 01:33:23,903 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-09 01:33:23,904 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-09 01:33:23,904 n: module.bert.pooler.dense.weight
2021-08-09 01:33:23,904 n: module.bert.pooler.dense.bias
2021-08-09 01:33:23,904 n: module.classifier.weight
2021-08-09 01:33:23,904 n: module.classifier.bias
2021-08-09 01:33:23,905 n: module.fit_dense.weight
2021-08-09 01:33:23,905 n: module.fit_dense.bias
2021-08-09 01:33:23,905 Total parameters: 67547907
2021-08-09 01:37:49,644 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-09 01:37:49,775 device: cuda n_gpu: 4
2021-08-09 01:38:03,622 Writing example 0 of 505555
2021-08-09 01:38:03,624 *** Example ***
2021-08-09 01:38:03,624 guid: aug-0
2021-08-09 01:38:03,624 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-09 01:38:03,624 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:38:03,624 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:38:03,624 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:38:03,624 label: neutral
2021-08-09 01:38:03,624 label_id: 2
2021-08-09 01:38:08,309 Writing example 10000 of 505555
2021-08-09 01:38:12,920 Writing example 20000 of 505555
2021-08-09 01:38:17,519 Writing example 30000 of 505555
2021-08-09 01:38:22,335 Writing example 40000 of 505555
2021-08-09 01:38:27,136 Writing example 50000 of 505555
2021-08-09 01:38:31,645 Writing example 60000 of 505555
2021-08-09 01:38:36,346 Writing example 70000 of 505555
2021-08-09 01:38:40,997 Writing example 80000 of 505555
2021-08-09 01:38:45,979 Writing example 90000 of 505555
2021-08-09 01:38:50,593 Writing example 100000 of 505555
2021-08-09 01:38:55,174 Writing example 110000 of 505555
2021-08-09 01:38:59,531 Writing example 120000 of 505555
2021-08-09 01:39:04,183 Writing example 130000 of 505555
2021-08-09 01:39:09,820 Writing example 140000 of 505555
2021-08-09 01:39:14,243 Writing example 150000 of 505555
2021-08-09 01:39:19,007 Writing example 160000 of 505555
2021-08-09 01:39:23,651 Writing example 170000 of 505555
2021-08-09 01:39:28,226 Writing example 180000 of 505555
2021-08-09 01:39:32,861 Writing example 190000 of 505555
2021-08-09 01:39:37,430 Writing example 200000 of 505555
2021-08-09 01:39:42,936 Writing example 210000 of 505555
2021-08-09 01:39:47,466 Writing example 220000 of 505555
2021-08-09 01:39:52,095 Writing example 230000 of 505555
2021-08-09 01:39:56,674 Writing example 240000 of 505555
2021-08-09 01:40:01,309 Writing example 250000 of 505555
2021-08-09 01:40:05,917 Writing example 260000 of 505555
2021-08-09 01:40:10,488 Writing example 270000 of 505555
2021-08-09 01:40:15,001 Writing example 280000 of 505555
2021-08-09 01:40:20,672 Writing example 290000 of 505555
2021-08-09 01:40:25,352 Writing example 300000 of 505555
2021-08-09 01:40:29,870 Writing example 310000 of 505555
2021-08-09 01:40:34,275 Writing example 320000 of 505555
2021-08-09 01:40:38,947 Writing example 330000 of 505555
2021-08-09 01:40:43,454 Writing example 340000 of 505555
2021-08-09 01:40:48,101 Writing example 350000 of 505555
2021-08-09 01:40:52,587 Writing example 360000 of 505555
2021-08-09 01:40:56,977 Writing example 370000 of 505555
2021-08-09 01:41:01,455 Writing example 380000 of 505555
2021-08-09 01:41:07,470 Writing example 390000 of 505555
2021-08-09 01:41:12,092 Writing example 400000 of 505555
2021-08-09 01:41:16,696 Writing example 410000 of 505555
2021-08-09 01:41:21,130 Writing example 420000 of 505555
2021-08-09 01:41:25,521 Writing example 430000 of 505555
2021-08-09 01:41:29,956 Writing example 440000 of 505555
2021-08-09 01:41:34,569 Writing example 450000 of 505555
2021-08-09 01:41:39,373 Writing example 460000 of 505555
2021-08-09 01:41:43,965 Writing example 470000 of 505555
2021-08-09 01:41:48,590 Writing example 480000 of 505555
2021-08-09 01:41:53,169 Writing example 490000 of 505555
2021-08-09 01:41:57,706 Writing example 500000 of 505555
2021-08-09 01:42:05,080 Writing example 0 of 9815
2021-08-09 01:42:05,081 *** Example ***
2021-08-09 01:42:05,081 guid: dev_matched-0
2021-08-09 01:42:05,081 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-09 01:42:05,081 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:42:05,081 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:42:05,081 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:42:05,081 label: neutral
2021-08-09 01:42:05,081 label_id: 2
2021-08-09 01:42:09,423 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-09 01:42:11,875 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/checkpoint-20000/pytorch_model.bin
2021-08-09 01:42:19,637 loading model...
2021-08-09 01:42:19,670 done!
2021-08-09 01:42:19,671 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-09 01:42:19,671 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-09 01:42:23,619 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-09 01:42:25,114 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity/pytorch_model.bin
2021-08-09 01:42:28,353 loading model...
2021-08-09 01:42:28,365 done!
2021-08-09 01:42:28,433 ***** Running training *****
2021-08-09 01:42:28,433   Num examples = 505555
2021-08-09 01:42:28,433   Batch size = 32
2021-08-09 01:42:28,433   Num steps = 47394
2021-08-09 01:42:28,434 n: module.bert.embeddings.word_embeddings.weight
2021-08-09 01:42:28,434 n: module.bert.embeddings.position_embeddings.weight
2021-08-09 01:42:28,434 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-09 01:42:28,434 n: module.bert.embeddings.LayerNorm.weight
2021-08-09 01:42:28,434 n: module.bert.embeddings.LayerNorm.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-09 01:42:28,435 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-09 01:42:28,436 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-09 01:42:28,437 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-09 01:42:28,438 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-09 01:42:28,439 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-09 01:42:28,439 n: module.bert.pooler.dense.weight
2021-08-09 01:42:28,440 n: module.bert.pooler.dense.bias
2021-08-09 01:42:28,440 n: module.classifier.weight
2021-08-09 01:42:28,440 n: module.classifier.bias
2021-08-09 01:42:28,440 n: module.fit_dense.weight
2021-08-09 01:42:28,440 n: module.fit_dense.bias
2021-08-09 01:42:28,440 Total parameters: 67547907
2021-08-09 01:51:32,816 ***** Running evaluation *****
2021-08-09 01:51:32,817   Epoch = 0 iter 2999 step
2021-08-09 01:51:32,817   Num examples = 9815
2021-08-09 01:51:32,817   Batch size = 32
2021-08-09 01:51:45,501 ***** Eval results *****
2021-08-09 01:51:45,501   acc = 0.08323993886907795
2021-08-09 01:51:45,501   att_loss = 0.0
2021-08-09 01:51:45,501   cls_loss = 0.23410593952068132
2021-08-09 01:51:45,501   eval_loss = 2.5719174550488253
2021-08-09 01:51:45,501   global_step = 2999
2021-08-09 01:51:45,501   loss = 0.23410593952068132
2021-08-09 01:51:45,502   rep_loss = 0.0
2021-08-09 01:51:45,502 ***** Save model *****
2021-08-09 01:51:51,458 Writing example 0 of 9832
2021-08-09 01:51:51,458 *** Example ***
2021-08-09 01:51:51,458 guid: dev_matched-0
2021-08-09 01:51:51,459 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 01:51:51,459 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:51:51,459 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:51:51,459 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 01:51:51,459 label: contradiction
2021-08-09 01:51:51,459 label_id: 0
2021-08-09 01:51:56,141 ***** Running mm evaluation *****
2021-08-09 01:51:56,141   Num examples = 9832
2021-08-09 01:51:56,141   Batch size = 32
2021-08-09 01:52:07,046 ***** Eval results *****
2021-08-09 01:52:07,046   acc = 0.07516273393002441
2021-08-09 01:52:07,046   eval_loss = 2.5813162311331017
2021-08-09 01:52:07,046   global_step = 2999
2021-08-09 02:01:08,496 ***** Running evaluation *****
2021-08-09 02:01:08,496   Epoch = 0 iter 5999 step
2021-08-09 02:01:08,496   Num examples = 9832
2021-08-09 02:01:08,496   Batch size = 32
2021-08-09 02:01:19,276 ***** Eval results *****
2021-08-09 02:01:19,276   acc = 0.07923108218063467
2021-08-09 02:01:19,276   att_loss = 0.0
2021-08-09 02:01:19,276   cls_loss = 0.22908960289260827
2021-08-09 02:01:19,276   eval_loss = 2.5690285377688222
2021-08-09 02:01:19,276   global_step = 5999
2021-08-09 02:01:19,276   loss = 0.22908960289260827
2021-08-09 02:01:19,276   rep_loss = 0.0
2021-08-09 02:10:03,320 ***** Running evaluation *****
2021-08-09 02:10:03,321   Epoch = 0 iter 8999 step
2021-08-09 02:10:03,321   Num examples = 9832
2021-08-09 02:10:03,321   Batch size = 32
2021-08-09 02:10:14,204 ***** Eval results *****
2021-08-09 02:10:14,204   acc = 0.08096013018714401
2021-08-09 02:10:14,205   att_loss = 0.0
2021-08-09 02:10:14,205   cls_loss = 0.2275242841131807
2021-08-09 02:10:14,205   eval_loss = 2.5146886106435353
2021-08-09 02:10:14,205   global_step = 8999
2021-08-09 02:10:14,205   loss = 0.2275242841131807
2021-08-09 02:10:14,205   rep_loss = 0.0
2021-08-09 02:18:59,606 ***** Running evaluation *****
2021-08-09 02:18:59,606   Epoch = 0 iter 11999 step
2021-08-09 02:18:59,606   Num examples = 9832
2021-08-09 02:18:59,606   Batch size = 32
2021-08-09 02:19:10,472 ***** Eval results *****
2021-08-09 02:19:10,472   acc = 0.07811228641171684
2021-08-09 02:19:10,472   att_loss = 0.0
2021-08-09 02:19:10,472   cls_loss = 0.22673266835859074
2021-08-09 02:19:10,472   eval_loss = 2.652736522160567
2021-08-09 02:19:10,472   global_step = 11999
2021-08-09 02:19:10,472   loss = 0.22673266835859074
2021-08-09 02:19:10,472   rep_loss = 0.0
2021-08-09 02:28:14,786 ***** Running evaluation *****
2021-08-09 02:28:14,786   Epoch = 0 iter 14999 step
2021-08-09 02:28:14,786   Num examples = 9832
2021-08-09 02:28:14,786   Batch size = 32
2021-08-09 02:28:25,604 ***** Eval results *****
2021-08-09 02:28:25,604   acc = 0.08523189585028479
2021-08-09 02:28:25,604   att_loss = 0.0
2021-08-09 02:28:25,604   cls_loss = 0.22605539599639812
2021-08-09 02:28:25,604   eval_loss = 2.5308906726248854
2021-08-09 02:28:25,604   global_step = 14999
2021-08-09 02:28:25,604   loss = 0.22605539599639812
2021-08-09 02:28:25,604   rep_loss = 0.0
2021-08-09 02:28:25,605 ***** Save model *****
2021-08-09 02:28:28,613 Writing example 0 of 9832
2021-08-09 02:28:28,613 *** Example ***
2021-08-09 02:28:28,614 guid: dev_matched-0
2021-08-09 02:28:28,614 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 02:28:28,614 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 02:28:28,614 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 02:28:28,614 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 02:28:28,614 label: contradiction
2021-08-09 02:28:28,614 label_id: 0
2021-08-09 02:28:33,166 ***** Running mm evaluation *****
2021-08-09 02:28:33,167   Num examples = 9832
2021-08-09 02:28:33,167   Batch size = 32
2021-08-09 02:28:45,642 ***** Eval results *****
2021-08-09 02:28:45,642   acc = 0.08523189585028479
2021-08-09 02:28:45,642   eval_loss = 2.5308906726248854
2021-08-09 02:28:45,642   global_step = 14999
2021-08-09 02:37:41,917 ***** Running evaluation *****
2021-08-09 02:37:41,918   Epoch = 1 iter 17999 step
2021-08-09 02:37:41,918   Num examples = 9832
2021-08-09 02:37:41,918   Batch size = 32
2021-08-09 02:37:52,755 ***** Eval results *****
2021-08-09 02:37:52,756   acc = 0.08390968266883646
2021-08-09 02:37:52,756   att_loss = 0.0
2021-08-09 02:37:52,756   cls_loss = 0.22122245377055735
2021-08-09 02:37:52,756   eval_loss = 2.5017179537129093
2021-08-09 02:37:52,756   global_step = 17999
2021-08-09 02:37:52,756   loss = 0.22122245377055735
2021-08-09 02:37:52,756   rep_loss = 0.0
2021-08-09 02:46:32,904 ***** Running evaluation *****
2021-08-09 02:46:32,905   Epoch = 1 iter 20999 step
2021-08-09 02:46:32,905   Num examples = 9832
2021-08-09 02:46:32,905   Batch size = 32
2021-08-09 02:46:43,870 ***** Eval results *****
2021-08-09 02:46:43,871   acc = 0.07862082994304312
2021-08-09 02:46:43,871   att_loss = 0.0
2021-08-09 02:46:43,871   cls_loss = 0.221616657499143
2021-08-09 02:46:43,871   eval_loss = 2.566035883767264
2021-08-09 02:46:43,871   global_step = 20999
2021-08-09 02:46:43,871   loss = 0.221616657499143
2021-08-09 02:46:43,871   rep_loss = 0.0
2021-08-09 02:55:23,874 ***** Running evaluation *****
2021-08-09 02:55:23,875   Epoch = 1 iter 23999 step
2021-08-09 02:55:23,875   Num examples = 9832
2021-08-09 02:55:23,875   Batch size = 32
2021-08-09 02:55:34,730 ***** Eval results *****
2021-08-09 02:55:34,730   acc = 0.08167209113100081
2021-08-09 02:55:34,730   att_loss = 0.0
2021-08-09 02:55:34,730   cls_loss = 0.2211196461896637
2021-08-09 02:55:34,730   eval_loss = 2.5286827172551836
2021-08-09 02:55:34,730   global_step = 23999
2021-08-09 02:55:34,731   loss = 0.2211196461896637
2021-08-09 02:55:34,731   rep_loss = 0.0
2021-08-09 03:04:31,408 ***** Running evaluation *****
2021-08-09 03:04:31,408   Epoch = 1 iter 26999 step
2021-08-09 03:04:31,408   Num examples = 9832
2021-08-09 03:04:31,408   Batch size = 32
2021-08-09 03:04:42,233 ***** Eval results *****
2021-08-09 03:04:42,233   acc = 0.08024816924328723
2021-08-09 03:04:42,233   att_loss = 0.0
2021-08-09 03:04:42,233   cls_loss = 0.22085973990382857
2021-08-09 03:04:42,233   eval_loss = 2.5440352648109585
2021-08-09 03:04:42,233   global_step = 26999
2021-08-09 03:04:42,234   loss = 0.22085973990382857
2021-08-09 03:04:42,234   rep_loss = 0.0
2021-08-09 03:13:22,315 ***** Running evaluation *****
2021-08-09 03:13:22,316   Epoch = 1 iter 29999 step
2021-08-09 03:13:22,316   Num examples = 9832
2021-08-09 03:13:22,316   Batch size = 32
2021-08-09 03:13:33,215 ***** Eval results *****
2021-08-09 03:13:33,215   acc = 0.08116354759967453
2021-08-09 03:13:33,215   att_loss = 0.0
2021-08-09 03:13:33,215   cls_loss = 0.22067969783738703
2021-08-09 03:13:33,216   eval_loss = 2.573478579521179
2021-08-09 03:13:33,216   global_step = 29999
2021-08-09 03:13:33,216   loss = 0.22067969783738703
2021-08-09 03:13:33,216   rep_loss = 0.0
2021-08-09 03:22:13,476 ***** Running evaluation *****
2021-08-09 03:22:13,476   Epoch = 2 iter 32999 step
2021-08-09 03:22:13,476   Num examples = 9832
2021-08-09 03:22:13,476   Batch size = 32
2021-08-09 03:22:24,359 ***** Eval results *****
2021-08-09 03:22:24,359   acc = 0.0838079739625712
2021-08-09 03:22:24,359   att_loss = 0.0
2021-08-09 03:22:24,359   cls_loss = 0.21825274632857344
2021-08-09 03:22:24,359   eval_loss = 2.520758228255557
2021-08-09 03:22:24,359   global_step = 32999
2021-08-09 03:22:24,359   loss = 0.21825274632857344
2021-08-09 03:22:24,359   rep_loss = 0.0
2021-08-09 03:31:19,591 ***** Running evaluation *****
2021-08-09 03:31:19,591   Epoch = 2 iter 35999 step
2021-08-09 03:31:19,591   Num examples = 9832
2021-08-09 03:31:19,591   Batch size = 32
2021-08-09 03:31:30,414 ***** Eval results *****
2021-08-09 03:31:30,414   acc = 0.08065500406834825
2021-08-09 03:31:30,414   att_loss = 0.0
2021-08-09 03:31:30,414   cls_loss = 0.21813293026775873
2021-08-09 03:31:30,414   eval_loss = 2.576484273780476
2021-08-09 03:31:30,414   global_step = 35999
2021-08-09 03:31:30,414   loss = 0.21813293026775873
2021-08-09 03:31:30,414   rep_loss = 0.0
2021-08-09 03:40:04,843 ***** Running evaluation *****
2021-08-09 03:40:04,844   Epoch = 2 iter 38999 step
2021-08-09 03:40:04,844   Num examples = 9832
2021-08-09 03:40:04,844   Batch size = 32
2021-08-09 03:40:15,779 ***** Eval results *****
2021-08-09 03:40:15,780   acc = 0.08116354759967453
2021-08-09 03:40:15,780   att_loss = 0.0
2021-08-09 03:40:15,780   cls_loss = 0.21801430359130516
2021-08-09 03:40:15,780   eval_loss = 2.55745025772553
2021-08-09 03:40:15,780   global_step = 38999
2021-08-09 03:40:15,780   loss = 0.21801430359130516
2021-08-09 03:40:15,780   rep_loss = 0.0
2021-08-09 03:48:50,976 ***** Running evaluation *****
2021-08-09 03:48:50,976   Epoch = 2 iter 41999 step
2021-08-09 03:48:50,977   Num examples = 9832
2021-08-09 03:48:50,977   Batch size = 32
2021-08-09 03:49:01,853 ***** Eval results *****
2021-08-09 03:49:01,853   acc = 0.07872253864930838
2021-08-09 03:49:01,853   att_loss = 0.0
2021-08-09 03:49:01,853   cls_loss = 0.21788793293905867
2021-08-09 03:49:01,853   eval_loss = 2.560156471930541
2021-08-09 03:49:01,853   global_step = 41999
2021-08-09 03:49:01,853   loss = 0.21788793293905867
2021-08-09 03:49:01,853   rep_loss = 0.0
2021-08-09 03:57:55,247 ***** Running evaluation *****
2021-08-09 03:57:55,248   Epoch = 2 iter 44999 step
2021-08-09 03:57:55,248   Num examples = 9832
2021-08-09 03:57:55,248   Batch size = 32
2021-08-09 03:58:06,200 ***** Eval results *****
2021-08-09 03:58:06,201   acc = 0.08136696501220504
2021-08-09 03:58:06,201   att_loss = 0.0
2021-08-09 03:58:06,201   cls_loss = 0.2177714433748669
2021-08-09 03:58:06,201   eval_loss = 2.5423734350637957
2021-08-09 03:58:06,201   global_step = 44999
2021-08-09 03:58:06,201   loss = 0.2177714433748669
2021-08-09 03:58:06,201   rep_loss = 0.0
2021-08-09 11:16:47,610 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-09 11:16:47,867 device: cuda n_gpu: 4
2021-08-09 11:16:59,872 Writing example 0 of 505555
2021-08-09 11:16:59,873 *** Example ***
2021-08-09 11:16:59,873 guid: aug-0
2021-08-09 11:16:59,873 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-09 11:16:59,873 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:16:59,873 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:16:59,874 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:16:59,874 label: neutral
2021-08-09 11:16:59,874 label_id: 2
2021-08-09 11:17:04,531 Writing example 10000 of 505555
2021-08-09 11:17:09,354 Writing example 20000 of 505555
2021-08-09 11:17:13,884 Writing example 30000 of 505555
2021-08-09 11:17:18,593 Writing example 40000 of 505555
2021-08-09 11:17:23,361 Writing example 50000 of 505555
2021-08-09 11:17:27,830 Writing example 60000 of 505555
2021-08-09 11:17:32,429 Writing example 70000 of 505555
2021-08-09 11:17:37,080 Writing example 80000 of 505555
2021-08-09 11:17:41,922 Writing example 90000 of 505555
2021-08-09 11:17:46,474 Writing example 100000 of 505555
2021-08-09 11:17:51,029 Writing example 110000 of 505555
2021-08-09 11:17:55,356 Writing example 120000 of 505555
2021-08-09 11:17:59,990 Writing example 130000 of 505555
2021-08-09 11:18:05,286 Writing example 140000 of 505555
2021-08-09 11:18:09,674 Writing example 150000 of 505555
2021-08-09 11:18:14,402 Writing example 160000 of 505555
2021-08-09 11:18:19,022 Writing example 170000 of 505555
2021-08-09 11:18:23,609 Writing example 180000 of 505555
2021-08-09 11:18:28,269 Writing example 190000 of 505555
2021-08-09 11:18:33,017 Writing example 200000 of 505555
2021-08-09 11:18:38,386 Writing example 210000 of 505555
2021-08-09 11:18:42,805 Writing example 220000 of 505555
2021-08-09 11:18:47,400 Writing example 230000 of 505555
2021-08-09 11:18:51,897 Writing example 240000 of 505555
2021-08-09 11:18:56,506 Writing example 250000 of 505555
2021-08-09 11:19:01,030 Writing example 260000 of 505555
2021-08-09 11:19:05,511 Writing example 270000 of 505555
2021-08-09 11:19:09,935 Writing example 280000 of 505555
2021-08-09 11:19:15,471 Writing example 290000 of 505555
2021-08-09 11:19:20,197 Writing example 300000 of 505555
2021-08-09 11:19:24,733 Writing example 310000 of 505555
2021-08-09 11:19:29,363 Writing example 320000 of 505555
2021-08-09 11:19:34,055 Writing example 330000 of 505555
2021-08-09 11:19:38,456 Writing example 340000 of 505555
2021-08-09 11:19:42,924 Writing example 350000 of 505555
2021-08-09 11:19:47,378 Writing example 360000 of 505555
2021-08-09 11:19:51,733 Writing example 370000 of 505555
2021-08-09 11:19:56,187 Writing example 380000 of 505555
2021-08-09 11:20:02,586 Writing example 390000 of 505555
2021-08-09 11:20:07,333 Writing example 400000 of 505555
2021-08-09 11:20:11,957 Writing example 410000 of 505555
2021-08-09 11:20:16,539 Writing example 420000 of 505555
2021-08-09 11:20:20,891 Writing example 430000 of 505555
2021-08-09 11:20:25,266 Writing example 440000 of 505555
2021-08-09 11:20:29,925 Writing example 450000 of 505555
2021-08-09 11:20:34,601 Writing example 460000 of 505555
2021-08-09 11:20:39,379 Writing example 470000 of 505555
2021-08-09 11:20:44,237 Writing example 480000 of 505555
2021-08-09 11:20:48,824 Writing example 490000 of 505555
2021-08-09 11:20:53,329 Writing example 500000 of 505555
2021-08-09 11:21:00,870 Writing example 0 of 9815
2021-08-09 11:21:00,871 *** Example ***
2021-08-09 11:21:00,871 guid: dev_matched-0
2021-08-09 11:21:00,871 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-09 11:21:00,871 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:21:00,871 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:21:00,871 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 11:21:00,871 label: neutral
2021-08-09 11:21:00,871 label_id: 2
2021-08-09 11:21:05,500 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-09 11:21:08,086 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-09 11:21:16,857 loading model...
2021-08-09 11:21:16,891 done!
2021-08-09 11:21:16,891 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-09 11:21:16,891 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-09 11:21:28,287 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-09 11:21:29,772 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-08-09 11:21:32,199 loading model...
2021-08-09 11:21:32,212 done!
2021-08-09 11:21:32,212 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-09 11:21:32,212 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-09 11:21:32,279 ***** Running training *****
2021-08-09 11:21:32,279   Num examples = 505555
2021-08-09 11:21:32,279   Batch size = 32
2021-08-09 11:21:32,279   Num steps = 157980
2021-08-09 11:21:32,280 n: module.bert.embeddings.word_embeddings.weight
2021-08-09 11:21:32,280 n: module.bert.embeddings.position_embeddings.weight
2021-08-09 11:21:32,280 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-09 11:21:32,280 n: module.bert.embeddings.LayerNorm.weight
2021-08-09 11:21:32,281 n: module.bert.embeddings.LayerNorm.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-09 11:21:32,281 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-09 11:21:32,282 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-09 11:21:32,283 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-09 11:21:32,284 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-09 11:21:32,285 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-09 11:21:32,285 n: module.bert.pooler.dense.weight
2021-08-09 11:21:32,285 n: module.bert.pooler.dense.bias
2021-08-09 11:21:32,285 n: module.classifier.weight
2021-08-09 11:21:32,285 n: module.classifier.bias
2021-08-09 11:21:32,286 n: module.fit_dense.weight
2021-08-09 11:21:32,286 n: module.fit_dense.bias
2021-08-09 11:21:32,286 Total parameters: 67547907
2021-08-09 11:31:04,277 ***** Running evaluation *****
2021-08-09 11:31:04,278   Epoch = 0 iter 2999 step
2021-08-09 11:31:04,278   Num examples = 9815
2021-08-09 11:31:04,278   Batch size = 32
2021-08-09 11:31:04,328 ***** Eval results *****
2021-08-09 11:31:04,329   att_loss = 2.041318833172103
2021-08-09 11:31:04,329   cls_loss = 0.0
2021-08-09 11:31:04,329   global_step = 2999
2021-08-09 11:31:04,329   loss = 2.998632909695917
2021-08-09 11:31:04,329   rep_loss = 0.9573140753710735
2021-08-09 11:31:04,329 ***** Save model *****
2021-08-09 11:40:28,913 ***** Running evaluation *****
2021-08-09 11:40:28,913   Epoch = 0 iter 5999 step
2021-08-09 11:40:28,913   Num examples = 9815
2021-08-09 11:40:28,913   Batch size = 32
2021-08-09 11:40:28,975 ***** Eval results *****
2021-08-09 11:40:28,975   att_loss = 1.9818612202383474
2021-08-09 11:40:28,975   cls_loss = 0.0
2021-08-09 11:40:28,975   global_step = 5999
2021-08-09 11:40:28,975   loss = 2.920272602480955
2021-08-09 11:40:28,975   rep_loss = 0.9384113804342986
2021-08-09 11:40:28,976 ***** Save model *****
2021-08-09 11:49:35,726 ***** Running evaluation *****
2021-08-09 11:49:35,726   Epoch = 0 iter 8999 step
2021-08-09 11:49:35,726   Num examples = 9815
2021-08-09 11:49:35,726   Batch size = 32
2021-08-09 11:49:35,729 ***** Eval results *****
2021-08-09 11:49:35,729   att_loss = 1.952979224683815
2021-08-09 11:49:35,729   cls_loss = 0.0
2021-08-09 11:49:35,729   global_step = 8999
2021-08-09 11:49:35,729   loss = 2.881038705777269
2021-08-09 11:49:35,729   rep_loss = 0.9280594791130352
2021-08-09 11:49:35,730 ***** Save model *****
2021-08-09 11:58:52,723 ***** Running evaluation *****
2021-08-09 11:58:52,723   Epoch = 0 iter 11999 step
2021-08-09 11:58:52,723   Num examples = 9815
2021-08-09 11:58:52,723   Batch size = 32
2021-08-09 11:58:52,726 ***** Eval results *****
2021-08-09 11:58:52,726   att_loss = 1.9325415412429294
2021-08-09 11:58:52,726   cls_loss = 0.0
2021-08-09 11:58:52,726   global_step = 11999
2021-08-09 11:58:52,726   loss = 2.8531555290152224
2021-08-09 11:58:52,726   rep_loss = 0.9206139862472807
2021-08-09 11:58:52,726 ***** Save model *****
2021-08-09 12:08:19,980 ***** Running evaluation *****
2021-08-09 12:08:19,980   Epoch = 0 iter 14999 step
2021-08-09 12:08:19,981   Num examples = 9815
2021-08-09 12:08:19,981   Batch size = 32
2021-08-09 12:08:20,043 ***** Eval results *****
2021-08-09 12:08:20,043   att_loss = 1.9172977366219506
2021-08-09 12:08:20,043   cls_loss = 0.0
2021-08-09 12:08:20,043   global_step = 14999
2021-08-09 12:08:20,043   loss = 2.832207025388963
2021-08-09 12:08:20,043   rep_loss = 0.9149092872052663
2021-08-09 12:08:20,044 ***** Save model *****
2021-08-09 12:17:33,354 ***** Running evaluation *****
2021-08-09 12:17:33,355   Epoch = 1 iter 17999 step
2021-08-09 12:17:33,355   Num examples = 9815
2021-08-09 12:17:33,355   Batch size = 32
2021-08-09 12:17:44,709 ***** Eval results *****
2021-08-09 12:17:44,709   att_loss = 1.840045294345698
2021-08-09 12:17:44,709   cls_loss = 0.0
2021-08-09 12:17:44,709   global_step = 17999
2021-08-09 12:17:44,709   loss = 2.7258943853893913
2021-08-09 12:17:44,709   rep_loss = 0.8858490893105277
2021-08-09 12:17:44,709 ***** Save model *****
2021-08-09 12:26:58,709 ***** Running evaluation *****
2021-08-09 12:26:58,710   Epoch = 1 iter 20999 step
2021-08-09 12:26:58,710   Num examples = 9815
2021-08-09 12:26:58,710   Batch size = 32
2021-08-09 12:26:58,853 ***** Eval results *****
2021-08-09 12:26:58,854   att_loss = 1.8341067144903305
2021-08-09 12:26:58,854   cls_loss = 0.0
2021-08-09 12:26:58,854   global_step = 20999
2021-08-09 12:26:58,854   loss = 2.717950952211221
2021-08-09 12:26:58,854   rep_loss = 0.8838442374687657
2021-08-09 12:26:58,885 ***** Save model *****
2021-08-09 12:36:18,324 ***** Running evaluation *****
2021-08-09 12:36:18,324   Epoch = 1 iter 23999 step
2021-08-09 12:36:18,324   Num examples = 9815
2021-08-09 12:36:18,325   Batch size = 32
2021-08-09 12:36:18,386 ***** Eval results *****
2021-08-09 12:36:18,386   att_loss = 1.828688274184577
2021-08-09 12:36:18,386   cls_loss = 0.0
2021-08-09 12:36:18,386   global_step = 23999
2021-08-09 12:36:18,387   loss = 2.71067354827665
2021-08-09 12:36:18,387   rep_loss = 0.8819852738522302
2021-08-09 12:36:18,387 ***** Save model *****
2021-08-09 12:45:39,453 ***** Running evaluation *****
2021-08-09 12:45:39,453   Epoch = 1 iter 26999 step
2021-08-09 12:45:39,453   Num examples = 9815
2021-08-09 12:45:39,453   Batch size = 32
2021-08-09 12:45:39,502 ***** Eval results *****
2021-08-09 12:45:39,502   att_loss = 1.8253285830472794
2021-08-09 12:45:39,502   cls_loss = 0.0
2021-08-09 12:45:39,503   global_step = 26999
2021-08-09 12:45:39,503   loss = 2.7057771798957937
2021-08-09 12:45:39,503   rep_loss = 0.8804485969176922
2021-08-09 12:45:39,503 ***** Save model *****
2021-08-09 12:54:53,781 ***** Running evaluation *****
2021-08-09 12:54:53,781   Epoch = 1 iter 29999 step
2021-08-09 12:54:53,781   Num examples = 9815
2021-08-09 12:54:53,781   Batch size = 32
2021-08-09 12:54:53,783 ***** Eval results *****
2021-08-09 12:54:53,783   att_loss = 1.8207121121195888
2021-08-09 12:54:53,783   cls_loss = 0.0
2021-08-09 12:54:53,783   global_step = 29999
2021-08-09 12:54:53,783   loss = 2.6995749471892894
2021-08-09 12:54:53,783   rep_loss = 0.8788628350487142
2021-08-09 12:54:53,784 ***** Save model *****
2021-08-09 13:04:10,263 ***** Running evaluation *****
2021-08-09 13:04:10,264   Epoch = 2 iter 32999 step
2021-08-09 13:04:10,264   Num examples = 9815
2021-08-09 13:04:10,264   Batch size = 32
2021-08-09 13:04:10,327 ***** Eval results *****
2021-08-09 13:04:10,327   att_loss = 1.7955191813446503
2021-08-09 13:04:10,327   cls_loss = 0.0
2021-08-09 13:04:10,327   global_step = 32999
2021-08-09 13:04:10,327   loss = 2.6651763375622837
2021-08-09 13:04:10,327   rep_loss = 0.869657156727438
2021-08-09 13:04:10,327 ***** Save model *****
2021-08-09 13:13:33,500 ***** Running evaluation *****
2021-08-09 13:13:33,500   Epoch = 2 iter 35999 step
2021-08-09 13:13:33,500   Num examples = 9815
2021-08-09 13:13:33,500   Batch size = 32
2021-08-09 13:13:33,550 ***** Eval results *****
2021-08-09 13:13:33,550   att_loss = 1.7948437950271598
2021-08-09 13:13:33,550   cls_loss = 0.0
2021-08-09 13:13:33,550   global_step = 35999
2021-08-09 13:13:33,551   loss = 2.6635549018092246
2021-08-09 13:13:33,551   rep_loss = 0.8687111063894835
2021-08-09 13:13:33,551 ***** Save model *****
2021-08-09 13:22:34,484 ***** Running evaluation *****
2021-08-09 13:22:34,485   Epoch = 2 iter 38999 step
2021-08-09 13:22:34,485   Num examples = 9815
2021-08-09 13:22:34,485   Batch size = 32
2021-08-09 13:22:34,487 ***** Eval results *****
2021-08-09 13:22:34,487   att_loss = 1.7927796539705734
2021-08-09 13:22:34,487   cls_loss = 0.0
2021-08-09 13:22:34,487   global_step = 38999
2021-08-09 13:22:34,487   loss = 2.6606623912266616
2021-08-09 13:22:34,487   rep_loss = 0.8678827367327462
2021-08-09 13:22:34,487 ***** Save model *****
2021-08-09 13:31:41,270 ***** Running evaluation *****
2021-08-09 13:31:41,270   Epoch = 2 iter 41999 step
2021-08-09 13:31:41,270   Num examples = 9815
2021-08-09 13:31:41,271   Batch size = 32
2021-08-09 13:31:41,483 ***** Eval results *****
2021-08-09 13:31:41,483   att_loss = 1.7921780583767504
2021-08-09 13:31:41,483   cls_loss = 0.0
2021-08-09 13:31:41,483   global_step = 41999
2021-08-09 13:31:41,483   loss = 2.6592505465192793
2021-08-09 13:31:41,483   rep_loss = 0.867072487013805
2021-08-09 13:31:41,524 ***** Save model *****
2021-08-09 13:41:21,578 ***** Running evaluation *****
2021-08-09 13:41:21,579   Epoch = 2 iter 44999 step
2021-08-09 13:41:21,579   Num examples = 9815
2021-08-09 13:41:21,579   Batch size = 32
2021-08-09 13:41:21,629 ***** Eval results *****
2021-08-09 13:41:21,629   att_loss = 1.7915799700572563
2021-08-09 13:41:21,629   cls_loss = 0.0
2021-08-09 13:41:21,629   global_step = 44999
2021-08-09 13:41:21,629   loss = 2.6578805443422837
2021-08-09 13:41:21,629   rep_loss = 0.8663005734133932
2021-08-09 13:41:21,630 ***** Save model *****
2021-08-09 13:51:06,064 ***** Running evaluation *****
2021-08-09 13:51:06,064   Epoch = 3 iter 47999 step
2021-08-09 13:51:06,065   Num examples = 9815
2021-08-09 13:51:06,065   Batch size = 32
2021-08-09 13:51:06,066 ***** Eval results *****
2021-08-09 13:51:06,066   att_loss = 1.76070080059619
2021-08-09 13:51:06,067   cls_loss = 0.0
2021-08-09 13:51:06,067   global_step = 47999
2021-08-09 13:51:06,067   loss = 2.621019790192281
2021-08-09 13:51:06,067   rep_loss = 0.8603189857538082
2021-08-09 13:51:06,067 ***** Save model *****
2021-08-09 14:00:18,588 ***** Running evaluation *****
2021-08-09 14:00:18,588   Epoch = 3 iter 50999 step
2021-08-09 14:00:18,588   Num examples = 9815
2021-08-09 14:00:18,588   Batch size = 32
2021-08-09 14:00:18,591 ***** Eval results *****
2021-08-09 14:00:18,591   att_loss = 1.7735242060583276
2021-08-09 14:00:18,591   cls_loss = 0.0
2021-08-09 14:00:18,591   global_step = 50999
2021-08-09 14:00:18,591   loss = 2.633739892057506
2021-08-09 14:00:18,591   rep_loss = 0.860215687189618
2021-08-09 14:00:18,591 ***** Save model *****
2021-08-09 14:09:21,336 ***** Running evaluation *****
2021-08-09 14:09:21,336   Epoch = 3 iter 53999 step
2021-08-09 14:09:21,336   Num examples = 9815
2021-08-09 14:09:21,336   Batch size = 32
2021-08-09 14:09:21,457 ***** Eval results *****
2021-08-09 14:09:21,457   att_loss = 1.7758013851438663
2021-08-09 14:09:21,457   cls_loss = 0.0
2021-08-09 14:09:21,457   global_step = 53999
2021-08-09 14:09:21,457   loss = 2.635732605708178
2021-08-09 14:09:21,457   rep_loss = 0.8599312209162543
2021-08-09 14:09:21,493 ***** Save model *****
2021-08-09 14:18:48,479 ***** Running evaluation *****
2021-08-09 14:18:48,480   Epoch = 3 iter 56999 step
2021-08-09 14:18:48,480   Num examples = 9815
2021-08-09 14:18:48,480   Batch size = 32
2021-08-09 14:18:48,543 ***** Eval results *****
2021-08-09 14:18:48,543   att_loss = 1.7750749700085562
2021-08-09 14:18:48,543   cls_loss = 0.0
2021-08-09 14:18:48,543   global_step = 56999
2021-08-09 14:18:48,543   loss = 2.6343585936989653
2021-08-09 14:18:48,543   rep_loss = 0.8592836232808404
2021-08-09 14:18:48,543 ***** Save model *****
2021-08-09 14:28:12,833 ***** Running evaluation *****
2021-08-09 14:28:12,834   Epoch = 3 iter 59999 step
2021-08-09 14:28:12,834   Num examples = 9815
2021-08-09 14:28:12,834   Batch size = 32
2021-08-09 14:28:12,836 ***** Eval results *****
2021-08-09 14:28:12,836   att_loss = 1.7736956118213332
2021-08-09 14:28:12,836   cls_loss = 0.0
2021-08-09 14:28:12,836   global_step = 59999
2021-08-09 14:28:12,836   loss = 2.6324494387412156
2021-08-09 14:28:12,836   rep_loss = 0.8587538265794197
2021-08-09 14:28:12,837 ***** Save model *****
2021-08-09 14:37:15,670 ***** Running evaluation *****
2021-08-09 14:37:15,671   Epoch = 3 iter 62999 step
2021-08-09 14:37:15,671   Num examples = 9815
2021-08-09 14:37:15,671   Batch size = 32
2021-08-09 14:37:15,722 ***** Eval results *****
2021-08-09 14:37:15,722   att_loss = 1.7729092726223135
2021-08-09 14:37:15,722   cls_loss = 0.0
2021-08-09 14:37:15,722   global_step = 62999
2021-08-09 14:37:15,722   loss = 2.6311258035682403
2021-08-09 14:37:15,722   rep_loss = 0.8582165307931432
2021-08-09 14:37:15,723 ***** Save model *****
2021-08-09 14:46:17,507 ***** Running evaluation *****
2021-08-09 14:46:17,507   Epoch = 4 iter 65999 step
2021-08-09 14:46:17,507   Num examples = 9815
2021-08-09 14:46:17,507   Batch size = 32
2021-08-09 14:46:17,509 ***** Eval results *****
2021-08-09 14:46:17,509   att_loss = 1.759354858751776
2021-08-09 14:46:17,509   cls_loss = 0.0
2021-08-09 14:46:17,509   global_step = 65999
2021-08-09 14:46:17,509   loss = 2.6136539458039394
2021-08-09 14:46:17,509   rep_loss = 0.8542990855657634
2021-08-09 14:46:17,509 ***** Save model *****
2021-08-09 14:55:33,452 ***** Running evaluation *****
2021-08-09 14:55:33,453   Epoch = 4 iter 68999 step
2021-08-09 14:55:33,453   Num examples = 9815
2021-08-09 14:55:33,453   Batch size = 32
2021-08-09 14:55:33,572 ***** Eval results *****
2021-08-09 14:55:33,573   att_loss = 1.760522898901796
2021-08-09 14:55:33,573   cls_loss = 0.0
2021-08-09 14:55:33,573   global_step = 68999
2021-08-09 14:55:33,573   loss = 2.6146254673951748
2021-08-09 14:55:33,573   rep_loss = 0.8541025667587163
2021-08-09 14:55:33,573 ***** Save model *****
2021-08-09 15:04:35,210 ***** Running evaluation *****
2021-08-09 15:04:35,210   Epoch = 4 iter 71999 step
2021-08-09 15:04:35,210   Num examples = 9815
2021-08-09 15:04:35,211   Batch size = 32
2021-08-09 15:04:35,261 ***** Eval results *****
2021-08-09 15:04:35,261   att_loss = 1.7629750953896086
2021-08-09 15:04:35,261   cls_loss = 0.0
2021-08-09 15:04:35,261   global_step = 71999
2021-08-09 15:04:35,261   loss = 2.6169756364508356
2021-08-09 15:04:35,261   rep_loss = 0.854000539545224
2021-08-09 15:04:35,292 ***** Save model *****
2021-08-09 15:13:36,066 ***** Running evaluation *****
2021-08-09 15:13:36,067   Epoch = 4 iter 74999 step
2021-08-09 15:13:36,067   Num examples = 9815
2021-08-09 15:13:36,067   Batch size = 32
2021-08-09 15:13:36,069 ***** Eval results *****
2021-08-09 15:13:36,069   att_loss = 1.7611886679777862
2021-08-09 15:13:36,069   cls_loss = 0.0
2021-08-09 15:13:36,069   global_step = 74999
2021-08-09 15:13:36,069   loss = 2.6147963744755813
2021-08-09 15:13:36,069   rep_loss = 0.853607704892453
2021-08-09 15:13:36,069 ***** Save model *****
2021-08-09 15:22:58,208 ***** Running evaluation *****
2021-08-09 15:22:58,235   Epoch = 4 iter 77999 step
2021-08-09 15:22:58,235   Num examples = 9815
2021-08-09 15:22:58,235   Batch size = 32
2021-08-09 15:22:58,331 ***** Eval results *****
2021-08-09 15:22:58,331   att_loss = 1.7610684205673093
2021-08-09 15:22:58,331   cls_loss = 0.0
2021-08-09 15:22:58,331   global_step = 77999
2021-08-09 15:22:58,331   loss = 2.6143347412808704
2021-08-09 15:22:58,331   rep_loss = 0.8532663193529636
2021-08-09 15:22:58,354 ***** Save model *****
2021-08-09 15:32:02,142 ***** Running evaluation *****
2021-08-09 15:32:02,142   Epoch = 5 iter 80999 step
2021-08-09 15:32:02,142   Num examples = 9815
2021-08-09 15:32:02,142   Batch size = 32
2021-08-09 15:32:02,367 ***** Eval results *****
2021-08-09 15:32:02,367   att_loss = 1.7537161492185844
2021-08-09 15:32:02,367   cls_loss = 0.0
2021-08-09 15:32:02,367   global_step = 80999
2021-08-09 15:32:02,367   loss = 2.6041664435176366
2021-08-09 15:32:02,367   rep_loss = 0.8504502956044798
2021-08-09 15:32:02,367 ***** Save model *****
2021-08-09 15:41:10,366 ***** Running evaluation *****
2021-08-09 15:41:10,366   Epoch = 5 iter 83999 step
2021-08-09 15:41:10,366   Num examples = 9815
2021-08-09 15:41:10,366   Batch size = 32
2021-08-09 15:41:10,368 ***** Eval results *****
2021-08-09 15:41:10,368   att_loss = 1.7542252379707006
2021-08-09 15:41:10,368   cls_loss = 0.0
2021-08-09 15:41:10,368   global_step = 83999
2021-08-09 15:41:10,368   loss = 2.6043886548626993
2021-08-09 15:41:10,368   rep_loss = 0.8501634178558589
2021-08-09 15:41:10,368 ***** Save model *****
2021-08-09 15:50:31,755 ***** Running evaluation *****
2021-08-09 15:50:31,756   Epoch = 5 iter 86999 step
2021-08-09 15:50:31,756   Num examples = 9815
2021-08-09 15:50:31,756   Batch size = 32
2021-08-09 15:50:31,816 ***** Eval results *****
2021-08-09 15:50:31,816   att_loss = 1.7537281835164034
2021-08-09 15:50:31,816   cls_loss = 0.0
2021-08-09 15:50:31,816   global_step = 86999
2021-08-09 15:50:31,816   loss = 2.603607703169152
2021-08-09 15:50:31,816   rep_loss = 0.8498795208137332
2021-08-09 15:50:31,817 ***** Save model *****
2021-08-09 15:59:38,550 ***** Running evaluation *****
2021-08-09 15:59:38,550   Epoch = 5 iter 89999 step
2021-08-09 15:59:38,550   Num examples = 9815
2021-08-09 15:59:38,550   Batch size = 32
2021-08-09 15:59:38,783 ***** Eval results *****
2021-08-09 15:59:38,783   att_loss = 1.754623024683902
2021-08-09 15:59:38,783   cls_loss = 0.0
2021-08-09 15:59:38,783   global_step = 89999
2021-08-09 15:59:38,784   loss = 2.60428655281824
2021-08-09 15:59:38,784   rep_loss = 0.8496635285458152
2021-08-09 15:59:38,799 ***** Save model *****
2021-08-09 16:08:45,097 ***** Running evaluation *****
2021-08-09 16:08:45,098   Epoch = 5 iter 92999 step
2021-08-09 16:08:45,098   Num examples = 9815
2021-08-09 16:08:45,098   Batch size = 32
2021-08-09 16:08:45,160 ***** Eval results *****
2021-08-09 16:08:45,160   att_loss = 1.752597479342359
2021-08-09 16:08:45,160   cls_loss = 0.0
2021-08-09 16:08:45,160   global_step = 92999
2021-08-09 16:08:45,160   loss = 2.6018534401974556
2021-08-09 16:08:45,160   rep_loss = 0.8492559610720883
2021-08-09 16:08:45,160 ***** Save model *****
2021-08-09 16:17:56,440 ***** Running evaluation *****
2021-08-09 16:17:56,441   Epoch = 6 iter 95999 step
2021-08-09 16:17:56,441   Num examples = 9815
2021-08-09 16:17:56,441   Batch size = 32
2021-08-09 16:17:56,501 ***** Eval results *****
2021-08-09 16:17:56,501   att_loss = 1.7341926711143096
2021-08-09 16:17:56,501   cls_loss = 0.0
2021-08-09 16:17:56,501   global_step = 95999
2021-08-09 16:17:56,501   loss = 2.579946852242228
2021-08-09 16:17:56,501   rep_loss = 0.8457541792083633
2021-08-09 16:17:56,501 ***** Save model *****
2021-08-09 16:27:16,228 ***** Running evaluation *****
2021-08-09 16:27:16,229   Epoch = 6 iter 98999 step
2021-08-09 16:27:16,229   Num examples = 9815
2021-08-09 16:27:16,229   Batch size = 32
2021-08-09 16:27:16,279 ***** Eval results *****
2021-08-09 16:27:16,279   att_loss = 1.7443822066310013
2021-08-09 16:27:16,279   cls_loss = 0.0
2021-08-09 16:27:16,279   global_step = 98999
2021-08-09 16:27:16,279   loss = 2.590935862678667
2021-08-09 16:27:16,279   rep_loss = 0.8465536546180605
2021-08-09 16:27:16,292 ***** Save model *****
2021-08-09 16:36:44,342 ***** Running evaluation *****
2021-08-09 16:36:44,342   Epoch = 6 iter 101999 step
2021-08-09 16:36:44,342   Num examples = 9815
2021-08-09 16:36:44,342   Batch size = 32
2021-08-09 16:36:44,344 ***** Eval results *****
2021-08-09 16:36:44,344   att_loss = 1.7463634188537904
2021-08-09 16:36:44,344   cls_loss = 0.0
2021-08-09 16:36:44,344   global_step = 101999
2021-08-09 16:36:44,344   loss = 2.592939681971128
2021-08-09 16:36:44,344   rep_loss = 0.8465762619105313
2021-08-09 16:36:44,345 ***** Save model *****
2021-08-09 16:45:51,001 ***** Running evaluation *****
2021-08-09 16:45:51,002   Epoch = 6 iter 104999 step
2021-08-09 16:45:51,002   Num examples = 9815
2021-08-09 16:45:51,002   Batch size = 32
2021-08-09 16:45:51,003 ***** Eval results *****
2021-08-09 16:45:51,004   att_loss = 1.7461483307046342
2021-08-09 16:45:51,004   cls_loss = 0.0
2021-08-09 16:45:51,004   global_step = 104999
2021-08-09 16:45:51,004   loss = 2.5925796087172737
2021-08-09 16:45:51,004   rep_loss = 0.8464312775573305
2021-08-09 16:45:51,004 ***** Save model *****
2021-08-09 16:54:59,149 ***** Running evaluation *****
2021-08-09 16:54:59,149   Epoch = 6 iter 107999 step
2021-08-09 16:54:59,149   Num examples = 9815
2021-08-09 16:54:59,149   Batch size = 32
2021-08-09 16:54:59,279 ***** Eval results *****
2021-08-09 16:54:59,279   att_loss = 1.7450394602459318
2021-08-09 16:54:59,279   cls_loss = 0.0
2021-08-09 16:54:59,279   global_step = 107999
2021-08-09 16:54:59,280   loss = 2.591269098677378
2021-08-09 16:54:59,280   rep_loss = 0.8462296386389861
2021-08-09 16:54:59,296 ***** Save model *****
2021-08-09 17:04:18,844 ***** Running evaluation *****
2021-08-09 17:04:18,844   Epoch = 7 iter 110999 step
2021-08-09 17:04:18,844   Num examples = 9815
2021-08-09 17:04:18,844   Batch size = 32
2021-08-09 17:04:18,848 ***** Eval results *****
2021-08-09 17:04:18,848   att_loss = 1.7377843037067255
2021-08-09 17:04:18,848   cls_loss = 0.0
2021-08-09 17:04:18,848   global_step = 110999
2021-08-09 17:04:18,848   loss = 2.581252551251982
2021-08-09 17:04:18,848   rep_loss = 0.8434682501430373
2021-08-09 17:04:18,848 ***** Save model *****
2021-08-09 17:13:38,561 ***** Running evaluation *****
2021-08-09 17:13:38,561   Epoch = 7 iter 113999 step
2021-08-09 17:13:38,561   Num examples = 9815
2021-08-09 17:13:38,561   Batch size = 32
2021-08-09 17:13:38,563 ***** Eval results *****
2021-08-09 17:13:38,563   att_loss = 1.7344324970538756
2021-08-09 17:13:38,563   cls_loss = 0.0
2021-08-09 17:13:38,563   global_step = 113999
2021-08-09 17:13:38,563   loss = 2.578006728737018
2021-08-09 17:13:38,563   rep_loss = 0.8435742318927106
2021-08-09 17:13:38,564 ***** Save model *****
2021-08-09 17:22:45,222 ***** Running evaluation *****
2021-08-09 17:22:45,222   Epoch = 7 iter 116999 step
2021-08-09 17:22:45,222   Num examples = 9815
2021-08-09 17:22:45,222   Batch size = 32
2021-08-09 17:22:45,338 ***** Eval results *****
2021-08-09 17:22:45,338   att_loss = 1.7361562971427105
2021-08-09 17:22:45,338   cls_loss = 0.0
2021-08-09 17:22:45,338   global_step = 116999
2021-08-09 17:22:45,338   loss = 2.5797519978468677
2021-08-09 17:22:45,338   rep_loss = 0.8435957007878064
2021-08-09 17:22:45,338 ***** Save model *****
2021-08-09 17:32:03,218 ***** Running evaluation *****
2021-08-09 17:32:03,218   Epoch = 7 iter 119999 step
2021-08-09 17:32:03,218   Num examples = 9815
2021-08-09 17:32:03,218   Batch size = 32
2021-08-09 17:32:03,220 ***** Eval results *****
2021-08-09 17:32:03,220   att_loss = 1.7366149652198006
2021-08-09 17:32:03,220   cls_loss = 0.0
2021-08-09 17:32:03,220   global_step = 119999
2021-08-09 17:32:03,220   loss = 2.580174814067915
2021-08-09 17:32:03,220   rep_loss = 0.8435598481959014
2021-08-09 17:32:03,220 ***** Save model *****
2021-08-09 17:41:30,353 ***** Running evaluation *****
2021-08-09 17:41:30,353   Epoch = 7 iter 122999 step
2021-08-09 17:41:30,353   Num examples = 9815
2021-08-09 17:41:30,353   Batch size = 32
2021-08-09 17:41:30,416 ***** Eval results *****
2021-08-09 17:41:30,416   att_loss = 1.7378044352085442
2021-08-09 17:41:30,416   cls_loss = 0.0
2021-08-09 17:41:30,416   global_step = 122999
2021-08-09 17:41:30,416   loss = 2.5813298136953824
2021-08-09 17:41:30,416   rep_loss = 0.8435253783908021
2021-08-09 17:41:30,489 ***** Save model *****
2021-08-09 17:50:38,710 ***** Running evaluation *****
2021-08-09 17:50:38,710   Epoch = 7 iter 125999 step
2021-08-09 17:50:38,710   Num examples = 9815
2021-08-09 17:50:38,710   Batch size = 32
2021-08-09 17:50:38,765 ***** Eval results *****
2021-08-09 17:50:38,765   att_loss = 1.7384518122584862
2021-08-09 17:50:38,765   cls_loss = 0.0
2021-08-09 17:50:38,765   global_step = 125999
2021-08-09 17:50:38,765   loss = 2.5818767886448595
2021-08-09 17:50:38,765   rep_loss = 0.843424976084734
2021-08-09 17:50:38,765 ***** Save model *****
2021-08-09 17:59:44,577 ***** Running evaluation *****
2021-08-09 17:59:44,577   Epoch = 8 iter 128999 step
2021-08-09 17:59:44,577   Num examples = 9815
2021-08-09 17:59:44,577   Batch size = 32
2021-08-09 17:59:44,579 ***** Eval results *****
2021-08-09 17:59:44,579   att_loss = 1.7334294149114342
2021-08-09 17:59:44,579   cls_loss = 0.0
2021-08-09 17:59:44,579   global_step = 128999
2021-08-09 17:59:44,579   loss = 2.5752522948139256
2021-08-09 17:59:44,579   rep_loss = 0.8418228787856166
2021-08-09 17:59:44,579 ***** Save model *****
2021-08-09 18:09:03,723 ***** Running evaluation *****
2021-08-09 18:09:03,723   Epoch = 8 iter 131999 step
2021-08-09 18:09:03,723   Num examples = 9815
2021-08-09 18:09:03,723   Batch size = 32
2021-08-09 18:09:03,725 ***** Eval results *****
2021-08-09 18:09:03,725   att_loss = 1.7334902545754016
2021-08-09 18:09:03,725   cls_loss = 0.0
2021-08-09 18:09:03,725   global_step = 131999
2021-08-09 18:09:03,725   loss = 2.575274602621661
2021-08-09 18:09:03,725   rep_loss = 0.8417843474093443
2021-08-09 18:09:03,725 ***** Save model *****
2021-08-09 18:18:05,162 ***** Running evaluation *****
2021-08-09 18:18:05,162   Epoch = 8 iter 134999 step
2021-08-09 18:18:05,162   Num examples = 9815
2021-08-09 18:18:05,162   Batch size = 32
2021-08-09 18:18:05,409 ***** Eval results *****
2021-08-09 18:18:05,409   att_loss = 1.7327735347382496
2021-08-09 18:18:05,409   cls_loss = 0.0
2021-08-09 18:18:05,409   global_step = 134999
2021-08-09 18:18:05,409   loss = 2.574303539879988
2021-08-09 18:18:05,409   rep_loss = 0.8415300046712662
2021-08-09 18:18:05,410 ***** Save model *****
2021-08-09 18:27:06,667 ***** Running evaluation *****
2021-08-09 18:27:06,667   Epoch = 8 iter 137999 step
2021-08-09 18:27:06,667   Num examples = 9815
2021-08-09 18:27:06,668   Batch size = 32
2021-08-09 18:27:06,669 ***** Eval results *****
2021-08-09 18:27:06,669   att_loss = 1.7325923050368495
2021-08-09 18:27:06,670   cls_loss = 0.0
2021-08-09 18:27:06,670   global_step = 137999
2021-08-09 18:27:06,670   loss = 2.5739935500633084
2021-08-09 18:27:06,670   rep_loss = 0.8414012449340886
2021-08-09 18:27:06,670 ***** Save model *****
2021-08-09 18:36:19,877 ***** Running evaluation *****
2021-08-09 18:36:19,878   Epoch = 8 iter 140999 step
2021-08-09 18:36:19,878   Num examples = 9815
2021-08-09 18:36:19,878   Batch size = 32
2021-08-09 18:36:19,880 ***** Eval results *****
2021-08-09 18:36:19,880   att_loss = 1.7330465371503514
2021-08-09 18:36:19,881   cls_loss = 0.0
2021-08-09 18:36:19,881   global_step = 140999
2021-08-09 18:36:19,881   loss = 2.57438248487682
2021-08-09 18:36:19,881   rep_loss = 0.8413359481832404
2021-08-09 18:36:19,881 ***** Save model *****
2021-08-09 18:45:42,797 ***** Running evaluation *****
2021-08-09 18:45:42,798   Epoch = 9 iter 143999 step
2021-08-09 18:45:42,798   Num examples = 9815
2021-08-09 18:45:42,798   Batch size = 32
2021-08-09 18:45:42,912 ***** Eval results *****
2021-08-09 18:45:42,912   att_loss = 1.7291454461228408
2021-08-09 18:45:42,912   cls_loss = 0.0
2021-08-09 18:45:42,912   global_step = 143999
2021-08-09 18:45:42,912   loss = 2.568726996766648
2021-08-09 18:45:42,912   rep_loss = 0.8395815503485726
2021-08-09 18:45:42,913 ***** Save model *****
2021-08-09 18:54:45,930 ***** Running evaluation *****
2021-08-09 18:54:45,931   Epoch = 9 iter 146999 step
2021-08-09 18:54:45,931   Num examples = 9815
2021-08-09 18:54:45,931   Batch size = 32
2021-08-09 18:54:45,933 ***** Eval results *****
2021-08-09 18:54:45,933   att_loss = 1.728043568933656
2021-08-09 18:54:45,933   cls_loss = 0.0
2021-08-09 18:54:45,933   global_step = 146999
2021-08-09 18:54:45,933   loss = 2.5678844654834787
2021-08-09 18:54:45,933   rep_loss = 0.8398408957455251
2021-08-09 18:54:45,934 ***** Save model *****
2021-08-09 19:03:54,115 ***** Running evaluation *****
2021-08-09 19:03:54,115   Epoch = 9 iter 149999 step
2021-08-09 19:03:54,115   Num examples = 9815
2021-08-09 19:03:54,115   Batch size = 32
2021-08-09 19:03:54,118 ***** Eval results *****
2021-08-09 19:03:54,118   att_loss = 1.7276233381912107
2021-08-09 19:03:54,118   cls_loss = 0.0
2021-08-09 19:03:54,118   global_step = 149999
2021-08-09 19:03:54,118   loss = 2.5672845829169098
2021-08-09 19:03:54,118   rep_loss = 0.8396612442300739
2021-08-09 19:03:54,118 ***** Save model *****
2021-08-09 19:13:09,136 ***** Running evaluation *****
2021-08-09 19:13:09,137   Epoch = 9 iter 152999 step
2021-08-09 19:13:09,137   Num examples = 9815
2021-08-09 19:13:09,137   Batch size = 32
2021-08-09 19:13:09,316 ***** Eval results *****
2021-08-09 19:13:09,316   att_loss = 1.7294965192153493
2021-08-09 19:13:09,316   cls_loss = 0.0
2021-08-09 19:13:09,316   global_step = 152999
2021-08-09 19:13:09,316   loss = 2.569195469657628
2021-08-09 19:13:09,316   rep_loss = 0.8396989497590045
2021-08-09 19:13:09,316 ***** Save model *****
2021-08-09 21:07:41,678 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-09 21:07:41,961 device: cuda n_gpu: 4
2021-08-09 21:07:53,945 Writing example 0 of 505555
2021-08-09 21:07:53,946 *** Example ***
2021-08-09 21:07:53,946 guid: aug-0
2021-08-09 21:07:53,946 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-09 21:07:53,946 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:07:53,946 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:07:53,946 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:07:53,946 label: neutral
2021-08-09 21:07:53,946 label_id: 2
2021-08-09 21:07:58,603 Writing example 10000 of 505555
2021-08-09 21:08:03,216 Writing example 20000 of 505555
2021-08-09 21:08:07,736 Writing example 30000 of 505555
2021-08-09 21:08:12,408 Writing example 40000 of 505555
2021-08-09 21:08:17,166 Writing example 50000 of 505555
2021-08-09 21:08:21,628 Writing example 60000 of 505555
2021-08-09 21:08:26,224 Writing example 70000 of 505555
2021-08-09 21:08:30,762 Writing example 80000 of 505555
2021-08-09 21:08:35,645 Writing example 90000 of 505555
2021-08-09 21:08:45,528 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=300, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-09 21:08:45,651 device: cuda n_gpu: 4
2021-08-09 21:08:53,145 Writing example 0 of 505555
2021-08-09 21:08:53,147 *** Example ***
2021-08-09 21:08:53,147 guid: aug-0
2021-08-09 21:08:53,147 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-09 21:08:53,147 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:08:53,147 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:08:53,147 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:08:53,147 label: neutral
2021-08-09 21:08:53,147 label_id: 2
2021-08-09 21:08:57,870 Writing example 10000 of 505555
2021-08-09 21:09:02,427 Writing example 20000 of 505555
2021-08-09 21:09:06,910 Writing example 30000 of 505555
2021-08-09 21:09:11,573 Writing example 40000 of 505555
2021-08-09 21:09:16,330 Writing example 50000 of 505555
2021-08-09 21:09:21,044 Writing example 60000 of 505555
2021-08-09 21:09:25,665 Writing example 70000 of 505555
2021-08-09 21:09:30,201 Writing example 80000 of 505555
2021-08-09 21:09:35,066 Writing example 90000 of 505555
2021-08-09 21:09:39,610 Writing example 100000 of 505555
2021-08-09 21:09:44,164 Writing example 110000 of 505555
2021-08-09 21:09:48,561 Writing example 120000 of 505555
2021-08-09 21:09:53,187 Writing example 130000 of 505555
2021-08-09 21:09:58,458 Writing example 140000 of 505555
2021-08-09 21:10:02,889 Writing example 150000 of 505555
2021-08-09 21:10:07,674 Writing example 160000 of 505555
2021-08-09 21:10:12,295 Writing example 170000 of 505555
2021-08-09 21:10:16,843 Writing example 180000 of 505555
2021-08-09 21:10:21,411 Writing example 190000 of 505555
2021-08-09 21:10:25,944 Writing example 200000 of 505555
2021-08-09 21:10:31,292 Writing example 210000 of 505555
2021-08-09 21:10:35,897 Writing example 220000 of 505555
2021-08-09 21:10:40,499 Writing example 230000 of 505555
2021-08-09 21:10:45,003 Writing example 240000 of 505555
2021-08-09 21:10:49,599 Writing example 250000 of 505555
2021-08-09 21:10:54,132 Writing example 260000 of 505555
2021-08-09 21:10:58,587 Writing example 270000 of 505555
2021-08-09 21:11:03,057 Writing example 280000 of 505555
2021-08-09 21:11:08,661 Writing example 290000 of 505555
2021-08-09 21:11:13,300 Writing example 300000 of 505555
2021-08-09 21:11:17,864 Writing example 310000 of 505555
2021-08-09 21:11:22,248 Writing example 320000 of 505555
2021-08-09 21:11:26,886 Writing example 330000 of 505555
2021-08-09 21:11:31,281 Writing example 340000 of 505555
2021-08-09 21:11:35,901 Writing example 350000 of 505555
2021-08-09 21:11:40,350 Writing example 360000 of 505555
2021-08-09 21:11:44,707 Writing example 370000 of 505555
2021-08-09 21:11:49,142 Writing example 380000 of 505555
2021-08-09 21:11:55,109 Writing example 390000 of 505555
2021-08-09 21:11:59,668 Writing example 400000 of 505555
2021-08-09 21:12:04,233 Writing example 410000 of 505555
2021-08-09 21:12:08,634 Writing example 420000 of 505555
2021-08-09 21:12:12,979 Writing example 430000 of 505555
2021-08-09 21:12:17,335 Writing example 440000 of 505555
2021-08-09 21:12:21,907 Writing example 450000 of 505555
2021-08-09 21:12:26,484 Writing example 460000 of 505555
2021-08-09 21:12:31,008 Writing example 470000 of 505555
2021-08-09 21:12:35,626 Writing example 480000 of 505555
2021-08-09 21:12:40,217 Writing example 490000 of 505555
2021-08-09 21:12:44,783 Writing example 500000 of 505555
2021-08-09 21:12:52,736 Writing example 0 of 9815
2021-08-09 21:12:52,737 *** Example ***
2021-08-09 21:12:52,737 guid: dev_matched-0
2021-08-09 21:12:52,737 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-09 21:12:52,737 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:12:52,737 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:12:52,737 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:12:52,737 label: neutral
2021-08-09 21:12:52,737 label_id: 2
2021-08-09 21:12:57,128 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-09 21:12:59,569 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-09 21:13:14,807 loading model...
2021-08-09 21:13:14,841 done!
2021-08-09 21:13:14,841 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-09 21:13:14,841 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-09 21:13:22,823 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-09 21:13:24,312 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate/pytorch_model.bin
2021-08-09 21:13:27,632 loading model...
2021-08-09 21:13:27,646 done!
2021-08-09 21:13:27,713 ***** Running training *****
2021-08-09 21:13:27,713   Num examples = 505555
2021-08-09 21:13:27,713   Batch size = 32
2021-08-09 21:13:27,713   Num steps = 47394
2021-08-09 21:13:27,714 n: module.bert.embeddings.word_embeddings.weight
2021-08-09 21:13:27,715 n: module.bert.embeddings.position_embeddings.weight
2021-08-09 21:13:27,715 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-09 21:13:27,715 n: module.bert.embeddings.LayerNorm.weight
2021-08-09 21:13:27,715 n: module.bert.embeddings.LayerNorm.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-09 21:13:27,715 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-09 21:13:27,716 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-09 21:13:27,717 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-09 21:13:27,718 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-09 21:13:27,719 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-09 21:13:27,720 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-09 21:13:27,720 n: module.bert.pooler.dense.weight
2021-08-09 21:13:27,720 n: module.bert.pooler.dense.bias
2021-08-09 21:13:27,720 n: module.classifier.weight
2021-08-09 21:13:27,720 n: module.classifier.bias
2021-08-09 21:13:27,720 n: module.fit_dense.weight
2021-08-09 21:13:27,720 n: module.fit_dense.bias
2021-08-09 21:13:27,720 Total parameters: 67547907
2021-08-09 21:14:27,610 ***** Running evaluation *****
2021-08-09 21:14:27,611   Epoch = 0 iter 299 step
2021-08-09 21:14:27,611   Num examples = 9815
2021-08-09 21:14:27,611   Batch size = 32
2021-08-09 21:14:40,287 ***** Eval results *****
2021-08-09 21:14:40,288   acc = 0.10585838003056545
2021-08-09 21:14:40,288   att_loss = 0.0
2021-08-09 21:14:40,288   cls_loss = 0.2945678510693802
2021-08-09 21:14:40,288   eval_loss = 1.7849164638146515
2021-08-09 21:14:40,288   global_step = 299
2021-08-09 21:14:40,288   loss = 0.2945678510693802
2021-08-09 21:14:40,288   rep_loss = 0.0
2021-08-09 21:14:40,289 ***** Save model *****
2021-08-09 21:14:46,235 Writing example 0 of 9832
2021-08-09 21:14:46,236 *** Example ***
2021-08-09 21:14:46,236 guid: dev_matched-0
2021-08-09 21:14:46,236 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 21:14:46,236 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:14:46,236 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:14:46,236 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:14:46,236 label: contradiction
2021-08-09 21:14:46,236 label_id: 0
2021-08-09 21:14:50,765 ***** Running mm evaluation *****
2021-08-09 21:14:50,765   Num examples = 9832
2021-08-09 21:14:50,765   Batch size = 32
2021-08-09 21:15:03,759 ***** Eval results *****
2021-08-09 21:15:03,759   acc = 0.09580960130187144
2021-08-09 21:15:03,759   eval_loss = 1.8109235349413637
2021-08-09 21:15:03,759   global_step = 299
2021-08-09 21:15:58,018 ***** Running evaluation *****
2021-08-09 21:15:58,019   Epoch = 0 iter 599 step
2021-08-09 21:15:58,019   Num examples = 9832
2021-08-09 21:15:58,019   Batch size = 32
2021-08-09 21:16:10,498 ***** Eval results *****
2021-08-09 21:16:10,498   acc = 0.08970707892595606
2021-08-09 21:16:10,498   att_loss = 0.0
2021-08-09 21:16:10,498   cls_loss = 0.25598551392654745
2021-08-09 21:16:10,498   eval_loss = 2.243629776425176
2021-08-09 21:16:10,498   global_step = 599
2021-08-09 21:16:10,498   loss = 0.25598551392654745
2021-08-09 21:16:10,498   rep_loss = 0.0
2021-08-09 21:17:02,664 ***** Running evaluation *****
2021-08-09 21:17:02,664   Epoch = 0 iter 899 step
2021-08-09 21:17:02,665   Num examples = 9832
2021-08-09 21:17:02,665   Batch size = 32
2021-08-09 21:17:13,403 ***** Eval results *****
2021-08-09 21:17:13,404   acc = 0.0993694060211554
2021-08-09 21:17:13,404   att_loss = 0.0
2021-08-09 21:17:13,404   cls_loss = 0.2419209591174152
2021-08-09 21:17:13,404   eval_loss = 2.3399492289338792
2021-08-09 21:17:13,404   global_step = 899
2021-08-09 21:17:13,404   loss = 0.2419209591174152
2021-08-09 21:17:13,404   rep_loss = 0.0
2021-08-09 21:18:05,491 ***** Running evaluation *****
2021-08-09 21:18:05,491   Epoch = 0 iter 1199 step
2021-08-09 21:18:05,491   Num examples = 9832
2021-08-09 21:18:05,491   Batch size = 32
2021-08-09 21:18:18,073 ***** Eval results *****
2021-08-09 21:18:18,073   acc = 0.08329943043124491
2021-08-09 21:18:18,073   att_loss = 0.0
2021-08-09 21:18:18,073   cls_loss = 0.23480914331804822
2021-08-09 21:18:18,073   eval_loss = 2.510400582443584
2021-08-09 21:18:18,073   global_step = 1199
2021-08-09 21:18:18,074   loss = 0.23480914331804822
2021-08-09 21:18:18,074   rep_loss = 0.0
2021-08-09 21:19:11,576 ***** Running evaluation *****
2021-08-09 21:19:11,577   Epoch = 0 iter 1499 step
2021-08-09 21:19:11,577   Num examples = 9832
2021-08-09 21:19:11,577   Batch size = 32
2021-08-09 21:19:22,423 ***** Eval results *****
2021-08-09 21:19:22,423   acc = 0.09255492270138324
2021-08-09 21:19:22,424   att_loss = 0.0
2021-08-09 21:19:22,424   cls_loss = 0.23040450816157662
2021-08-09 21:19:22,424   eval_loss = 2.4632030539698415
2021-08-09 21:19:22,424   global_step = 1499
2021-08-09 21:19:22,424   loss = 0.23040450816157662
2021-08-09 21:19:22,424   rep_loss = 0.0
2021-08-09 21:20:14,774 ***** Running evaluation *****
2021-08-09 21:20:14,774   Epoch = 0 iter 1799 step
2021-08-09 21:20:14,774   Num examples = 9832
2021-08-09 21:20:14,774   Batch size = 32
2021-08-09 21:20:27,114 ***** Eval results *****
2021-08-09 21:20:27,114   acc = 0.08716436126932466
2021-08-09 21:20:27,114   att_loss = 0.0
2021-08-09 21:20:27,114   cls_loss = 0.22753374186603806
2021-08-09 21:20:27,115   eval_loss = 2.567736495624889
2021-08-09 21:20:27,115   global_step = 1799
2021-08-09 21:20:27,115   loss = 0.22753374186603806
2021-08-09 21:20:27,115   rep_loss = 0.0
2021-08-09 21:21:19,239 ***** Running evaluation *****
2021-08-09 21:21:19,239   Epoch = 0 iter 2099 step
2021-08-09 21:21:19,239   Num examples = 9832
2021-08-09 21:21:19,239   Batch size = 32
2021-08-09 21:21:30,040 ***** Eval results *****
2021-08-09 21:21:30,040   acc = 0.09489422294548414
2021-08-09 21:21:30,040   att_loss = 0.0
2021-08-09 21:21:30,041   cls_loss = 0.22501807112163336
2021-08-09 21:21:30,041   eval_loss = 2.505637656945687
2021-08-09 21:21:30,041   global_step = 2099
2021-08-09 21:21:30,041   loss = 0.22501807112163336
2021-08-09 21:21:30,041   rep_loss = 0.0
2021-08-09 21:22:23,899 ***** Running evaluation *****
2021-08-09 21:22:23,899   Epoch = 0 iter 2399 step
2021-08-09 21:22:23,900   Num examples = 9832
2021-08-09 21:22:23,900   Batch size = 32
2021-08-09 21:22:36,346 ***** Eval results *****
2021-08-09 21:22:36,346   acc = 0.0966232709519935
2021-08-09 21:22:36,346   att_loss = 0.0
2021-08-09 21:22:36,346   cls_loss = 0.223211541591758
2021-08-09 21:22:36,346   eval_loss = 2.5788144802118276
2021-08-09 21:22:36,346   global_step = 2399
2021-08-09 21:22:36,346   loss = 0.223211541591758
2021-08-09 21:22:36,347   rep_loss = 0.0
2021-08-09 21:23:28,569 ***** Running evaluation *****
2021-08-09 21:23:28,569   Epoch = 0 iter 2699 step
2021-08-09 21:23:28,569   Num examples = 9832
2021-08-09 21:23:28,569   Batch size = 32
2021-08-09 21:23:39,363 ***** Eval results *****
2021-08-09 21:23:39,363   acc = 0.08685923515052889
2021-08-09 21:23:39,363   att_loss = 0.0
2021-08-09 21:23:39,363   cls_loss = 0.22210255934932402
2021-08-09 21:23:39,363   eval_loss = 2.6184774234697414
2021-08-09 21:23:39,363   global_step = 2699
2021-08-09 21:23:39,363   loss = 0.22210255934932402
2021-08-09 21:23:39,363   rep_loss = 0.0
2021-08-09 21:24:31,508 ***** Running evaluation *****
2021-08-09 21:24:31,509   Epoch = 0 iter 2999 step
2021-08-09 21:24:31,509   Num examples = 9832
2021-08-09 21:24:31,509   Batch size = 32
2021-08-09 21:24:43,942 ***** Eval results *****
2021-08-09 21:24:43,942   acc = 0.09174125305126118
2021-08-09 21:24:43,942   att_loss = 0.0
2021-08-09 21:24:43,942   cls_loss = 0.22117895684408403
2021-08-09 21:24:43,942   eval_loss = 2.5611986092158725
2021-08-09 21:24:43,942   global_step = 2999
2021-08-09 21:24:43,942   loss = 0.22117895684408403
2021-08-09 21:24:43,942   rep_loss = 0.0
2021-08-09 21:25:37,716 ***** Running evaluation *****
2021-08-09 21:25:37,716   Epoch = 0 iter 3299 step
2021-08-09 21:25:37,716   Num examples = 9832
2021-08-09 21:25:37,716   Batch size = 32
2021-08-09 21:25:48,554 ***** Eval results *****
2021-08-09 21:25:48,554   acc = 0.09540276647681041
2021-08-09 21:25:48,554   att_loss = 0.0
2021-08-09 21:25:48,554   cls_loss = 0.22057264745596794
2021-08-09 21:25:48,554   eval_loss = 2.6054431718665283
2021-08-09 21:25:48,554   global_step = 3299
2021-08-09 21:25:48,554   loss = 0.22057264745596794
2021-08-09 21:25:48,554   rep_loss = 0.0
2021-08-09 21:26:40,696 ***** Running evaluation *****
2021-08-09 21:26:40,696   Epoch = 0 iter 3599 step
2021-08-09 21:26:40,696   Num examples = 9832
2021-08-09 21:26:40,696   Batch size = 32
2021-08-09 21:26:51,466 ***** Eval results *****
2021-08-09 21:26:51,466   acc = 0.0993694060211554
2021-08-09 21:26:51,466   att_loss = 0.0
2021-08-09 21:26:51,466   cls_loss = 0.21989244909941008
2021-08-09 21:26:51,466   eval_loss = 2.6865086385181973
2021-08-09 21:26:51,466   global_step = 3599
2021-08-09 21:26:51,466   loss = 0.21989244909941008
2021-08-09 21:26:51,466   rep_loss = 0.0
2021-08-09 21:27:45,217 ***** Running evaluation *****
2021-08-09 21:27:45,218   Epoch = 0 iter 3899 step
2021-08-09 21:27:45,218   Num examples = 9832
2021-08-09 21:27:45,218   Batch size = 32
2021-08-09 21:27:55,967 ***** Eval results *****
2021-08-09 21:27:55,967   acc = 0.08879170056956875
2021-08-09 21:27:55,968   att_loss = 0.0
2021-08-09 21:27:55,968   cls_loss = 0.21939477030590088
2021-08-09 21:27:55,968   eval_loss = 2.624453996683096
2021-08-09 21:27:55,968   global_step = 3899
2021-08-09 21:27:55,968   loss = 0.21939477030590088
2021-08-09 21:27:55,968   rep_loss = 0.0
2021-08-09 21:28:49,565 ***** Running evaluation *****
2021-08-09 21:28:49,565   Epoch = 0 iter 4199 step
2021-08-09 21:28:49,565   Num examples = 9832
2021-08-09 21:28:49,565   Batch size = 32
2021-08-09 21:29:00,321 ***** Eval results *****
2021-08-09 21:29:00,321   acc = 0.09031733116354759
2021-08-09 21:29:00,321   att_loss = 0.0
2021-08-09 21:29:00,321   cls_loss = 0.2189272539506046
2021-08-09 21:29:00,321   eval_loss = 2.6311756248597975
2021-08-09 21:29:00,321   global_step = 4199
2021-08-09 21:29:00,321   loss = 0.2189272539506046
2021-08-09 21:29:00,321   rep_loss = 0.0
2021-08-09 21:29:53,865 ***** Running evaluation *****
2021-08-09 21:29:53,865   Epoch = 0 iter 4499 step
2021-08-09 21:29:53,865   Num examples = 9832
2021-08-09 21:29:53,865   Batch size = 32
2021-08-09 21:30:04,634 ***** Eval results *****
2021-08-09 21:30:04,634   acc = 0.08624898291293735
2021-08-09 21:30:04,635   att_loss = 0.0
2021-08-09 21:30:04,635   cls_loss = 0.21870328207086473
2021-08-09 21:30:04,635   eval_loss = 2.5514010663156386
2021-08-09 21:30:04,635   global_step = 4499
2021-08-09 21:30:04,635   loss = 0.21870328207086473
2021-08-09 21:30:04,635   rep_loss = 0.0
2021-08-09 21:30:56,739 ***** Running evaluation *****
2021-08-09 21:30:56,739   Epoch = 0 iter 4799 step
2021-08-09 21:30:56,739   Num examples = 9832
2021-08-09 21:30:56,739   Batch size = 32
2021-08-09 21:31:07,509 ***** Eval results *****
2021-08-09 21:31:07,509   acc = 0.08909682668836452
2021-08-09 21:31:07,509   att_loss = 0.0
2021-08-09 21:31:07,509   cls_loss = 0.2183858345892062
2021-08-09 21:31:07,509   eval_loss = 2.6278834451328623
2021-08-09 21:31:07,509   global_step = 4799
2021-08-09 21:31:07,509   loss = 0.2183858345892062
2021-08-09 21:31:07,509   rep_loss = 0.0
2021-08-09 21:32:02,827 ***** Running evaluation *****
2021-08-09 21:32:02,828   Epoch = 0 iter 5099 step
2021-08-09 21:32:02,828   Num examples = 9832
2021-08-09 21:32:02,828   Batch size = 32
2021-08-09 21:32:13,571 ***** Eval results *****
2021-08-09 21:32:13,571   acc = 0.08818144833197722
2021-08-09 21:32:13,571   att_loss = 0.0
2021-08-09 21:32:13,571   cls_loss = 0.2183238559667913
2021-08-09 21:32:13,571   eval_loss = 2.64136133178488
2021-08-09 21:32:13,571   global_step = 5099
2021-08-09 21:32:13,571   loss = 0.2183238559667913
2021-08-09 21:32:13,571   rep_loss = 0.0
2021-08-09 21:33:05,617 ***** Running evaluation *****
2021-08-09 21:33:05,617   Epoch = 0 iter 5399 step
2021-08-09 21:33:05,618   Num examples = 9832
2021-08-09 21:33:05,618   Batch size = 32
2021-08-09 21:33:16,349 ***** Eval results *****
2021-08-09 21:33:16,349   acc = 0.10506509357200976
2021-08-09 21:33:16,350   att_loss = 0.0
2021-08-09 21:33:16,350   cls_loss = 0.21813437921688675
2021-08-09 21:33:16,350   eval_loss = 2.482209369346693
2021-08-09 21:33:16,350   global_step = 5399
2021-08-09 21:33:16,350   loss = 0.21813437921688675
2021-08-09 21:33:16,350   rep_loss = 0.0
2021-08-09 21:34:08,490 ***** Running evaluation *****
2021-08-09 21:34:08,490   Epoch = 0 iter 5699 step
2021-08-09 21:34:08,490   Num examples = 9832
2021-08-09 21:34:08,491   Batch size = 32
2021-08-09 21:34:20,742 ***** Eval results *****
2021-08-09 21:34:20,742   acc = 0.0966232709519935
2021-08-09 21:34:20,742   att_loss = 0.0
2021-08-09 21:34:20,742   cls_loss = 0.21793608811157256
2021-08-09 21:34:20,742   eval_loss = 2.445054776095725
2021-08-09 21:34:20,742   global_step = 5699
2021-08-09 21:34:20,742   loss = 0.21793608811157256
2021-08-09 21:34:20,742   rep_loss = 0.0
2021-08-09 21:35:14,455 ***** Running evaluation *****
2021-08-09 21:35:14,455   Epoch = 0 iter 5999 step
2021-08-09 21:35:14,455   Num examples = 9832
2021-08-09 21:35:14,455   Batch size = 32
2021-08-09 21:35:25,242 ***** Eval results *****
2021-08-09 21:35:25,242   acc = 0.08818144833197722
2021-08-09 21:35:25,242   att_loss = 0.0
2021-08-09 21:35:25,242   cls_loss = 0.21777348111303355
2021-08-09 21:35:25,242   eval_loss = 2.608709007114559
2021-08-09 21:35:25,242   global_step = 5999
2021-08-09 21:35:25,242   loss = 0.21777348111303355
2021-08-09 21:35:25,242   rep_loss = 0.0
2021-08-09 21:36:17,171 ***** Running evaluation *****
2021-08-09 21:36:17,171   Epoch = 0 iter 6299 step
2021-08-09 21:36:17,171   Num examples = 9832
2021-08-09 21:36:17,171   Batch size = 32
2021-08-09 21:36:29,491 ***** Eval results *****
2021-08-09 21:36:29,492   acc = 0.09631814483319773
2021-08-09 21:36:29,492   att_loss = 0.0
2021-08-09 21:36:29,492   cls_loss = 0.21759655967338215
2021-08-09 21:36:29,492   eval_loss = 2.3815271672490357
2021-08-09 21:36:29,492   global_step = 6299
2021-08-09 21:36:29,492   loss = 0.21759655967338215
2021-08-09 21:36:29,492   rep_loss = 0.0
2021-08-09 21:37:21,523 ***** Running evaluation *****
2021-08-09 21:37:21,524   Epoch = 0 iter 6599 step
2021-08-09 21:37:21,524   Num examples = 9832
2021-08-09 21:37:21,524   Batch size = 32
2021-08-09 21:37:32,268 ***** Eval results *****
2021-08-09 21:37:32,268   acc = 0.10903173311635476
2021-08-09 21:37:32,268   att_loss = 0.0
2021-08-09 21:37:32,268   cls_loss = 0.21738904855715216
2021-08-09 21:37:32,268   eval_loss = 2.417680907559085
2021-08-09 21:37:32,268   global_step = 6599
2021-08-09 21:37:32,268   loss = 0.21738904855715216
2021-08-09 21:37:32,268   rep_loss = 0.0
2021-08-09 21:37:32,268 ***** Save model *****
2021-08-09 21:37:33,175 Writing example 0 of 9832
2021-08-09 21:37:33,176 *** Example ***
2021-08-09 21:37:33,176 guid: dev_matched-0
2021-08-09 21:37:33,176 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-09 21:37:33,176 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:37:33,176 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:37:33,176 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-09 21:37:33,176 label: contradiction
2021-08-09 21:37:33,176 label_id: 0
2021-08-09 21:37:37,677 ***** Running mm evaluation *****
2021-08-09 21:37:37,677   Num examples = 9832
2021-08-09 21:37:37,677   Batch size = 32
2021-08-09 21:37:49,928 ***** Eval results *****
2021-08-09 21:37:49,928   acc = 0.10903173311635476
2021-08-09 21:37:49,928   eval_loss = 2.417680907559085
2021-08-09 21:37:49,928   global_step = 6599
2021-08-09 21:38:40,423 ***** Running evaluation *****
2021-08-09 21:38:40,424   Epoch = 0 iter 6899 step
2021-08-09 21:38:40,424   Num examples = 9832
2021-08-09 21:38:40,424   Batch size = 32
2021-08-09 21:38:51,193 ***** Eval results *****
2021-08-09 21:38:51,193   acc = 0.09255492270138324
2021-08-09 21:38:51,193   att_loss = 0.0
2021-08-09 21:38:51,193   cls_loss = 0.21727010817661857
2021-08-09 21:38:51,193   eval_loss = 2.4539903843557678
2021-08-09 21:38:51,193   global_step = 6899
2021-08-09 21:38:51,194   loss = 0.21727010817661857
2021-08-09 21:38:51,194   rep_loss = 0.0
2021-08-09 21:39:44,833 ***** Running evaluation *****
2021-08-09 21:39:44,834   Epoch = 0 iter 7199 step
2021-08-09 21:39:44,834   Num examples = 9832
2021-08-09 21:39:44,834   Batch size = 32
2021-08-09 21:39:55,652 ***** Eval results *****
2021-08-09 21:39:55,653   acc = 0.0870626525630594
2021-08-09 21:39:55,653   att_loss = 0.0
2021-08-09 21:39:55,653   cls_loss = 0.2172385960101154
2021-08-09 21:39:55,653   eval_loss = 2.5087226118360246
2021-08-09 21:39:55,653   global_step = 7199
2021-08-09 21:39:55,653   loss = 0.2172385960101154
2021-08-09 21:39:55,653   rep_loss = 0.0
2021-08-09 21:40:47,765 ***** Running evaluation *****
2021-08-09 21:40:47,765   Epoch = 0 iter 7499 step
2021-08-09 21:40:47,765   Num examples = 9832
2021-08-09 21:40:47,765   Batch size = 32
2021-08-09 21:40:58,546 ***** Eval results *****
2021-08-09 21:40:58,547   acc = 0.08950366151342555
2021-08-09 21:40:58,547   att_loss = 0.0
2021-08-09 21:40:58,547   cls_loss = 0.2170591988628237
2021-08-09 21:40:58,547   eval_loss = 2.5389304462965434
2021-08-09 21:40:58,547   global_step = 7499
2021-08-09 21:40:58,547   loss = 0.2170591988628237
2021-08-09 21:40:58,547   rep_loss = 0.0
2021-08-09 21:41:48,946 ***** Running evaluation *****
2021-08-09 21:41:48,946   Epoch = 0 iter 7799 step
2021-08-09 21:41:48,946   Num examples = 9832
2021-08-09 21:41:48,946   Batch size = 32
2021-08-09 21:42:01,400 ***** Eval results *****
2021-08-09 21:42:01,400   acc = 0.08696094385679415
2021-08-09 21:42:01,400   att_loss = 0.0
2021-08-09 21:42:01,400   cls_loss = 0.21699299887137957
2021-08-09 21:42:01,400   eval_loss = 2.6529069225509447
2021-08-09 21:42:01,400   global_step = 7799
2021-08-09 21:42:01,400   loss = 0.21699299887137957
2021-08-09 21:42:01,400   rep_loss = 0.0
2021-08-09 21:42:53,466 ***** Running evaluation *****
2021-08-09 21:42:53,466   Epoch = 0 iter 8099 step
2021-08-09 21:42:53,466   Num examples = 9832
2021-08-09 21:42:53,466   Batch size = 32
2021-08-09 21:43:04,231 ***** Eval results *****
2021-08-09 21:43:04,231   acc = 0.09438567941415785
2021-08-09 21:43:04,231   att_loss = 0.0
2021-08-09 21:43:04,231   cls_loss = 0.21691505454553323
2021-08-09 21:43:04,231   eval_loss = 2.417045478696947
2021-08-09 21:43:04,231   global_step = 8099
2021-08-09 21:43:04,231   loss = 0.21691505454553323
2021-08-09 21:43:04,231   rep_loss = 0.0
2021-08-09 21:43:56,238 ***** Running evaluation *****
2021-08-09 21:43:56,239   Epoch = 0 iter 8399 step
2021-08-09 21:43:56,239   Num examples = 9832
2021-08-09 21:43:56,239   Batch size = 32
2021-08-09 21:44:07,007 ***** Eval results *****
2021-08-09 21:44:07,007   acc = 0.09641985353946297
2021-08-09 21:44:07,007   att_loss = 0.0
2021-08-09 21:44:07,007   cls_loss = 0.21674737669035202
2021-08-09 21:44:07,007   eval_loss = 2.527499269355427
2021-08-09 21:44:07,007   global_step = 8399
2021-08-09 21:44:07,007   loss = 0.21674737669035202
2021-08-09 21:44:07,007   rep_loss = 0.0
2021-08-09 21:44:59,212 ***** Running evaluation *****
2021-08-09 21:44:59,212   Epoch = 0 iter 8699 step
2021-08-09 21:44:59,212   Num examples = 9832
2021-08-09 21:44:59,212   Batch size = 32
2021-08-09 21:45:10,012 ***** Eval results *****
2021-08-09 21:45:10,013   acc = 0.0828925956061839
2021-08-09 21:45:10,013   att_loss = 0.0
2021-08-09 21:45:10,013   cls_loss = 0.21657740610887954
2021-08-09 21:45:10,013   eval_loss = 2.6070883049593343
2021-08-09 21:45:10,013   global_step = 8699
2021-08-09 21:45:10,013   loss = 0.21657740610887954
2021-08-09 21:45:10,013   rep_loss = 0.0
2021-08-09 21:46:03,787 ***** Running evaluation *****
2021-08-09 21:46:03,787   Epoch = 0 iter 8999 step
2021-08-09 21:46:03,787   Num examples = 9832
2021-08-09 21:46:03,787   Batch size = 32
2021-08-09 21:46:14,571 ***** Eval results *****
2021-08-09 21:46:14,571   acc = 0.08777461350691619
2021-08-09 21:46:14,572   att_loss = 0.0
2021-08-09 21:46:14,572   cls_loss = 0.21632756384886268
2021-08-09 21:46:14,572   eval_loss = 2.572864341271388
2021-08-09 21:46:14,572   global_step = 8999
2021-08-09 21:46:14,572   loss = 0.21632756384886268
2021-08-09 21:46:14,572   rep_loss = 0.0
2021-08-09 21:47:05,059 ***** Running evaluation *****
2021-08-09 21:47:05,059   Epoch = 0 iter 9299 step
2021-08-09 21:47:05,059   Num examples = 9832
2021-08-09 21:47:05,060   Batch size = 32
2021-08-09 21:47:15,832 ***** Eval results *****
2021-08-09 21:47:15,832   acc = 0.09794548413344183
2021-08-09 21:47:15,832   att_loss = 0.0
2021-08-09 21:47:15,832   cls_loss = 0.21621822658175102
2021-08-09 21:47:15,832   eval_loss = 2.4960967503584826
2021-08-09 21:47:15,832   global_step = 9299
2021-08-09 21:47:15,832   loss = 0.21621822658175102
2021-08-09 21:47:15,832   rep_loss = 0.0
2021-08-09 21:48:07,965 ***** Running evaluation *****
2021-08-09 21:48:07,966   Epoch = 0 iter 9599 step
2021-08-09 21:48:07,966   Num examples = 9832
2021-08-09 21:48:07,966   Batch size = 32
2021-08-09 21:48:18,777 ***** Eval results *****
2021-08-09 21:48:18,777   acc = 0.08624898291293735
2021-08-09 21:48:18,777   att_loss = 0.0
2021-08-09 21:48:18,777   cls_loss = 0.21611627705526396
2021-08-09 21:48:18,777   eval_loss = 2.534831619107878
2021-08-09 21:48:18,777   global_step = 9599
2021-08-09 21:48:18,777   loss = 0.21611627705526396
2021-08-09 21:48:18,777   rep_loss = 0.0
2021-08-09 21:49:12,469 ***** Running evaluation *****
2021-08-09 21:49:12,469   Epoch = 0 iter 9899 step
2021-08-09 21:49:12,469   Num examples = 9832
2021-08-09 21:49:12,469   Batch size = 32
2021-08-09 21:49:23,231 ***** Eval results *****
2021-08-09 21:49:23,231   acc = 0.0935720097640358
2021-08-09 21:49:23,232   att_loss = 0.0
2021-08-09 21:49:23,232   cls_loss = 0.215932569487888
2021-08-09 21:49:23,232   eval_loss = 2.6129592076524513
2021-08-09 21:49:23,232   global_step = 9899
2021-08-09 21:49:23,232   loss = 0.215932569487888
2021-08-09 21:49:23,232   rep_loss = 0.0
2021-08-09 21:50:15,233 ***** Running evaluation *****
2021-08-09 21:50:15,233   Epoch = 0 iter 10199 step
2021-08-09 21:50:15,233   Num examples = 9832
2021-08-09 21:50:15,233   Batch size = 32
2021-08-09 21:50:26,029 ***** Eval results *****
2021-08-09 21:50:26,029   acc = 0.0951993490642799
2021-08-09 21:50:26,029   att_loss = 0.0
2021-08-09 21:50:26,030   cls_loss = 0.21583265970093163
2021-08-09 21:50:26,030   eval_loss = 2.5251392826631474
2021-08-09 21:50:26,030   global_step = 10199
2021-08-09 21:50:26,030   loss = 0.21583265970093163
2021-08-09 21:50:26,030   rep_loss = 0.0
2021-08-09 21:51:16,511 ***** Running evaluation *****
2021-08-09 21:51:16,512   Epoch = 0 iter 10499 step
2021-08-09 21:51:16,512   Num examples = 9832
2021-08-09 21:51:16,512   Batch size = 32
2021-08-09 21:51:28,863 ***** Eval results *****
2021-08-09 21:51:28,863   acc = 0.09082587469487388
2021-08-09 21:51:28,863   att_loss = 0.0
2021-08-09 21:51:28,863   cls_loss = 0.21563163332137986
2021-08-09 21:51:28,863   eval_loss = 2.6067828590219673
2021-08-09 21:51:28,864   global_step = 10499
2021-08-09 21:51:28,864   loss = 0.21563163332137986
2021-08-09 21:51:28,864   rep_loss = 0.0
2021-08-09 21:52:20,961 ***** Running evaluation *****
2021-08-09 21:52:20,961   Epoch = 0 iter 10799 step
2021-08-09 21:52:20,961   Num examples = 9832
2021-08-09 21:52:20,961   Batch size = 32
2021-08-09 21:52:31,760 ***** Eval results *****
2021-08-09 21:52:31,760   acc = 0.09601301871440195
2021-08-09 21:52:31,760   att_loss = 0.0
2021-08-09 21:52:31,760   cls_loss = 0.21555113546251534
2021-08-09 21:52:31,760   eval_loss = 2.4351624525212623
2021-08-09 21:52:31,760   global_step = 10799
2021-08-09 21:52:31,760   loss = 0.21555113546251534
2021-08-09 21:52:31,760   rep_loss = 0.0
2021-08-09 21:53:23,834 ***** Running evaluation *****
2021-08-09 21:53:23,834   Epoch = 0 iter 11099 step
2021-08-09 21:53:23,834   Num examples = 9832
2021-08-09 21:53:23,834   Batch size = 32
2021-08-09 21:53:34,615 ***** Eval results *****
2021-08-09 21:53:34,615   acc = 0.09245321399511798
2021-08-09 21:53:34,616   att_loss = 0.0
2021-08-09 21:53:34,616   cls_loss = 0.21548683242960465
2021-08-09 21:53:34,616   eval_loss = 2.575720115141435
2021-08-09 21:53:34,616   global_step = 11099
2021-08-09 21:53:34,616   loss = 0.21548683242960465
2021-08-09 21:53:34,616   rep_loss = 0.0
2021-08-09 21:54:26,774 ***** Running evaluation *****
2021-08-09 21:54:26,775   Epoch = 0 iter 11399 step
2021-08-09 21:54:26,775   Num examples = 9832
2021-08-09 21:54:26,775   Batch size = 32
2021-08-09 21:54:37,594 ***** Eval results *****
2021-08-09 21:54:37,594   acc = 0.09469080553295361
2021-08-09 21:54:37,594   att_loss = 0.0
2021-08-09 21:54:37,594   cls_loss = 0.21538186887448604
2021-08-09 21:54:37,594   eval_loss = 2.542710364638985
2021-08-09 21:54:37,594   global_step = 11399
2021-08-09 21:54:37,594   loss = 0.21538186887448604
2021-08-09 21:54:37,595   rep_loss = 0.0
2021-08-09 21:55:31,252 ***** Running evaluation *****
2021-08-09 21:55:31,252   Epoch = 0 iter 11699 step
2021-08-09 21:55:31,252   Num examples = 9832
2021-08-09 21:55:31,252   Batch size = 32
2021-08-09 21:55:42,001 ***** Eval results *****
2021-08-09 21:55:42,001   acc = 0.09275834011391375
2021-08-09 21:55:42,001   att_loss = 0.0
2021-08-09 21:55:42,001   cls_loss = 0.2153331892901949
2021-08-09 21:55:42,001   eval_loss = 2.6043121706355703
2021-08-09 21:55:42,001   global_step = 11699
2021-08-09 21:55:42,001   loss = 0.2153331892901949
2021-08-09 21:55:42,001   rep_loss = 0.0
2021-08-09 21:56:32,496 ***** Running evaluation *****
2021-08-09 21:56:32,497   Epoch = 0 iter 11999 step
2021-08-09 21:56:32,497   Num examples = 9832
2021-08-09 21:56:32,497   Batch size = 32
2021-08-09 21:56:43,274 ***** Eval results *****
2021-08-09 21:56:43,274   acc = 0.08574043938161106
2021-08-09 21:56:43,274   att_loss = 0.0
2021-08-09 21:56:43,274   cls_loss = 0.21522108691887554
2021-08-09 21:56:43,274   eval_loss = 2.606028017285582
2021-08-09 21:56:43,274   global_step = 11999
2021-08-09 21:56:43,274   loss = 0.21522108691887554
2021-08-09 21:56:43,274   rep_loss = 0.0
2021-08-09 21:57:35,507 ***** Running evaluation *****
2021-08-09 21:57:35,507   Epoch = 0 iter 12299 step
2021-08-09 21:57:35,507   Num examples = 9832
2021-08-09 21:57:35,507   Batch size = 32
2021-08-09 21:57:46,271 ***** Eval results *****
2021-08-09 21:57:46,271   acc = 0.08919853539462978
2021-08-09 21:57:46,271   att_loss = 0.0
2021-08-09 21:57:46,271   cls_loss = 0.21508639043826902
2021-08-09 21:57:46,271   eval_loss = 2.5666529350466543
2021-08-09 21:57:46,271   global_step = 12299
2021-08-09 21:57:46,271   loss = 0.21508639043826902
2021-08-09 21:57:46,271   rep_loss = 0.0
2021-08-09 21:58:39,965 ***** Running evaluation *****
2021-08-09 21:58:39,966   Epoch = 0 iter 12599 step
2021-08-09 21:58:39,966   Num examples = 9832
2021-08-09 21:58:39,966   Batch size = 32
2021-08-09 21:58:50,769 ***** Eval results *****
2021-08-09 21:58:50,769   acc = 0.09072416598860862
2021-08-09 21:58:50,769   att_loss = 0.0
2021-08-09 21:58:50,769   cls_loss = 0.21496389877234143
2021-08-09 21:58:50,769   eval_loss = 2.492051103672424
2021-08-09 21:58:50,769   global_step = 12599
2021-08-09 21:58:50,769   loss = 0.21496389877234143
2021-08-09 21:58:50,769   rep_loss = 0.0
2021-08-09 21:59:42,872 ***** Running evaluation *****
2021-08-09 21:59:42,873   Epoch = 0 iter 12899 step
2021-08-09 21:59:42,873   Num examples = 9832
2021-08-09 21:59:42,873   Batch size = 32
2021-08-09 21:59:53,651 ***** Eval results *****
2021-08-09 21:59:53,651   acc = 0.08299430431244914
2021-08-09 21:59:53,651   att_loss = 0.0
2021-08-09 21:59:53,651   cls_loss = 0.214789225710844
2021-08-09 21:59:53,652   eval_loss = 2.6805275082588196
2021-08-09 21:59:53,652   global_step = 12899
2021-08-09 21:59:53,652   loss = 0.214789225710844
2021-08-09 21:59:53,652   rep_loss = 0.0
2021-08-09 22:00:44,186 ***** Running evaluation *****
2021-08-09 22:00:44,187   Epoch = 0 iter 13199 step
2021-08-09 22:00:44,187   Num examples = 9832
2021-08-09 22:00:44,187   Batch size = 32
2021-08-09 22:00:56,548 ***** Eval results *****
2021-08-09 22:00:56,548   acc = 0.09011391375101709
2021-08-09 22:00:56,548   att_loss = 0.0
2021-08-09 22:00:56,548   cls_loss = 0.21472054350415867
2021-08-09 22:00:56,548   eval_loss = 2.666496612034835
2021-08-09 22:00:56,548   global_step = 13199
2021-08-09 22:00:56,548   loss = 0.21472054350415867
2021-08-09 22:00:56,548   rep_loss = 0.0
2021-08-09 22:01:48,720 ***** Running evaluation *****
2021-08-09 22:01:48,720   Epoch = 0 iter 13499 step
2021-08-09 22:01:48,720   Num examples = 9832
2021-08-09 22:01:48,720   Batch size = 32
2021-08-09 22:01:59,485 ***** Eval results *****
2021-08-09 22:01:59,485   acc = 0.08899511798209926
2021-08-09 22:01:59,485   att_loss = 0.0
2021-08-09 22:01:59,485   cls_loss = 0.21459084666541828
2021-08-09 22:01:59,485   eval_loss = 2.580885591445031
2021-08-09 22:01:59,485   global_step = 13499
2021-08-09 22:01:59,485   loss = 0.21459084666541828
2021-08-09 22:01:59,485   rep_loss = 0.0
2021-08-09 22:02:51,424 ***** Running evaluation *****
2021-08-09 22:02:51,425   Epoch = 0 iter 13799 step
2021-08-09 22:02:51,425   Num examples = 9832
2021-08-09 22:02:51,426   Batch size = 32
2021-08-09 22:03:02,197 ***** Eval results *****
2021-08-09 22:03:02,197   acc = 0.08685923515052889
2021-08-09 22:03:02,197   att_loss = 0.0
2021-08-09 22:03:02,197   cls_loss = 0.21452037494107773
2021-08-09 22:03:02,197   eval_loss = 2.5627812123917915
2021-08-09 22:03:02,197   global_step = 13799
2021-08-09 22:03:02,197   loss = 0.21452037494107773
2021-08-09 22:03:02,197   rep_loss = 0.0
2021-08-09 22:03:54,327 ***** Running evaluation *****
2021-08-09 22:03:54,328   Epoch = 0 iter 14099 step
2021-08-09 22:03:54,328   Num examples = 9832
2021-08-09 22:03:54,328   Batch size = 32
2021-08-09 22:04:05,078 ***** Eval results *****
2021-08-09 22:04:05,078   acc = 0.08716436126932466
2021-08-09 22:04:05,078   att_loss = 0.0
2021-08-09 22:04:05,078   cls_loss = 0.21444279395127874
2021-08-09 22:04:05,078   eval_loss = 2.590122380813995
2021-08-09 22:04:05,078   global_step = 14099
2021-08-09 22:04:05,078   loss = 0.21444279395127874
2021-08-09 22:04:05,078   rep_loss = 0.0
2021-08-09 22:04:57,285 ***** Running evaluation *****
2021-08-09 22:04:57,286   Epoch = 0 iter 14399 step
2021-08-09 22:04:57,286   Num examples = 9832
2021-08-09 22:04:57,286   Batch size = 32
2021-08-09 22:05:09,601 ***** Eval results *****
2021-08-09 22:05:09,602   acc = 0.09286004882017901
2021-08-09 22:05:09,602   att_loss = 0.0
2021-08-09 22:05:09,602   cls_loss = 0.21434486626490046
2021-08-09 22:05:09,602   eval_loss = 2.558155321455621
2021-08-09 22:05:09,602   global_step = 14399
2021-08-09 22:05:09,602   loss = 0.21434486626490046
2021-08-09 22:05:09,602   rep_loss = 0.0
2021-08-09 22:06:00,021 ***** Running evaluation *****
2021-08-09 22:06:00,022   Epoch = 0 iter 14699 step
2021-08-09 22:06:00,022   Num examples = 9832
2021-08-09 22:06:00,022   Batch size = 32
2021-08-09 22:06:10,779 ***** Eval results *****
2021-08-09 22:06:10,780   acc = 0.0863506916192026
2021-08-09 22:06:10,780   att_loss = 0.0
2021-08-09 22:06:10,780   cls_loss = 0.21425027433962504
2021-08-09 22:06:10,780   eval_loss = 2.532793010984148
2021-08-09 22:06:10,780   global_step = 14699
2021-08-09 22:06:10,780   loss = 0.21425027433962504
2021-08-09 22:06:10,780   rep_loss = 0.0
2021-08-09 22:07:02,778 ***** Running evaluation *****
2021-08-09 22:07:02,778   Epoch = 0 iter 14999 step
2021-08-09 22:07:02,778   Num examples = 9832
2021-08-09 22:07:02,778   Batch size = 32
2021-08-09 22:07:13,519 ***** Eval results *****
2021-08-09 22:07:13,520   acc = 0.0959113100081367
2021-08-09 22:07:13,520   att_loss = 0.0
2021-08-09 22:07:13,520   cls_loss = 0.21417765868740152
2021-08-09 22:07:13,520   eval_loss = 2.530388658696955
2021-08-09 22:07:13,520   global_step = 14999
2021-08-09 22:07:13,520   loss = 0.21417765868740152
2021-08-09 22:07:13,520   rep_loss = 0.0
2021-08-09 22:08:07,178 ***** Running evaluation *****
2021-08-09 22:08:07,178   Epoch = 0 iter 15299 step
2021-08-09 22:08:07,178   Num examples = 9832
2021-08-09 22:08:07,179   Batch size = 32
2021-08-09 22:08:17,945 ***** Eval results *****
2021-08-09 22:08:17,945   acc = 0.09336859235150528
2021-08-09 22:08:17,945   att_loss = 0.0
2021-08-09 22:08:17,945   cls_loss = 0.21404946863912305
2021-08-09 22:08:17,945   eval_loss = 2.5438621508610715
2021-08-09 22:08:17,945   global_step = 15299
2021-08-09 22:08:17,945   loss = 0.21404946863912305
2021-08-09 22:08:17,945   rep_loss = 0.0
2021-08-09 22:09:08,385 ***** Running evaluation *****
2021-08-09 22:09:08,386   Epoch = 0 iter 15599 step
2021-08-09 22:09:08,386   Num examples = 9832
2021-08-09 22:09:08,386   Batch size = 32
2021-08-09 22:09:19,223 ***** Eval results *****
2021-08-09 22:09:19,223   acc = 0.08848657445077299
2021-08-09 22:09:19,223   att_loss = 0.0
2021-08-09 22:09:19,223   cls_loss = 0.2139169953978897
2021-08-09 22:09:19,223   eval_loss = 2.5179227102886546
2021-08-09 22:09:19,223   global_step = 15599
2021-08-09 22:09:19,223   loss = 0.2139169953978897
2021-08-09 22:09:19,224   rep_loss = 0.0
2021-08-09 22:10:09,823 ***** Running evaluation *****
2021-08-09 22:10:09,823   Epoch = 1 iter 15899 step
2021-08-09 22:10:09,824   Num examples = 9832
2021-08-09 22:10:09,824   Batch size = 32
2021-08-09 22:10:20,670 ***** Eval results *****
2021-08-09 22:10:20,670   acc = 0.0977420667209113
2021-08-09 22:10:20,670   att_loss = 0.0
2021-08-09 22:10:20,670   cls_loss = 0.20736870553233835
2021-08-09 22:10:20,670   eval_loss = 2.60603081173711
2021-08-09 22:10:20,670   global_step = 15899
2021-08-09 22:10:20,670   loss = 0.20736870553233835
2021-08-09 22:10:20,670   rep_loss = 0.0
2021-08-09 22:11:14,283 ***** Running evaluation *****
2021-08-09 22:11:14,283   Epoch = 1 iter 16199 step
2021-08-09 22:11:14,283   Num examples = 9832
2021-08-09 22:11:14,283   Batch size = 32
2021-08-09 22:11:24,966 ***** Eval results *****
2021-08-09 22:11:24,967   acc = 0.09570789259560618
2021-08-09 22:11:24,967   att_loss = 0.0
2021-08-09 22:11:24,967   cls_loss = 0.20546043729247002
2021-08-09 22:11:24,967   eval_loss = 2.479651915175574
2021-08-09 22:11:24,967   global_step = 16199
2021-08-09 22:11:24,967   loss = 0.20546043729247002
2021-08-09 22:11:24,967   rep_loss = 0.0
2021-08-09 22:12:15,009 ***** Running evaluation *****
2021-08-09 22:12:15,009   Epoch = 1 iter 16499 step
2021-08-09 22:12:15,009   Num examples = 9832
2021-08-09 22:12:15,010   Batch size = 32
2021-08-09 22:12:25,644 ***** Eval results *****
2021-08-09 22:12:25,644   acc = 0.09224979658258747
2021-08-09 22:12:25,644   att_loss = 0.0
2021-08-09 22:12:25,644   cls_loss = 0.2061370170533062
2021-08-09 22:12:25,644   eval_loss = 2.5191817848713365
2021-08-09 22:12:25,644   global_step = 16499
2021-08-09 22:12:25,645   loss = 0.2061370170533062
2021-08-09 22:12:25,645   rep_loss = 0.0
2021-08-09 22:13:17,336 ***** Running evaluation *****
2021-08-09 22:13:17,337   Epoch = 1 iter 16799 step
2021-08-09 22:13:17,337   Num examples = 9832
2021-08-09 22:13:17,337   Batch size = 32
2021-08-09 22:13:27,976 ***** Eval results *****
2021-08-09 22:13:27,976   acc = 0.09570789259560618
2021-08-09 22:13:27,976   att_loss = 0.0
2021-08-09 22:13:27,976   cls_loss = 0.2066683303434532
2021-08-09 22:13:27,976   eval_loss = 2.614765277156582
2021-08-09 22:13:27,976   global_step = 16799
2021-08-09 22:13:27,976   loss = 0.2066683303434532
2021-08-09 22:13:27,976   rep_loss = 0.0
2021-08-09 22:14:21,576 ***** Running evaluation *****
2021-08-09 22:14:21,577   Epoch = 1 iter 17099 step
2021-08-09 22:14:21,577   Num examples = 9832
2021-08-09 22:14:21,577   Batch size = 32
2021-08-09 22:14:32,412 ***** Eval results *****
2021-08-09 22:14:32,413   acc = 0.09286004882017901
2021-08-09 22:14:32,413   att_loss = 0.0
2021-08-09 22:14:32,413   cls_loss = 0.2067201370394844
2021-08-09 22:14:32,413   eval_loss = 2.5150140237498593
2021-08-09 22:14:32,413   global_step = 17099
2021-08-09 22:14:32,413   loss = 0.2067201370394844
2021-08-09 22:14:32,413   rep_loss = 0.0
2021-08-09 22:15:23,905 ***** Running evaluation *****
2021-08-09 22:15:23,905   Epoch = 1 iter 17399 step
2021-08-09 22:15:23,905   Num examples = 9832
2021-08-09 22:15:23,905   Batch size = 32
2021-08-09 22:15:34,531 ***** Eval results *****
2021-08-09 22:15:34,531   acc = 0.0982506102522376
2021-08-09 22:15:34,531   att_loss = 0.0
2021-08-09 22:15:34,531   cls_loss = 0.20697155095017902
2021-08-09 22:15:34,531   eval_loss = 2.6277745784103095
2021-08-09 22:15:34,531   global_step = 17399
2021-08-09 22:15:34,531   loss = 0.20697155095017902
2021-08-09 22:15:34,532   rep_loss = 0.0
2021-08-09 22:16:24,341 ***** Running evaluation *****
2021-08-09 22:16:24,342   Epoch = 1 iter 17699 step
2021-08-09 22:16:24,342   Num examples = 9832
2021-08-09 22:16:24,342   Batch size = 32
2021-08-09 22:16:34,961 ***** Eval results *****
2021-08-09 22:16:34,961   acc = 0.09286004882017901
2021-08-09 22:16:34,961   att_loss = 0.0
2021-08-09 22:16:34,961   cls_loss = 0.20713062052943718
2021-08-09 22:16:34,961   eval_loss = 2.5847878223889835
2021-08-09 22:16:34,961   global_step = 17699
2021-08-09 22:16:34,961   loss = 0.20713062052943718
2021-08-09 22:16:34,961   rep_loss = 0.0
2021-08-09 22:17:26,510 ***** Running evaluation *****
2021-08-09 22:17:26,510   Epoch = 1 iter 17999 step
2021-08-09 22:17:26,510   Num examples = 9832
2021-08-09 22:17:26,510   Batch size = 32
2021-08-09 22:17:37,141 ***** Eval results *****
2021-08-09 22:17:37,141   acc = 0.09794548413344183
2021-08-09 22:17:37,141   att_loss = 0.0
2021-08-09 22:17:37,142   cls_loss = 0.20679008294538387
2021-08-09 22:17:37,142   eval_loss = 2.504592075363382
2021-08-09 22:17:37,142   global_step = 17999
2021-08-09 22:17:37,142   loss = 0.20679008294538387
2021-08-09 22:17:37,142   rep_loss = 0.0
2021-08-09 22:18:28,452 ***** Running evaluation *****
2021-08-09 22:18:28,452   Epoch = 1 iter 18299 step
2021-08-09 22:18:28,453   Num examples = 9832
2021-08-09 22:18:28,453   Batch size = 32
2021-08-09 22:18:39,059 ***** Eval results *****
2021-08-09 22:18:39,059   acc = 0.09296175752644427
2021-08-09 22:18:39,059   att_loss = 0.0
2021-08-09 22:18:39,059   cls_loss = 0.20671708364097752
2021-08-09 22:18:39,059   eval_loss = 2.6369870065094587
2021-08-09 22:18:39,059   global_step = 18299
2021-08-09 22:18:39,059   loss = 0.20671708364097752
2021-08-09 22:18:39,059   rep_loss = 0.0
2021-08-09 22:19:28,929 ***** Running evaluation *****
2021-08-09 22:19:28,929   Epoch = 1 iter 18599 step
2021-08-09 22:19:28,929   Num examples = 9832
2021-08-09 22:19:28,929   Batch size = 32
2021-08-09 22:19:39,561 ***** Eval results *****
2021-08-09 22:19:39,561   acc = 0.08502847843775427
2021-08-09 22:19:39,562   att_loss = 0.0
2021-08-09 22:19:39,562   cls_loss = 0.20673314517883265
2021-08-09 22:19:39,562   eval_loss = 2.536987830292095
2021-08-09 22:19:39,562   global_step = 18599
2021-08-09 22:19:39,562   loss = 0.20673314517883265
2021-08-09 22:19:39,562   rep_loss = 0.0
2021-08-09 22:20:32,631 ***** Running evaluation *****
2021-08-09 22:20:32,631   Epoch = 1 iter 18899 step
2021-08-09 22:20:32,632   Num examples = 9832
2021-08-09 22:20:32,632   Batch size = 32
2021-08-09 22:20:43,248 ***** Eval results *****
2021-08-09 22:20:43,248   acc = 0.09875915378356387
2021-08-09 22:20:43,249   att_loss = 0.0
2021-08-09 22:20:43,249   cls_loss = 0.20665766777376404
2021-08-09 22:20:43,249   eval_loss = 2.528320411970089
2021-08-09 22:20:43,249   global_step = 18899
2021-08-09 22:20:43,249   loss = 0.20665766777376404
2021-08-09 22:20:43,249   rep_loss = 0.0
2021-08-09 22:21:33,163 ***** Running evaluation *****
2021-08-09 22:21:33,163   Epoch = 1 iter 19199 step
2021-08-09 22:21:33,163   Num examples = 9832
2021-08-09 22:21:33,163   Batch size = 32
2021-08-09 22:21:43,781 ***** Eval results *****
2021-08-09 22:21:43,781   acc = 0.08848657445077299
2021-08-09 22:21:43,781   att_loss = 0.0
2021-08-09 22:21:43,781   cls_loss = 0.20668123690074627
2021-08-09 22:21:43,781   eval_loss = 2.503508011242012
2021-08-09 22:21:43,782   global_step = 19199
2021-08-09 22:21:43,782   loss = 0.20668123690074627
2021-08-09 22:21:43,782   rep_loss = 0.0
2021-08-09 22:22:33,642 ***** Running evaluation *****
2021-08-09 22:22:33,642   Epoch = 1 iter 19499 step
2021-08-09 22:22:33,642   Num examples = 9832
2021-08-09 22:22:33,642   Batch size = 32
2021-08-09 22:22:45,736 ***** Eval results *****
2021-08-09 22:22:45,737   acc = 0.0854353132628153
2021-08-09 22:22:45,737   att_loss = 0.0
2021-08-09 22:22:45,737   cls_loss = 0.20672185897376208
2021-08-09 22:22:45,737   eval_loss = 2.5283966513423177
2021-08-09 22:22:45,737   global_step = 19499
2021-08-09 22:22:45,737   loss = 0.20672185897376208
2021-08-09 22:22:45,737   rep_loss = 0.0
2021-08-09 22:23:37,193 ***** Running evaluation *****
2021-08-09 22:23:37,193   Epoch = 1 iter 19799 step
2021-08-09 22:23:37,193   Num examples = 9832
2021-08-09 22:23:37,193   Batch size = 32
2021-08-09 22:23:47,791 ***** Eval results *****
2021-08-09 22:23:47,792   acc = 0.08767290480065093
2021-08-09 22:23:47,792   att_loss = 0.0
2021-08-09 22:23:47,792   cls_loss = 0.20656643956489965
2021-08-09 22:23:47,792   eval_loss = 2.5576011437874335
2021-08-09 22:23:47,792   global_step = 19799
2021-08-09 22:23:47,792   loss = 0.20656643956489965
2021-08-09 22:23:47,792   rep_loss = 0.0
2021-08-09 22:24:37,770 ***** Running evaluation *****
2021-08-09 22:24:37,770   Epoch = 1 iter 20099 step
2021-08-09 22:24:37,770   Num examples = 9832
2021-08-09 22:24:37,770   Batch size = 32
2021-08-09 22:24:48,410 ***** Eval results *****
2021-08-09 22:24:48,410   acc = 0.09641985353946297
2021-08-09 22:24:48,410   att_loss = 0.0
2021-08-09 22:24:48,410   cls_loss = 0.2065775328231562
2021-08-09 22:24:48,410   eval_loss = 2.57392758595479
2021-08-09 22:24:48,411   global_step = 20099
2021-08-09 22:24:48,411   loss = 0.2065775328231562
2021-08-09 22:24:48,411   rep_loss = 0.0
2021-08-09 22:25:39,795 ***** Running evaluation *****
2021-08-09 22:25:39,796   Epoch = 1 iter 20399 step
2021-08-09 22:25:39,796   Num examples = 9832
2021-08-09 22:25:39,796   Batch size = 32
2021-08-09 22:25:50,364 ***** Eval results *****
2021-08-09 22:25:50,364   acc = 0.0984540276647681
2021-08-09 22:25:50,364   att_loss = 0.0
2021-08-09 22:25:50,365   cls_loss = 0.20639669127929108
2021-08-09 22:25:50,365   eval_loss = 2.5693524100563745
2021-08-09 22:25:50,365   global_step = 20399
2021-08-09 22:25:50,365   loss = 0.20639669127929108
2021-08-09 22:25:50,365   rep_loss = 0.0
2021-08-09 22:26:41,720 ***** Running evaluation *****
2021-08-09 22:26:41,721   Epoch = 1 iter 20699 step
2021-08-09 22:26:41,721   Num examples = 9832
2021-08-09 22:26:41,721   Batch size = 32
2021-08-09 22:26:52,327 ***** Eval results *****
2021-08-09 22:26:52,327   acc = 0.09489422294548414
2021-08-09 22:26:52,328   att_loss = 0.0
2021-08-09 22:26:52,328   cls_loss = 0.2062982220755283
2021-08-09 22:26:52,328   eval_loss = 2.5900259149539004
2021-08-09 22:26:52,328   global_step = 20699
2021-08-09 22:26:52,328   loss = 0.2062982220755283
2021-08-09 22:26:52,328   rep_loss = 0.0
2021-08-09 22:27:43,745 ***** Running evaluation *****
2021-08-09 22:27:43,746   Epoch = 1 iter 20999 step
2021-08-09 22:27:43,746   Num examples = 9832
2021-08-09 22:27:43,746   Batch size = 32
2021-08-09 22:27:54,317 ***** Eval results *****
2021-08-09 22:27:54,317   acc = 0.08807973962571196
2021-08-09 22:27:54,317   att_loss = 0.0
2021-08-09 22:27:54,317   cls_loss = 0.2062614462314214
2021-08-09 22:27:54,317   eval_loss = 2.598238122927678
2021-08-09 22:27:54,317   global_step = 20999
2021-08-09 22:27:54,317   loss = 0.2062614462314214
2021-08-09 22:27:54,317   rep_loss = 0.0
2021-08-09 22:28:44,083 ***** Running evaluation *****
2021-08-09 22:28:44,084   Epoch = 1 iter 21299 step
2021-08-09 22:28:44,084   Num examples = 9832
2021-08-09 22:28:44,084   Batch size = 32
2021-08-09 22:28:54,703 ***** Eval results *****
2021-08-09 22:28:54,703   acc = 0.09418226200162734
2021-08-09 22:28:54,703   att_loss = 0.0
2021-08-09 22:28:54,703   cls_loss = 0.2063214469337394
2021-08-09 22:28:54,703   eval_loss = 2.5198291811076077
2021-08-09 22:28:54,703   global_step = 21299
2021-08-09 22:28:54,703   loss = 0.2063214469337394
2021-08-09 22:28:54,703   rep_loss = 0.0
2021-08-09 22:29:46,118 ***** Running evaluation *****
2021-08-09 22:29:46,119   Epoch = 1 iter 21599 step
2021-08-09 22:29:46,119   Num examples = 9832
2021-08-09 22:29:46,119   Batch size = 32
2021-08-09 22:29:58,216 ***** Eval results *****
2021-08-09 22:29:58,216   acc = 0.09469080553295361
2021-08-09 22:29:58,216   att_loss = 0.0
2021-08-09 22:29:58,216   cls_loss = 0.206261570387679
2021-08-09 22:29:58,216   eval_loss = 2.5548902695829216
2021-08-09 22:29:58,216   global_step = 21599
2021-08-09 22:29:58,216   loss = 0.206261570387679
2021-08-09 22:29:58,216   rep_loss = 0.0
2021-08-09 22:30:48,175 ***** Running evaluation *****
2021-08-09 22:30:48,175   Epoch = 1 iter 21899 step
2021-08-09 22:30:48,175   Num examples = 9832
2021-08-09 22:30:48,175   Batch size = 32
2021-08-09 22:30:58,745 ***** Eval results *****
2021-08-09 22:30:58,745   acc = 0.09530105777054516
2021-08-09 22:30:58,745   att_loss = 0.0
2021-08-09 22:30:58,745   cls_loss = 0.20624485301240666
2021-08-09 22:30:58,745   eval_loss = 2.5161961735068976
2021-08-09 22:30:58,745   global_step = 21899
2021-08-09 22:30:58,745   loss = 0.20624485301240666
2021-08-09 22:30:58,745   rep_loss = 0.0
2021-08-09 22:31:48,406 ***** Running evaluation *****
2021-08-09 22:31:48,406   Epoch = 1 iter 22199 step
2021-08-09 22:31:48,406   Num examples = 9832
2021-08-09 22:31:48,406   Batch size = 32
2021-08-09 22:31:58,972 ***** Eval results *****
2021-08-09 22:31:58,972   acc = 0.0959113100081367
2021-08-09 22:31:58,972   att_loss = 0.0
2021-08-09 22:31:58,972   cls_loss = 0.20619430616113882
2021-08-09 22:31:58,972   eval_loss = 2.6133221805869757
2021-08-09 22:31:58,972   global_step = 22199
2021-08-09 22:31:58,972   loss = 0.20619430616113882
2021-08-09 22:31:58,972   rep_loss = 0.0
2021-08-09 22:32:51,875 ***** Running evaluation *****
2021-08-09 22:32:51,875   Epoch = 1 iter 22499 step
2021-08-09 22:32:51,875   Num examples = 9832
2021-08-09 22:32:51,875   Batch size = 32
2021-08-09 22:33:02,459 ***** Eval results *****
2021-08-09 22:33:02,460   acc = 0.08655410903173312
2021-08-09 22:33:02,460   att_loss = 0.0
2021-08-09 22:33:02,460   cls_loss = 0.20625327656498918
2021-08-09 22:33:02,460   eval_loss = 2.524962376464497
2021-08-09 22:33:02,460   global_step = 22499
2021-08-09 22:33:02,460   loss = 0.20625327656498918
2021-08-09 22:33:02,460   rep_loss = 0.0
2021-08-09 22:33:52,294 ***** Running evaluation *****
2021-08-09 22:33:52,295   Epoch = 1 iter 22799 step
2021-08-09 22:33:52,295   Num examples = 9832
2021-08-09 22:33:52,295   Batch size = 32
2021-08-09 22:34:02,885 ***** Eval results *****
2021-08-09 22:34:02,885   acc = 0.09123270951993491
2021-08-09 22:34:02,886   att_loss = 0.0
2021-08-09 22:34:02,886   cls_loss = 0.2062377427973214
2021-08-09 22:34:02,886   eval_loss = 2.5653850335579413
2021-08-09 22:34:02,886   global_step = 22799
2021-08-09 22:34:02,886   loss = 0.2062377427973214
2021-08-09 22:34:02,886   rep_loss = 0.0
2021-08-09 22:34:54,200 ***** Running evaluation *****
2021-08-09 22:34:54,200   Epoch = 1 iter 23099 step
2021-08-09 22:34:54,200   Num examples = 9832
2021-08-09 22:34:54,200   Batch size = 32
2021-08-09 22:35:04,812 ***** Eval results *****
2021-08-09 22:35:04,812   acc = 0.08197721724979658
2021-08-09 22:35:04,812   att_loss = 0.0
2021-08-09 22:35:04,812   cls_loss = 0.20611679118849646
2021-08-09 22:35:04,812   eval_loss = 2.6106537325041637
2021-08-09 22:35:04,812   global_step = 23099
2021-08-09 22:35:04,812   loss = 0.20611679118849646
2021-08-09 22:35:04,812   rep_loss = 0.0
2021-08-09 22:35:56,309 ***** Running evaluation *****
2021-08-09 22:35:56,310   Epoch = 1 iter 23399 step
2021-08-09 22:35:56,310   Num examples = 9832
2021-08-09 22:35:56,310   Batch size = 32
2021-08-09 22:36:06,906 ***** Eval results *****
2021-08-09 22:36:06,907   acc = 0.08787632221318145
2021-08-09 22:36:06,907   att_loss = 0.0
2021-08-09 22:36:06,907   cls_loss = 0.206013473109295
2021-08-09 22:36:06,907   eval_loss = 2.597936162700901
2021-08-09 22:36:06,907   global_step = 23399
2021-08-09 22:36:06,907   loss = 0.206013473109295
2021-08-09 22:36:06,907   rep_loss = 0.0
2021-08-09 22:36:56,765 ***** Running evaluation *****
2021-08-09 22:36:56,765   Epoch = 1 iter 23699 step
2021-08-09 22:36:56,765   Num examples = 9832
2021-08-09 22:36:56,765   Batch size = 32
2021-08-09 22:37:07,369 ***** Eval results *****
2021-08-09 22:37:07,369   acc = 0.08767290480065093
2021-08-09 22:37:07,369   att_loss = 0.0
2021-08-09 22:37:07,369   cls_loss = 0.20598497418923675
2021-08-09 22:37:07,369   eval_loss = 2.6242373291548198
2021-08-09 22:37:07,369   global_step = 23699
2021-08-09 22:37:07,369   loss = 0.20598497418923675
2021-08-09 22:37:07,369   rep_loss = 0.0
2021-08-09 22:37:58,733 ***** Running evaluation *****
2021-08-09 22:37:58,733   Epoch = 1 iter 23999 step
2021-08-09 22:37:58,733   Num examples = 9832
2021-08-09 22:37:58,733   Batch size = 32
2021-08-09 22:38:09,315 ***** Eval results *****
2021-08-09 22:38:09,315   acc = 0.08431651749389747
2021-08-09 22:38:09,315   att_loss = 0.0
2021-08-09 22:38:09,315   cls_loss = 0.20606697021228718
2021-08-09 22:38:09,315   eval_loss = 2.5851493710047238
2021-08-09 22:38:09,315   global_step = 23999
2021-08-09 22:38:09,315   loss = 0.20606697021228718
2021-08-09 22:38:09,315   rep_loss = 0.0
2021-08-09 22:39:00,620 ***** Running evaluation *****
2021-08-09 22:39:00,621   Epoch = 1 iter 24299 step
2021-08-09 22:39:00,621   Num examples = 9832
2021-08-09 22:39:00,621   Batch size = 32
2021-08-09 22:39:11,166 ***** Eval results *****
2021-08-09 22:39:11,166   acc = 0.08716436126932466
2021-08-09 22:39:11,166   att_loss = 0.0
2021-08-09 22:39:11,166   cls_loss = 0.20602389446511968
2021-08-09 22:39:11,166   eval_loss = 2.6027467901056465
2021-08-09 22:39:11,166   global_step = 24299
2021-08-09 22:39:11,166   loss = 0.20602389446511968
2021-08-09 22:39:11,166   rep_loss = 0.0
2021-08-09 22:40:02,303 ***** Running evaluation *****
2021-08-09 22:40:02,304   Epoch = 1 iter 24599 step
2021-08-09 22:40:02,304   Num examples = 9832
2021-08-09 22:40:02,304   Batch size = 32
2021-08-09 22:40:12,903 ***** Eval results *****
2021-08-09 22:40:12,903   acc = 0.09021562245728235
2021-08-09 22:40:12,903   att_loss = 0.0
2021-08-09 22:40:12,903   cls_loss = 0.20601443429112962
2021-08-09 22:40:12,903   eval_loss = 2.5789288212726644
2021-08-09 22:40:12,903   global_step = 24599
2021-08-09 22:40:12,903   loss = 0.20601443429112962
2021-08-09 22:40:12,903   rep_loss = 0.0
2021-08-09 22:41:02,558 ***** Running evaluation *****
2021-08-09 22:41:02,559   Epoch = 1 iter 24899 step
2021-08-09 22:41:02,559   Num examples = 9832
2021-08-09 22:41:02,559   Batch size = 32
2021-08-09 22:41:13,128 ***** Eval results *****
2021-08-09 22:41:13,128   acc = 0.09326688364524004
2021-08-09 22:41:13,128   att_loss = 0.0
2021-08-09 22:41:13,128   cls_loss = 0.2059398620695272
2021-08-09 22:41:13,128   eval_loss = 2.57232042643931
2021-08-09 22:41:13,128   global_step = 24899
2021-08-09 22:41:13,128   loss = 0.2059398620695272
2021-08-09 22:41:13,128   rep_loss = 0.0
2021-08-09 22:42:05,884 ***** Running evaluation *****
2021-08-09 22:42:05,884   Epoch = 1 iter 25199 step
2021-08-09 22:42:05,884   Num examples = 9832
2021-08-09 22:42:05,885   Batch size = 32
2021-08-09 22:42:16,517 ***** Eval results *****
2021-08-09 22:42:16,518   acc = 0.0863506916192026
2021-08-09 22:42:16,518   att_loss = 0.0
2021-08-09 22:42:16,518   cls_loss = 0.20588398625965817
2021-08-09 22:42:16,518   eval_loss = 2.580170425501737
2021-08-09 22:42:16,518   global_step = 25199
2021-08-09 22:42:16,518   loss = 0.20588398625965817
2021-08-09 22:42:16,518   rep_loss = 0.0
2021-08-09 22:43:06,238 ***** Running evaluation *****
2021-08-09 22:43:06,239   Epoch = 1 iter 25499 step
2021-08-09 22:43:06,239   Num examples = 9832
2021-08-09 22:43:06,239   Batch size = 32
2021-08-09 22:43:16,793 ***** Eval results *****
2021-08-09 22:43:16,793   acc = 0.09163954434499594
2021-08-09 22:43:16,793   att_loss = 0.0
2021-08-09 22:43:16,793   cls_loss = 0.20577904855132215
2021-08-09 22:43:16,793   eval_loss = 2.5702101224428646
2021-08-09 22:43:16,793   global_step = 25499
2021-08-09 22:43:16,793   loss = 0.20577904855132215
2021-08-09 22:43:16,793   rep_loss = 0.0
2021-08-09 22:44:06,466 ***** Running evaluation *****
2021-08-09 22:44:06,466   Epoch = 1 iter 25799 step
2021-08-09 22:44:06,466   Num examples = 9832
2021-08-09 22:44:06,466   Batch size = 32
2021-08-09 22:44:17,070 ***** Eval results *****
2021-08-09 22:44:17,070   acc = 0.08268917819365337
2021-08-09 22:44:17,070   att_loss = 0.0
2021-08-09 22:44:17,070   cls_loss = 0.20574012113689077
2021-08-09 22:44:17,070   eval_loss = 2.6345261443745005
2021-08-09 22:44:17,070   global_step = 25799
2021-08-09 22:44:17,070   loss = 0.20574012113689077
2021-08-09 22:44:17,070   rep_loss = 0.0
2021-08-09 22:45:09,838 ***** Running evaluation *****
2021-08-09 22:45:09,839   Epoch = 1 iter 26099 step
2021-08-09 22:45:09,839   Num examples = 9832
2021-08-09 22:45:09,839   Batch size = 32
2021-08-09 22:45:20,412 ***** Eval results *****
2021-08-09 22:45:20,412   acc = 0.08421480878763223
2021-08-09 22:45:20,412   att_loss = 0.0
2021-08-09 22:45:20,412   cls_loss = 0.2057227267332302
2021-08-09 22:45:20,413   eval_loss = 2.575141704701758
2021-08-09 22:45:20,413   global_step = 26099
2021-08-09 22:45:20,413   loss = 0.2057227267332302
2021-08-09 22:45:20,413   rep_loss = 0.0
2021-08-09 22:46:10,094 ***** Running evaluation *****
2021-08-09 22:46:10,095   Epoch = 1 iter 26399 step
2021-08-09 22:46:10,095   Num examples = 9832
2021-08-09 22:46:10,095   Batch size = 32
2021-08-09 22:46:20,665 ***** Eval results *****
2021-08-09 22:46:20,665   acc = 0.08746948738812042
2021-08-09 22:46:20,665   att_loss = 0.0
2021-08-09 22:46:20,665   cls_loss = 0.2057159754037632
2021-08-09 22:46:20,665   eval_loss = 2.627170205890358
2021-08-09 22:46:20,665   global_step = 26399
2021-08-09 22:46:20,665   loss = 0.2057159754037632
2021-08-09 22:46:20,665   rep_loss = 0.0
2021-08-09 22:47:11,856 ***** Running evaluation *****
2021-08-09 22:47:11,856   Epoch = 1 iter 26699 step
2021-08-09 22:47:11,856   Num examples = 9832
2021-08-09 22:47:11,856   Batch size = 32
2021-08-09 22:47:22,427 ***** Eval results *****
2021-08-09 22:47:22,427   acc = 0.09184296175752645
2021-08-09 22:47:22,427   att_loss = 0.0
2021-08-09 22:47:22,427   cls_loss = 0.2056600813220022
2021-08-09 22:47:22,427   eval_loss = 2.6236836631576734
2021-08-09 22:47:22,427   global_step = 26699
2021-08-09 22:47:22,428   loss = 0.2056600813220022
2021-08-09 22:47:22,428   rep_loss = 0.0
2021-08-09 22:48:13,768 ***** Running evaluation *****
2021-08-09 22:48:13,769   Epoch = 1 iter 26999 step
2021-08-09 22:48:13,769   Num examples = 9832
2021-08-09 22:48:13,769   Batch size = 32
2021-08-09 22:48:24,352 ***** Eval results *****
2021-08-09 22:48:24,352   acc = 0.08757119609438568
2021-08-09 22:48:24,352   att_loss = 0.0
2021-08-09 22:48:24,352   cls_loss = 0.20562921799822434
2021-08-09 22:48:24,352   eval_loss = 2.5351962052382433
2021-08-09 22:48:24,353   global_step = 26999
2021-08-09 22:48:24,353   loss = 0.20562921799822434
2021-08-09 22:48:24,353   rep_loss = 0.0
2021-08-09 22:49:14,125 ***** Running evaluation *****
2021-08-09 22:49:14,125   Epoch = 1 iter 27299 step
2021-08-09 22:49:14,125   Num examples = 9832
2021-08-09 22:49:14,125   Batch size = 32
2021-08-09 22:49:26,189 ***** Eval results *****
2021-08-09 22:49:26,189   acc = 0.08909682668836452
2021-08-09 22:49:26,189   att_loss = 0.0
2021-08-09 22:49:26,189   cls_loss = 0.20560612696848188
2021-08-09 22:49:26,189   eval_loss = 2.5494449370867245
2021-08-09 22:49:26,189   global_step = 27299
2021-08-09 22:49:26,189   loss = 0.20560612696848188
2021-08-09 22:49:26,190   rep_loss = 0.0
2021-08-09 22:50:16,030 ***** Running evaluation *****
2021-08-09 22:50:16,030   Epoch = 1 iter 27599 step
2021-08-09 22:50:16,031   Num examples = 9832
2021-08-09 22:50:16,031   Batch size = 32
2021-08-09 22:50:26,550 ***** Eval results *****
2021-08-09 22:50:26,551   acc = 0.09336859235150528
2021-08-09 22:50:26,551   att_loss = 0.0
2021-08-09 22:50:26,551   cls_loss = 0.20551940064623783
2021-08-09 22:50:26,551   eval_loss = 2.643190609170245
2021-08-09 22:50:26,551   global_step = 27599
2021-08-09 22:50:26,551   loss = 0.20551940064623783
2021-08-09 22:50:26,551   rep_loss = 0.0
2021-08-09 22:51:17,791 ***** Running evaluation *****
2021-08-09 22:51:17,792   Epoch = 1 iter 27899 step
2021-08-09 22:51:17,792   Num examples = 9832
2021-08-09 22:51:17,792   Batch size = 32
2021-08-09 22:51:28,383 ***** Eval results *****
2021-08-09 22:51:28,383   acc = 0.08879170056956875
2021-08-09 22:51:28,383   att_loss = 0.0
2021-08-09 22:51:28,383   cls_loss = 0.20549456830108848
2021-08-09 22:51:28,383   eval_loss = 2.626743021723512
2021-08-09 22:51:28,383   global_step = 27899
2021-08-09 22:51:28,383   loss = 0.20549456830108848
2021-08-09 22:51:28,383   rep_loss = 0.0
2021-08-09 22:52:19,622 ***** Running evaluation *****
2021-08-09 22:52:19,622   Epoch = 1 iter 28199 step
2021-08-09 22:52:19,622   Num examples = 9832
2021-08-09 22:52:19,623   Batch size = 32
2021-08-09 22:52:30,180 ***** Eval results *****
2021-08-09 22:52:30,180   acc = 0.0838079739625712
2021-08-09 22:52:30,180   att_loss = 0.0
2021-08-09 22:52:30,180   cls_loss = 0.20539136067646838
2021-08-09 22:52:30,180   eval_loss = 2.6735068877021986
2021-08-09 22:52:30,180   global_step = 28199
2021-08-09 22:52:30,180   loss = 0.20539136067646838
2021-08-09 22:52:30,180   rep_loss = 0.0
2021-08-09 22:53:19,847 ***** Running evaluation *****
2021-08-09 22:53:19,847   Epoch = 1 iter 28499 step
2021-08-09 22:53:19,847   Num examples = 9832
2021-08-09 22:53:19,847   Batch size = 32
2021-08-09 22:53:30,431 ***** Eval results *****
2021-08-09 22:53:30,431   acc = 0.08685923515052889
2021-08-09 22:53:30,431   att_loss = 0.0
2021-08-09 22:53:30,431   cls_loss = 0.20539396779995792
2021-08-09 22:53:30,431   eval_loss = 2.575472127307545
2021-08-09 22:53:30,431   global_step = 28499
2021-08-09 22:53:30,432   loss = 0.20539396779995792
2021-08-09 22:53:30,432   rep_loss = 0.0
2021-08-09 22:54:23,306 ***** Running evaluation *****
2021-08-09 22:54:23,306   Epoch = 1 iter 28799 step
2021-08-09 22:54:23,306   Num examples = 9832
2021-08-09 22:54:23,306   Batch size = 32
2021-08-09 22:54:33,870 ***** Eval results *****
2021-08-09 22:54:33,870   acc = 0.08940195280716029
2021-08-09 22:54:33,870   att_loss = 0.0
2021-08-09 22:54:33,870   cls_loss = 0.20534107992442036
2021-08-09 22:54:33,870   eval_loss = 2.6149821575585896
2021-08-09 22:54:33,870   global_step = 28799
2021-08-09 22:54:33,870   loss = 0.20534107992442036
2021-08-09 22:54:33,870   rep_loss = 0.0
2021-08-09 22:55:23,580 ***** Running evaluation *****
2021-08-09 22:55:23,580   Epoch = 1 iter 29099 step
2021-08-09 22:55:23,580   Num examples = 9832
2021-08-09 22:55:23,580   Batch size = 32
2021-08-09 22:55:34,157 ***** Eval results *****
2021-08-09 22:55:34,158   acc = 0.08665581773799837
2021-08-09 22:55:34,158   att_loss = 0.0
2021-08-09 22:55:34,158   cls_loss = 0.20536174768613216
2021-08-09 22:55:34,158   eval_loss = 2.6036859317259355
2021-08-09 22:55:34,158   global_step = 29099
2021-08-09 22:55:34,158   loss = 0.20536174768613216
2021-08-09 22:55:34,158   rep_loss = 0.0
2021-08-09 22:56:23,702 ***** Running evaluation *****
2021-08-09 22:56:23,702   Epoch = 1 iter 29399 step
2021-08-09 22:56:23,703   Num examples = 9832
2021-08-09 22:56:23,703   Batch size = 32
2021-08-09 22:56:34,253 ***** Eval results *****
2021-08-09 22:56:34,254   acc = 0.08940195280716029
2021-08-09 22:56:34,254   att_loss = 0.0
2021-08-09 22:56:34,254   cls_loss = 0.20534857269328585
2021-08-09 22:56:34,254   eval_loss = 2.6631117857895887
2021-08-09 22:56:34,254   global_step = 29399
2021-08-09 22:56:34,254   loss = 0.20534857269328585
2021-08-09 22:56:34,254   rep_loss = 0.0
2021-08-09 22:57:27,109 ***** Running evaluation *****
2021-08-09 22:57:27,109   Epoch = 1 iter 29699 step
2021-08-09 22:57:27,109   Num examples = 9832
2021-08-09 22:57:27,109   Batch size = 32
2021-08-09 22:57:37,700 ***** Eval results *****
2021-08-09 22:57:37,700   acc = 0.08523189585028479
2021-08-09 22:57:37,700   att_loss = 0.0
2021-08-09 22:57:37,700   cls_loss = 0.20534511592484886
2021-08-09 22:57:37,700   eval_loss = 2.5781857433257165
2021-08-09 22:57:37,700   global_step = 29699
2021-08-09 22:57:37,700   loss = 0.20534511592484886
2021-08-09 22:57:37,700   rep_loss = 0.0
2021-08-09 22:58:27,468 ***** Running evaluation *****
2021-08-09 22:58:27,468   Epoch = 1 iter 29999 step
2021-08-09 22:58:27,468   Num examples = 9832
2021-08-09 22:58:27,468   Batch size = 32
2021-08-09 22:58:38,048 ***** Eval results *****
2021-08-09 22:58:38,048   acc = 0.08421480878763223
2021-08-09 22:58:38,049   att_loss = 0.0
2021-08-09 22:58:38,049   cls_loss = 0.20527634236474465
2021-08-09 22:58:38,049   eval_loss = 2.6148032755046695
2021-08-09 22:58:38,049   global_step = 29999
2021-08-09 22:58:38,049   loss = 0.20527634236474465
2021-08-09 22:58:38,049   rep_loss = 0.0
2021-08-09 22:59:29,258 ***** Running evaluation *****
2021-08-09 22:59:29,258   Epoch = 1 iter 30299 step
2021-08-09 22:59:29,258   Num examples = 9832
2021-08-09 22:59:29,258   Batch size = 32
2021-08-09 22:59:39,951 ***** Eval results *****
2021-08-09 22:59:39,951   acc = 0.08197721724979658
2021-08-09 22:59:39,951   att_loss = 0.0
2021-08-09 22:59:39,951   cls_loss = 0.20521852418057926
2021-08-09 22:59:39,951   eval_loss = 2.5754581898837894
2021-08-09 22:59:39,952   global_step = 30299
2021-08-09 22:59:39,952   loss = 0.20521852418057926
2021-08-09 22:59:39,952   rep_loss = 0.0
2021-08-09 23:00:31,317 ***** Running evaluation *****
2021-08-09 23:00:31,317   Epoch = 1 iter 30599 step
2021-08-09 23:00:31,317   Num examples = 9832
2021-08-09 23:00:31,317   Batch size = 32
2021-08-09 23:00:41,885 ***** Eval results *****
2021-08-09 23:00:41,885   acc = 0.09001220504475183
2021-08-09 23:00:41,885   att_loss = 0.0
2021-08-09 23:00:41,885   cls_loss = 0.20520258268873626
2021-08-09 23:00:41,885   eval_loss = 2.5937673658519595
2021-08-09 23:00:41,885   global_step = 30599
2021-08-09 23:00:41,885   loss = 0.20520258268873626
2021-08-09 23:00:41,885   rep_loss = 0.0
2021-08-09 23:01:33,100 ***** Running evaluation *****
2021-08-09 23:01:33,101   Epoch = 1 iter 30899 step
2021-08-09 23:01:33,101   Num examples = 9832
2021-08-09 23:01:33,101   Batch size = 32
2021-08-09 23:01:43,648 ***** Eval results *****
2021-08-09 23:01:43,648   acc = 0.08970707892595606
2021-08-09 23:01:43,648   att_loss = 0.0
2021-08-09 23:01:43,648   cls_loss = 0.20511891185826842
2021-08-09 23:01:43,648   eval_loss = 2.5798707503777045
2021-08-09 23:01:43,648   global_step = 30899
2021-08-09 23:01:43,648   loss = 0.20511891185826842
2021-08-09 23:01:43,648   rep_loss = 0.0
2021-08-09 23:02:33,384 ***** Running evaluation *****
2021-08-09 23:02:33,384   Epoch = 1 iter 31199 step
2021-08-09 23:02:33,384   Num examples = 9832
2021-08-09 23:02:33,384   Batch size = 32
2021-08-09 23:02:43,956 ***** Eval results *****
2021-08-09 23:02:43,956   acc = 0.08858828315703825
2021-08-09 23:02:43,956   att_loss = 0.0
2021-08-09 23:02:43,956   cls_loss = 0.20508334894113267
2021-08-09 23:02:43,956   eval_loss = 2.6114022391183034
2021-08-09 23:02:43,956   global_step = 31199
2021-08-09 23:02:43,956   loss = 0.20508334894113267
2021-08-09 23:02:43,956   rep_loss = 0.0
2021-08-09 23:03:35,185 ***** Running evaluation *****
2021-08-09 23:03:35,185   Epoch = 1 iter 31499 step
2021-08-09 23:03:35,185   Num examples = 9832
2021-08-09 23:03:35,186   Batch size = 32
2021-08-09 23:03:45,782 ***** Eval results *****
2021-08-09 23:03:45,782   acc = 0.08482506102522376
2021-08-09 23:03:45,783   att_loss = 0.0
2021-08-09 23:03:45,783   cls_loss = 0.20506197663125972
2021-08-09 23:03:45,783   eval_loss = 2.638898674543802
2021-08-09 23:03:45,783   global_step = 31499
2021-08-09 23:03:45,783   loss = 0.20506197663125972
2021-08-09 23:03:45,783   rep_loss = 0.0
2021-08-09 23:04:37,177 ***** Running evaluation *****
2021-08-09 23:04:37,177   Epoch = 2 iter 31799 step
2021-08-09 23:04:37,177   Num examples = 9832
2021-08-09 23:04:37,177   Batch size = 32
2021-08-09 23:04:47,749 ***** Eval results *****
2021-08-09 23:04:47,749   acc = 0.0879780309194467
2021-08-09 23:04:47,749   att_loss = 0.0
2021-08-09 23:04:47,749   cls_loss = 0.20105973289811552
2021-08-09 23:04:47,749   eval_loss = 2.6698194393863925
2021-08-09 23:04:47,749   global_step = 31799
2021-08-09 23:04:47,750   loss = 0.20105973289811552
2021-08-09 23:04:47,750   rep_loss = 0.0
2021-08-09 23:05:37,491 ***** Running evaluation *****
2021-08-09 23:05:37,492   Epoch = 2 iter 32099 step
2021-08-09 23:05:37,492   Num examples = 9832
2021-08-09 23:05:37,492   Batch size = 32
2021-08-09 23:05:48,052 ***** Eval results *****
2021-08-09 23:05:48,052   acc = 0.0863506916192026
2021-08-09 23:05:48,052   att_loss = 0.0
2021-08-09 23:05:48,052   cls_loss = 0.20092927748948394
2021-08-09 23:05:48,052   eval_loss = 2.624793537251361
2021-08-09 23:05:48,052   global_step = 32099
2021-08-09 23:05:48,052   loss = 0.20092927748948394
2021-08-09 23:05:48,052   rep_loss = 0.0
2021-08-09 23:06:40,794 ***** Running evaluation *****
2021-08-09 23:06:40,795   Epoch = 2 iter 32399 step
2021-08-09 23:06:40,795   Num examples = 9832
2021-08-09 23:06:40,795   Batch size = 32
2021-08-09 23:06:51,371 ***** Eval results *****
2021-08-09 23:06:51,371   acc = 0.08604556550040683
2021-08-09 23:06:51,371   att_loss = 0.0
2021-08-09 23:06:51,371   cls_loss = 0.20142962925535063
2021-08-09 23:06:51,371   eval_loss = 2.611545952109547
2021-08-09 23:06:51,371   global_step = 32399
2021-08-09 23:06:51,371   loss = 0.20142962925535063
2021-08-09 23:06:51,371   rep_loss = 0.0
2021-08-09 23:07:41,126 ***** Running evaluation *****
2021-08-09 23:07:41,127   Epoch = 2 iter 32699 step
2021-08-09 23:07:41,127   Num examples = 9832
2021-08-09 23:07:41,127   Batch size = 32
2021-08-09 23:07:51,720 ***** Eval results *****
2021-08-09 23:07:51,720   acc = 0.09143612693246542
2021-08-09 23:07:51,720   att_loss = 0.0
2021-08-09 23:07:51,720   cls_loss = 0.20076304282368257
2021-08-09 23:07:51,720   eval_loss = 2.575970842466726
2021-08-09 23:07:51,720   global_step = 32699
2021-08-09 23:07:51,720   loss = 0.20076304282368257
2021-08-09 23:07:51,720   rep_loss = 0.0
2021-08-09 23:08:41,352 ***** Running evaluation *****
2021-08-09 23:08:41,352   Epoch = 2 iter 32999 step
2021-08-09 23:08:41,352   Num examples = 9832
2021-08-09 23:08:41,352   Batch size = 32
2021-08-09 23:08:53,392 ***** Eval results *****
2021-08-09 23:08:53,392   acc = 0.0919446704637917
2021-08-09 23:08:53,392   att_loss = 0.0
2021-08-09 23:08:53,392   cls_loss = 0.20047042448224975
2021-08-09 23:08:53,392   eval_loss = 2.599332044650982
2021-08-09 23:08:53,392   global_step = 32999
2021-08-09 23:08:53,392   loss = 0.20047042448224975
2021-08-09 23:08:53,393   rep_loss = 0.0
2021-08-09 23:09:44,735 ***** Running evaluation *****
2021-08-09 23:09:44,735   Epoch = 2 iter 33299 step
2021-08-09 23:09:44,735   Num examples = 9832
2021-08-09 23:09:44,735   Batch size = 32
2021-08-09 23:09:55,266 ***** Eval results *****
2021-08-09 23:09:55,266   acc = 0.08838486574450773
2021-08-09 23:09:55,266   att_loss = 0.0
2021-08-09 23:09:55,266   cls_loss = 0.20080745984929768
2021-08-09 23:09:55,267   eval_loss = 2.5412954711294793
2021-08-09 23:09:55,267   global_step = 33299
2021-08-09 23:09:55,267   loss = 0.20080745984929768
2021-08-09 23:09:55,267   rep_loss = 0.0
2021-08-09 23:10:44,983 ***** Running evaluation *****
2021-08-09 23:10:44,984   Epoch = 2 iter 33599 step
2021-08-09 23:10:44,984   Num examples = 9832
2021-08-09 23:10:44,984   Batch size = 32
2021-08-09 23:10:55,596 ***** Eval results *****
2021-08-09 23:10:55,596   acc = 0.0879780309194467
2021-08-09 23:10:55,597   att_loss = 0.0
2021-08-09 23:10:55,597   cls_loss = 0.2008483495881304
2021-08-09 23:10:55,597   eval_loss = 2.6050066452521783
2021-08-09 23:10:55,597   global_step = 33599
2021-08-09 23:10:55,597   loss = 0.2008483495881304
2021-08-09 23:10:55,597   rep_loss = 0.0
2021-08-09 23:11:46,852 ***** Running evaluation *****
2021-08-09 23:11:46,852   Epoch = 2 iter 33899 step
2021-08-09 23:11:46,852   Num examples = 9832
2021-08-09 23:11:46,853   Batch size = 32
2021-08-09 23:11:57,424 ***** Eval results *****
2021-08-09 23:11:57,425   acc = 0.08696094385679415
2021-08-09 23:11:57,425   att_loss = 0.0
2021-08-09 23:11:57,425   cls_loss = 0.20074104608606372
2021-08-09 23:11:57,425   eval_loss = 2.563665739127568
2021-08-09 23:11:57,425   global_step = 33899
2021-08-09 23:11:57,425   loss = 0.20074104608606372
2021-08-09 23:11:57,425   rep_loss = 0.0
2021-08-09 23:12:48,803 ***** Running evaluation *****
2021-08-09 23:12:48,803   Epoch = 2 iter 34199 step
2021-08-09 23:12:48,803   Num examples = 9832
2021-08-09 23:12:48,803   Batch size = 32
2021-08-09 23:12:59,394 ***** Eval results *****
2021-08-09 23:12:59,394   acc = 0.08909682668836452
2021-08-09 23:12:59,394   att_loss = 0.0
2021-08-09 23:12:59,394   cls_loss = 0.20088111277479326
2021-08-09 23:12:59,394   eval_loss = 2.620398084838669
2021-08-09 23:12:59,394   global_step = 34199
2021-08-09 23:12:59,394   loss = 0.20088111277479326
2021-08-09 23:12:59,394   rep_loss = 0.0
2021-08-09 23:13:50,620 ***** Running evaluation *****
2021-08-09 23:13:50,621   Epoch = 2 iter 34499 step
2021-08-09 23:13:50,621   Num examples = 9832
2021-08-09 23:13:50,621   Batch size = 32
2021-08-09 23:14:01,200 ***** Eval results *****
2021-08-09 23:14:01,201   acc = 0.08736777868185516
2021-08-09 23:14:01,201   att_loss = 0.0
2021-08-09 23:14:01,201   cls_loss = 0.2009985474545587
2021-08-09 23:14:01,201   eval_loss = 2.643764463337985
2021-08-09 23:14:01,201   global_step = 34499
2021-08-09 23:14:01,201   loss = 0.2009985474545587
2021-08-09 23:14:01,201   rep_loss = 0.0
2021-08-09 23:14:50,926 ***** Running evaluation *****
2021-08-09 23:14:50,927   Epoch = 2 iter 34799 step
2021-08-09 23:14:50,927   Num examples = 9832
2021-08-09 23:14:50,927   Batch size = 32
2021-08-09 23:15:01,531 ***** Eval results *****
2021-08-09 23:15:01,531   acc = 0.08930024410089504
2021-08-09 23:15:01,531   att_loss = 0.0
2021-08-09 23:15:01,531   cls_loss = 0.20092043382020583
2021-08-09 23:15:01,531   eval_loss = 2.6636465787887573
2021-08-09 23:15:01,531   global_step = 34799
2021-08-09 23:15:01,531   loss = 0.20092043382020583
2021-08-09 23:15:01,531   rep_loss = 0.0
2021-08-09 23:15:52,955 ***** Running evaluation *****
2021-08-09 23:15:52,956   Epoch = 2 iter 35099 step
2021-08-09 23:15:52,956   Num examples = 9832
2021-08-09 23:15:52,956   Batch size = 32
2021-08-09 23:16:05,296 ***** Eval results *****
2021-08-09 23:16:05,296   acc = 0.09143612693246542
2021-08-09 23:16:05,296   att_loss = 0.0
2021-08-09 23:16:05,296   cls_loss = 0.20097221939471188
2021-08-09 23:16:05,296   eval_loss = 2.6217122472725904
2021-08-09 23:16:05,296   global_step = 35099
2021-08-09 23:16:05,296   loss = 0.20097221939471188
2021-08-09 23:16:05,296   rep_loss = 0.0
2021-08-09 23:16:54,999 ***** Running evaluation *****
2021-08-09 23:16:55,000   Epoch = 2 iter 35399 step
2021-08-09 23:16:55,000   Num examples = 9832
2021-08-09 23:16:55,000   Batch size = 32
2021-08-09 23:17:05,608 ***** Eval results *****
2021-08-09 23:17:05,608   acc = 0.09062245728234337
2021-08-09 23:17:05,608   att_loss = 0.0
2021-08-09 23:17:05,608   cls_loss = 0.20094871519820615
2021-08-09 23:17:05,608   eval_loss = 2.5902370616987156
2021-08-09 23:17:05,608   global_step = 35399
2021-08-09 23:17:05,608   loss = 0.20094871519820615
2021-08-09 23:17:05,608   rep_loss = 0.0
2021-08-09 23:17:55,347 ***** Running evaluation *****
2021-08-09 23:17:55,347   Epoch = 2 iter 35699 step
2021-08-09 23:17:55,347   Num examples = 9832
2021-08-09 23:17:55,347   Batch size = 32
2021-08-09 23:18:05,926 ***** Eval results *****
2021-08-09 23:18:05,926   acc = 0.09021562245728235
2021-08-09 23:18:05,926   att_loss = 0.0
2021-08-09 23:18:05,926   cls_loss = 0.20095352818333692
2021-08-09 23:18:05,926   eval_loss = 2.6131304099962307
2021-08-09 23:18:05,926   global_step = 35699
2021-08-09 23:18:05,926   loss = 0.20095352818333692
2021-08-09 23:18:05,926   rep_loss = 0.0
2021-08-09 23:18:59,718 ***** Running evaluation *****
2021-08-09 23:18:59,719   Epoch = 2 iter 35999 step
2021-08-09 23:18:59,719   Num examples = 9832
2021-08-09 23:18:59,719   Batch size = 32
2021-08-09 23:19:10,380 ***** Eval results *****
2021-08-09 23:19:10,380   acc = 0.08787632221318145
2021-08-09 23:19:10,380   att_loss = 0.0
2021-08-09 23:19:10,380   cls_loss = 0.2010413353758023
2021-08-09 23:19:10,380   eval_loss = 2.6167112379879147
2021-08-09 23:19:10,380   global_step = 35999
2021-08-09 23:19:10,380   loss = 0.2010413353758023
2021-08-09 23:19:10,380   rep_loss = 0.0
2021-08-09 23:20:00,135 ***** Running evaluation *****
2021-08-09 23:20:00,136   Epoch = 2 iter 36299 step
2021-08-09 23:20:00,136   Num examples = 9832
2021-08-09 23:20:00,136   Batch size = 32
2021-08-09 23:20:10,885 ***** Eval results *****
2021-08-09 23:20:10,885   acc = 0.0847233523189585
2021-08-09 23:20:10,885   att_loss = 0.0
2021-08-09 23:20:10,886   cls_loss = 0.20087445120646197
2021-08-09 23:20:10,886   eval_loss = 2.6234418254394036
2021-08-09 23:20:10,886   global_step = 36299
2021-08-09 23:20:10,886   loss = 0.20087445120646197
2021-08-09 23:20:10,886   rep_loss = 0.0
2021-08-09 23:21:03,164 ***** Running evaluation *****
2021-08-09 23:21:03,165   Epoch = 2 iter 36599 step
2021-08-09 23:21:03,165   Num examples = 9832
2021-08-09 23:21:03,165   Batch size = 32
2021-08-09 23:21:13,728 ***** Eval results *****
2021-08-09 23:21:13,728   acc = 0.08492676973148902
2021-08-09 23:21:13,728   att_loss = 0.0
2021-08-09 23:21:13,728   cls_loss = 0.20080719896549942
2021-08-09 23:21:13,728   eval_loss = 2.6547269225120544
2021-08-09 23:21:13,728   global_step = 36599
2021-08-09 23:21:13,728   loss = 0.20080719896549942
2021-08-09 23:21:13,728   rep_loss = 0.0
2021-08-09 23:22:05,041 ***** Running evaluation *****
2021-08-09 23:22:05,042   Epoch = 2 iter 36899 step
2021-08-09 23:22:05,042   Num examples = 9832
2021-08-09 23:22:05,042   Batch size = 32
2021-08-09 23:22:15,628 ***** Eval results *****
2021-08-09 23:22:15,628   acc = 0.08604556550040683
2021-08-09 23:22:15,628   att_loss = 0.0
2021-08-09 23:22:15,628   cls_loss = 0.20083142792573858
2021-08-09 23:22:15,628   eval_loss = 2.6392173271674615
2021-08-09 23:22:15,628   global_step = 36899
2021-08-09 23:22:15,628   loss = 0.20083142792573858
2021-08-09 23:22:15,629   rep_loss = 0.0
2021-08-09 23:23:05,376 ***** Running evaluation *****
2021-08-09 23:23:05,376   Epoch = 2 iter 37199 step
2021-08-09 23:23:05,376   Num examples = 9832
2021-08-09 23:23:05,376   Batch size = 32
2021-08-09 23:23:15,978 ***** Eval results *****
2021-08-09 23:23:15,978   acc = 0.08777461350691619
2021-08-09 23:23:15,978   att_loss = 0.0
2021-08-09 23:23:15,978   cls_loss = 0.20082828526451782
2021-08-09 23:23:15,978   eval_loss = 2.6387781415666853
2021-08-09 23:23:15,978   global_step = 37199
2021-08-09 23:23:15,978   loss = 0.20082828526451782
2021-08-09 23:23:15,978   rep_loss = 0.0
2021-08-09 23:24:07,148 ***** Running evaluation *****
2021-08-09 23:24:07,148   Epoch = 2 iter 37499 step
2021-08-09 23:24:07,149   Num examples = 9832
2021-08-09 23:24:07,149   Batch size = 32
2021-08-09 23:24:17,731 ***** Eval results *****
2021-08-09 23:24:17,732   acc = 0.09021562245728235
2021-08-09 23:24:17,732   att_loss = 0.0
2021-08-09 23:24:17,732   cls_loss = 0.20085957140745236
2021-08-09 23:24:17,732   eval_loss = 2.588389957880045
2021-08-09 23:24:17,732   global_step = 37499
2021-08-09 23:24:17,732   loss = 0.20085957140745236
2021-08-09 23:24:17,732   rep_loss = 0.0
2021-08-09 23:25:09,096 ***** Running evaluation *****
2021-08-09 23:25:09,096   Epoch = 2 iter 37799 step
2021-08-09 23:25:09,096   Num examples = 9832
2021-08-09 23:25:09,096   Batch size = 32
2021-08-09 23:25:19,650 ***** Eval results *****
2021-08-09 23:25:19,650   acc = 0.08930024410089504
2021-08-09 23:25:19,650   att_loss = 0.0
2021-08-09 23:25:19,650   cls_loss = 0.20085251706560908
2021-08-09 23:25:19,650   eval_loss = 2.5798367757301826
2021-08-09 23:25:19,650   global_step = 37799
2021-08-09 23:25:19,650   loss = 0.20085251706560908
2021-08-09 23:25:19,650   rep_loss = 0.0
2021-08-09 23:26:10,966 ***** Running evaluation *****
2021-08-09 23:26:10,966   Epoch = 2 iter 38099 step
2021-08-09 23:26:10,966   Num examples = 9832
2021-08-09 23:26:10,966   Batch size = 32
2021-08-09 23:26:21,523 ***** Eval results *****
2021-08-09 23:26:21,523   acc = 0.0863506916192026
2021-08-09 23:26:21,523   att_loss = 0.0
2021-08-09 23:26:21,523   cls_loss = 0.20084714260300032
2021-08-09 23:26:21,523   eval_loss = 2.6332088585023756
2021-08-09 23:26:21,523   global_step = 38099
2021-08-09 23:26:21,523   loss = 0.20084714260300032
2021-08-09 23:26:21,523   rep_loss = 0.0
2021-08-09 23:27:11,232 ***** Running evaluation *****
2021-08-09 23:27:11,233   Epoch = 2 iter 38399 step
2021-08-09 23:27:11,233   Num examples = 9832
2021-08-09 23:27:11,233   Batch size = 32
2021-08-09 23:27:21,793 ***** Eval results *****
2021-08-09 23:27:21,793   acc = 0.08838486574450773
2021-08-09 23:27:21,794   att_loss = 0.0
2021-08-09 23:27:21,794   cls_loss = 0.2008923305258512
2021-08-09 23:27:21,794   eval_loss = 2.6403362658116722
2021-08-09 23:27:21,794   global_step = 38399
2021-08-09 23:27:21,794   loss = 0.2008923305258512
2021-08-09 23:27:21,794   rep_loss = 0.0
2021-08-09 23:28:14,431 ***** Running evaluation *****
2021-08-09 23:28:14,431   Epoch = 2 iter 38699 step
2021-08-09 23:28:14,431   Num examples = 9832
2021-08-09 23:28:14,431   Batch size = 32
2021-08-09 23:28:24,997 ***** Eval results *****
2021-08-09 23:28:24,997   acc = 0.09397884458909683
2021-08-09 23:28:24,997   att_loss = 0.0
2021-08-09 23:28:24,997   cls_loss = 0.20094322095064315
2021-08-09 23:28:24,998   eval_loss = 2.654085795600693
2021-08-09 23:28:24,998   global_step = 38699
2021-08-09 23:28:24,998   loss = 0.20094322095064315
2021-08-09 23:28:24,998   rep_loss = 0.0
2021-08-09 23:29:14,756 ***** Running evaluation *****
2021-08-09 23:29:14,756   Epoch = 2 iter 38999 step
2021-08-09 23:29:14,756   Num examples = 9832
2021-08-09 23:29:14,756   Batch size = 32
2021-08-09 23:29:25,334 ***** Eval results *****
2021-08-09 23:29:25,334   acc = 0.08777461350691619
2021-08-09 23:29:25,334   att_loss = 0.0
2021-08-09 23:29:25,334   cls_loss = 0.20085094474214776
2021-08-09 23:29:25,334   eval_loss = 2.6497105932854987
2021-08-09 23:29:25,334   global_step = 38999
2021-08-09 23:29:25,334   loss = 0.20085094474214776
2021-08-09 23:29:25,334   rep_loss = 0.0
2021-08-09 23:30:15,122 ***** Running evaluation *****
2021-08-09 23:30:15,123   Epoch = 2 iter 39299 step
2021-08-09 23:30:15,123   Num examples = 9832
2021-08-09 23:30:15,123   Batch size = 32
2021-08-09 23:30:25,744 ***** Eval results *****
2021-08-09 23:30:25,744   acc = 0.08736777868185516
2021-08-09 23:30:25,744   att_loss = 0.0
2021-08-09 23:30:25,744   cls_loss = 0.20073974723129107
2021-08-09 23:30:25,745   eval_loss = 2.6262510124739116
2021-08-09 23:30:25,745   global_step = 39299
2021-08-09 23:30:25,745   loss = 0.20073974723129107
2021-08-09 23:30:25,745   rep_loss = 0.0
2021-08-09 23:31:18,618 ***** Running evaluation *****
2021-08-09 23:31:18,619   Epoch = 2 iter 39599 step
2021-08-09 23:31:18,619   Num examples = 9832
2021-08-09 23:31:18,619   Batch size = 32
2021-08-09 23:31:29,185 ***** Eval results *****
2021-08-09 23:31:29,186   acc = 0.08187550854353133
2021-08-09 23:31:29,186   att_loss = 0.0
2021-08-09 23:31:29,186   cls_loss = 0.20066581703149927
2021-08-09 23:31:29,186   eval_loss = 2.64396447794778
2021-08-09 23:31:29,186   global_step = 39599
2021-08-09 23:31:29,186   loss = 0.20066581703149927
2021-08-09 23:31:29,186   rep_loss = 0.0
2021-08-09 23:32:18,947 ***** Running evaluation *****
2021-08-09 23:32:18,948   Epoch = 2 iter 39899 step
2021-08-09 23:32:18,948   Num examples = 9832
2021-08-09 23:32:18,948   Batch size = 32
2021-08-09 23:32:29,500 ***** Eval results *****
2021-08-09 23:32:29,500   acc = 0.08533360455655004
2021-08-09 23:32:29,500   att_loss = 0.0
2021-08-09 23:32:29,500   cls_loss = 0.2006377040421002
2021-08-09 23:32:29,500   eval_loss = 2.5940104931979984
2021-08-09 23:32:29,500   global_step = 39899
2021-08-09 23:32:29,500   loss = 0.2006377040421002
2021-08-09 23:32:29,501   rep_loss = 0.0
2021-08-09 23:33:20,742 ***** Running evaluation *****
2021-08-09 23:33:20,742   Epoch = 2 iter 40199 step
2021-08-09 23:33:20,743   Num examples = 9832
2021-08-09 23:33:20,743   Batch size = 32
2021-08-09 23:33:31,319 ***** Eval results *****
2021-08-09 23:33:31,320   acc = 0.09041903986981285
2021-08-09 23:33:31,320   att_loss = 0.0
2021-08-09 23:33:31,320   cls_loss = 0.20053544548220736
2021-08-09 23:33:31,320   eval_loss = 2.591167236303354
2021-08-09 23:33:31,320   global_step = 40199
2021-08-09 23:33:31,320   loss = 0.20053544548220736
2021-08-09 23:33:31,320   rep_loss = 0.0
2021-08-09 23:34:22,582 ***** Running evaluation *****
2021-08-09 23:34:22,582   Epoch = 2 iter 40499 step
2021-08-09 23:34:22,582   Num examples = 9832
2021-08-09 23:34:22,582   Batch size = 32
2021-08-09 23:34:33,194 ***** Eval results *****
2021-08-09 23:34:33,194   acc = 0.08889340927583401
2021-08-09 23:34:33,195   att_loss = 0.0
2021-08-09 23:34:33,195   cls_loss = 0.20051092468097506
2021-08-09 23:34:33,195   eval_loss = 2.5994967988559177
2021-08-09 23:34:33,195   global_step = 40499
2021-08-09 23:34:33,195   loss = 0.20051092468097506
2021-08-09 23:34:33,195   rep_loss = 0.0
2021-08-09 23:35:22,995 ***** Running evaluation *****
2021-08-09 23:35:22,995   Epoch = 2 iter 40799 step
2021-08-09 23:35:22,995   Num examples = 9832
2021-08-09 23:35:22,995   Batch size = 32
2021-08-09 23:35:35,086 ***** Eval results *****
2021-08-09 23:35:35,086   acc = 0.08879170056956875
2021-08-09 23:35:35,086   att_loss = 0.0
2021-08-09 23:35:35,086   cls_loss = 0.20047053369597018
2021-08-09 23:35:35,087   eval_loss = 2.613565722843269
2021-08-09 23:35:35,087   global_step = 40799
2021-08-09 23:35:35,087   loss = 0.20047053369597018
2021-08-09 23:35:35,087   rep_loss = 0.0
2021-08-09 23:36:24,633 ***** Running evaluation *****
2021-08-09 23:36:24,634   Epoch = 2 iter 41099 step
2021-08-09 23:36:24,634   Num examples = 9832
2021-08-09 23:36:24,634   Batch size = 32
2021-08-09 23:36:35,195 ***** Eval results *****
2021-08-09 23:36:35,195   acc = 0.08685923515052889
2021-08-09 23:36:35,195   att_loss = 0.0
2021-08-09 23:36:35,195   cls_loss = 0.20051714387345687
2021-08-09 23:36:35,195   eval_loss = 2.6341497812952315
2021-08-09 23:36:35,195   global_step = 41099
2021-08-09 23:36:35,195   loss = 0.20051714387345687
2021-08-09 23:36:35,195   rep_loss = 0.0
2021-08-09 23:37:26,423 ***** Running evaluation *****
2021-08-09 23:37:26,424   Epoch = 2 iter 41399 step
2021-08-09 23:37:26,424   Num examples = 9832
2021-08-09 23:37:26,424   Batch size = 32
2021-08-09 23:37:37,032 ***** Eval results *****
2021-08-09 23:37:37,033   acc = 0.09296175752644427
2021-08-09 23:37:37,033   att_loss = 0.0
2021-08-09 23:37:37,033   cls_loss = 0.2004906945534778
2021-08-09 23:37:37,033   eval_loss = 2.5726632135254994
2021-08-09 23:37:37,033   global_step = 41399
2021-08-09 23:37:37,033   loss = 0.2004906945534778
2021-08-09 23:37:37,033   rep_loss = 0.0
2021-08-09 23:38:28,322 ***** Running evaluation *****
2021-08-09 23:38:28,323   Epoch = 2 iter 41699 step
2021-08-09 23:38:28,323   Num examples = 9832
2021-08-09 23:38:28,323   Batch size = 32
2021-08-09 23:38:38,887 ***** Eval results *****
2021-08-09 23:38:38,888   acc = 0.08889340927583401
2021-08-09 23:38:38,888   att_loss = 0.0
2021-08-09 23:38:38,888   cls_loss = 0.20042200026937632
2021-08-09 23:38:38,888   eval_loss = 2.645861055943873
2021-08-09 23:38:38,888   global_step = 41699
2021-08-09 23:38:38,888   loss = 0.20042200026937632
2021-08-09 23:38:38,888   rep_loss = 0.0
2021-08-09 23:39:28,608 ***** Running evaluation *****
2021-08-09 23:39:28,608   Epoch = 2 iter 41999 step
2021-08-09 23:39:28,608   Num examples = 9832
2021-08-09 23:39:28,608   Batch size = 32
2021-08-09 23:39:39,215 ***** Eval results *****
2021-08-09 23:39:39,215   acc = 0.0886899918633035
2021-08-09 23:39:39,215   att_loss = 0.0
2021-08-09 23:39:39,215   cls_loss = 0.2004219762573423
2021-08-09 23:39:39,215   eval_loss = 2.619134069263161
2021-08-09 23:39:39,215   global_step = 41999
2021-08-09 23:39:39,215   loss = 0.2004219762573423
2021-08-09 23:39:39,215   rep_loss = 0.0
2021-08-09 23:40:32,063 ***** Running evaluation *****
2021-08-09 23:40:32,063   Epoch = 2 iter 42299 step
2021-08-09 23:40:32,063   Num examples = 9832
2021-08-09 23:40:32,063   Batch size = 32
2021-08-09 23:40:42,656 ***** Eval results *****
2021-08-09 23:40:42,656   acc = 0.08960537021969081
2021-08-09 23:40:42,657   att_loss = 0.0
2021-08-09 23:40:42,657   cls_loss = 0.20044819263863717
2021-08-09 23:40:42,657   eval_loss = 2.6143015630833513
2021-08-09 23:40:42,657   global_step = 42299
2021-08-09 23:40:42,657   loss = 0.20044819263863717
2021-08-09 23:40:42,657   rep_loss = 0.0
2021-08-09 23:41:32,419 ***** Running evaluation *****
2021-08-09 23:41:32,419   Epoch = 2 iter 42599 step
2021-08-09 23:41:32,419   Num examples = 9832
2021-08-09 23:41:32,419   Batch size = 32
2021-08-09 23:41:43,004 ***** Eval results *****
2021-08-09 23:41:43,005   acc = 0.08665581773799837
2021-08-09 23:41:43,005   att_loss = 0.0
2021-08-09 23:41:43,005   cls_loss = 0.20042454359434544
2021-08-09 23:41:43,005   eval_loss = 2.6025666648691352
2021-08-09 23:41:43,005   global_step = 42599
2021-08-09 23:41:43,005   loss = 0.20042454359434544
2021-08-09 23:41:43,005   rep_loss = 0.0
2021-08-09 23:42:32,829 ***** Running evaluation *****
2021-08-09 23:42:32,829   Epoch = 2 iter 42899 step
2021-08-09 23:42:32,829   Num examples = 9832
2021-08-09 23:42:32,829   Batch size = 32
2021-08-09 23:42:43,461 ***** Eval results *****
2021-08-09 23:42:43,461   acc = 0.08879170056956875
2021-08-09 23:42:43,461   att_loss = 0.0
2021-08-09 23:42:43,461   cls_loss = 0.2004358828421094
2021-08-09 23:42:43,461   eval_loss = 2.594474884596738
2021-08-09 23:42:43,461   global_step = 42899
2021-08-09 23:42:43,461   loss = 0.2004358828421094
2021-08-09 23:42:43,461   rep_loss = 0.0
2021-08-09 23:43:36,382 ***** Running evaluation *****
2021-08-09 23:43:36,383   Epoch = 2 iter 43199 step
2021-08-09 23:43:36,383   Num examples = 9832
2021-08-09 23:43:36,383   Batch size = 32
2021-08-09 23:43:47,030 ***** Eval results *****
2021-08-09 23:43:47,030   acc = 0.08685923515052889
2021-08-09 23:43:47,030   att_loss = 0.0
2021-08-09 23:43:47,030   cls_loss = 0.20045415768795544
2021-08-09 23:43:47,031   eval_loss = 2.5978753435147275
2021-08-09 23:43:47,031   global_step = 43199
2021-08-09 23:43:47,031   loss = 0.20045415768795544
2021-08-09 23:43:47,031   rep_loss = 0.0
2021-08-09 23:44:36,890 ***** Running evaluation *****
2021-08-09 23:44:36,890   Epoch = 2 iter 43499 step
2021-08-09 23:44:36,890   Num examples = 9832
2021-08-09 23:44:36,890   Batch size = 32
2021-08-09 23:44:47,524 ***** Eval results *****
2021-08-09 23:44:47,524   acc = 0.08665581773799837
2021-08-09 23:44:47,524   att_loss = 0.0
2021-08-09 23:44:47,524   cls_loss = 0.20048242987659431
2021-08-09 23:44:47,524   eval_loss = 2.5908650841031755
2021-08-09 23:44:47,524   global_step = 43499
2021-08-09 23:44:47,524   loss = 0.20048242987659431
2021-08-09 23:44:47,525   rep_loss = 0.0
2021-08-09 23:45:38,802 ***** Running evaluation *****
2021-08-09 23:45:38,802   Epoch = 2 iter 43799 step
2021-08-09 23:45:38,803   Num examples = 9832
2021-08-09 23:45:38,803   Batch size = 32
2021-08-09 23:45:49,370 ***** Eval results *****
2021-08-09 23:45:49,370   acc = 0.08746948738812042
2021-08-09 23:45:49,370   att_loss = 0.0
2021-08-09 23:45:49,370   cls_loss = 0.20046662440028803
2021-08-09 23:45:49,371   eval_loss = 2.606837950743638
2021-08-09 23:45:49,371   global_step = 43799
2021-08-09 23:45:49,371   loss = 0.20046662440028803
2021-08-09 23:45:49,371   rep_loss = 0.0
2021-08-09 23:46:40,822 ***** Running evaluation *****
2021-08-09 23:46:40,822   Epoch = 2 iter 44099 step
2021-08-09 23:46:40,822   Num examples = 9832
2021-08-09 23:46:40,822   Batch size = 32
2021-08-09 23:46:51,412 ***** Eval results *****
2021-08-09 23:46:51,412   acc = 0.08726606997558992
2021-08-09 23:46:51,412   att_loss = 0.0
2021-08-09 23:46:51,412   cls_loss = 0.20042749302016158
2021-08-09 23:46:51,412   eval_loss = 2.615958529633361
2021-08-09 23:46:51,412   global_step = 44099
2021-08-09 23:46:51,412   loss = 0.20042749302016158
2021-08-09 23:46:51,412   rep_loss = 0.0
2021-08-09 23:47:42,725 ***** Running evaluation *****
2021-08-09 23:47:42,725   Epoch = 2 iter 44399 step
2021-08-09 23:47:42,725   Num examples = 9832
2021-08-09 23:47:42,725   Batch size = 32
2021-08-09 23:47:53,298 ***** Eval results *****
2021-08-09 23:47:53,299   acc = 0.08940195280716029
2021-08-09 23:47:53,299   att_loss = 0.0
2021-08-09 23:47:53,299   cls_loss = 0.20040439238499302
2021-08-09 23:47:53,299   eval_loss = 2.626459672853544
2021-08-09 23:47:53,299   global_step = 44399
2021-08-09 23:47:53,299   loss = 0.20040439238499302
2021-08-09 23:47:53,299   rep_loss = 0.0
2021-08-09 23:48:43,092 ***** Running evaluation *****
2021-08-09 23:48:43,092   Epoch = 2 iter 44699 step
2021-08-09 23:48:43,092   Num examples = 9832
2021-08-09 23:48:43,093   Batch size = 32
2021-08-09 23:48:53,668 ***** Eval results *****
2021-08-09 23:48:53,668   acc = 0.08655410903173312
2021-08-09 23:48:53,668   att_loss = 0.0
2021-08-09 23:48:53,668   cls_loss = 0.20040447814477488
2021-08-09 23:48:53,668   eval_loss = 2.6383760494071167
2021-08-09 23:48:53,668   global_step = 44699
2021-08-09 23:48:53,668   loss = 0.20040447814477488
2021-08-09 23:48:53,668   rep_loss = 0.0
2021-08-09 23:49:45,117 ***** Running evaluation *****
2021-08-09 23:49:45,118   Epoch = 2 iter 44999 step
2021-08-09 23:49:45,118   Num examples = 9832
2021-08-09 23:49:45,118   Batch size = 32
2021-08-09 23:49:55,700 ***** Eval results *****
2021-08-09 23:49:55,700   acc = 0.08807973962571196
2021-08-09 23:49:55,700   att_loss = 0.0
2021-08-09 23:49:55,700   cls_loss = 0.20031889458272045
2021-08-09 23:49:55,700   eval_loss = 2.6101266540490187
2021-08-09 23:49:55,700   global_step = 44999
2021-08-09 23:49:55,700   loss = 0.20031889458272045
2021-08-09 23:49:55,700   rep_loss = 0.0
2021-08-09 23:50:47,027 ***** Running evaluation *****
2021-08-09 23:50:47,027   Epoch = 2 iter 45299 step
2021-08-09 23:50:47,027   Num examples = 9832
2021-08-09 23:50:47,027   Batch size = 32
2021-08-09 23:50:57,615 ***** Eval results *****
2021-08-09 23:50:57,615   acc = 0.09031733116354759
2021-08-09 23:50:57,616   att_loss = 0.0
2021-08-09 23:50:57,616   cls_loss = 0.20026989719197497
2021-08-09 23:50:57,616   eval_loss = 2.610462105119383
2021-08-09 23:50:57,616   global_step = 45299
2021-08-09 23:50:57,616   loss = 0.20026989719197497
2021-08-09 23:50:57,616   rep_loss = 0.0
2021-08-09 23:51:47,387 ***** Running evaluation *****
2021-08-09 23:51:47,387   Epoch = 2 iter 45599 step
2021-08-09 23:51:47,387   Num examples = 9832
2021-08-09 23:51:47,387   Batch size = 32
2021-08-09 23:51:57,968 ***** Eval results *****
2021-08-09 23:51:57,969   acc = 0.08767290480065093
2021-08-09 23:51:57,969   att_loss = 0.0
2021-08-09 23:51:57,969   cls_loss = 0.20022885407846774
2021-08-09 23:51:57,969   eval_loss = 2.629787496932141
2021-08-09 23:51:57,969   global_step = 45599
2021-08-09 23:51:57,969   loss = 0.20022885407846774
2021-08-09 23:51:57,969   rep_loss = 0.0
2021-08-09 23:52:50,708 ***** Running evaluation *****
2021-08-09 23:52:50,708   Epoch = 2 iter 45899 step
2021-08-09 23:52:50,708   Num examples = 9832
2021-08-09 23:52:50,708   Batch size = 32
2021-08-09 23:53:01,301 ***** Eval results *****
2021-08-09 23:53:01,301   acc = 0.08858828315703825
2021-08-09 23:53:01,301   att_loss = 0.0
2021-08-09 23:53:01,301   cls_loss = 0.20016810045324487
2021-08-09 23:53:01,301   eval_loss = 2.6249793117696587
2021-08-09 23:53:01,301   global_step = 45899
2021-08-09 23:53:01,301   loss = 0.20016810045324487
2021-08-09 23:53:01,301   rep_loss = 0.0
2021-08-09 23:53:50,990 ***** Running evaluation *****
2021-08-09 23:53:50,990   Epoch = 2 iter 46199 step
2021-08-09 23:53:50,990   Num examples = 9832
2021-08-09 23:53:50,990   Batch size = 32
2021-08-09 23:54:01,573 ***** Eval results *****
2021-08-09 23:54:01,574   acc = 0.0863506916192026
2021-08-09 23:54:01,574   att_loss = 0.0
2021-08-09 23:54:01,574   cls_loss = 0.20013022322381416
2021-08-09 23:54:01,574   eval_loss = 2.6379106563407104
2021-08-09 23:54:01,574   global_step = 46199
2021-08-09 23:54:01,574   loss = 0.20013022322381416
2021-08-09 23:54:01,574   rep_loss = 0.0
2021-08-09 23:54:51,375 ***** Running evaluation *****
2021-08-09 23:54:51,376   Epoch = 2 iter 46499 step
2021-08-09 23:54:51,376   Num examples = 9832
2021-08-09 23:54:51,376   Batch size = 32
2021-08-09 23:55:03,448 ***** Eval results *****
2021-08-09 23:55:03,448   acc = 0.08777461350691619
2021-08-09 23:55:03,448   att_loss = 0.0
2021-08-09 23:55:03,449   cls_loss = 0.20015450338885216
2021-08-09 23:55:03,449   eval_loss = 2.6171018264510413
2021-08-09 23:55:03,449   global_step = 46499
2021-08-09 23:55:03,449   loss = 0.20015450338885216
2021-08-09 23:55:03,449   rep_loss = 0.0
2021-08-09 23:55:54,832 ***** Running evaluation *****
2021-08-09 23:55:54,833   Epoch = 2 iter 46799 step
2021-08-09 23:55:54,833   Num examples = 9832
2021-08-09 23:55:54,833   Batch size = 32
2021-08-09 23:56:05,438 ***** Eval results *****
2021-08-09 23:56:05,439   acc = 0.08838486574450773
2021-08-09 23:56:05,439   att_loss = 0.0
2021-08-09 23:56:05,439   cls_loss = 0.20011207541042994
2021-08-09 23:56:05,439   eval_loss = 2.622879306991379
2021-08-09 23:56:05,439   global_step = 46799
2021-08-09 23:56:05,439   loss = 0.20011207541042994
2021-08-09 23:56:05,439   rep_loss = 0.0
2021-08-09 23:56:55,212 ***** Running evaluation *****
2021-08-09 23:56:55,212   Epoch = 2 iter 47099 step
2021-08-09 23:56:55,213   Num examples = 9832
2021-08-09 23:56:55,213   Batch size = 32
2021-08-09 23:57:05,792 ***** Eval results *****
2021-08-09 23:57:05,792   acc = 0.0886899918633035
2021-08-09 23:57:05,792   att_loss = 0.0
2021-08-09 23:57:05,792   cls_loss = 0.20010421233589185
2021-08-09 23:57:05,792   eval_loss = 2.6177902167493645
2021-08-09 23:57:05,792   global_step = 47099
2021-08-09 23:57:05,793   loss = 0.20010421233589185
2021-08-09 23:57:05,793   rep_loss = 0.0
2021-08-10 00:38:58,910 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-10 00:38:59,048 device: cuda n_gpu: 4
2021-08-10 00:39:00,750 Writing example 0 of 9815
2021-08-10 00:39:00,751 *** Example ***
2021-08-10 00:39:00,751 guid: dev_matched-0
2021-08-10 00:39:00,751 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-10 00:39:00,751 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:39:00,751 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:39:00,751 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:39:00,751 label: neutral
2021-08-10 00:39:00,751 label_id: 2
2021-08-10 00:39:05,259 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-10 00:39:06,826 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/final/pytorch_model.bin
2021-08-10 00:39:14,381 loading model...
2021-08-10 00:39:14,396 done!
2021-08-10 00:39:23,411 ***** Running evaluation *****
2021-08-10 00:39:23,411   Num examples = 9815
2021-08-10 00:39:23,411   Batch size = 32
2021-08-10 00:39:35,578 ***** Eval results *****
2021-08-10 00:39:35,579   acc = 0.12022414671421294
2021-08-10 00:39:35,579   eval_loss = 2.403890840005409
2021-08-10 00:40:49,596 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-10 00:40:49,722 device: cuda n_gpu: 4
2021-08-10 00:40:50,467 Writing example 0 of 9815
2021-08-10 00:40:50,468 *** Example ***
2021-08-10 00:40:50,468 guid: dev_matched-0
2021-08-10 00:40:50,468 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-10 00:40:50,468 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:40:50,468 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:40:50,468 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:40:50,468 label: neutral
2021-08-10 00:40:50,468 label_id: 2
2021-08-10 00:40:54,875 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-10 00:40:56,390 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate/pytorch_model.bin
2021-08-10 00:40:58,790 loading model...
2021-08-10 00:40:58,808 done!
2021-08-10 00:41:01,611 ***** Running evaluation *****
2021-08-10 00:41:01,611   Num examples = 9815
2021-08-10 00:41:01,611   Batch size = 32
2021-08-10 00:41:13,726 ***** Eval results *****
2021-08-10 00:41:13,727   acc = 0.22272032603158431
2021-08-10 00:41:13,727   eval_loss = 1.101334180816377
2021-08-10 00:41:39,589 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-10 00:41:39,715 device: cuda n_gpu: 4
2021-08-10 00:41:43,474 Writing example 0 of 9815
2021-08-10 00:41:43,474 *** Example ***
2021-08-10 00:41:43,474 guid: dev_matched-0
2021-08-10 00:41:43,475 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-10 00:41:43,475 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:41:43,475 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:41:43,475 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-10 00:41:43,475 label: neutral
2021-08-10 00:41:43,475 label_id: 2
2021-08-10 00:41:47,908 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-10 00:41:50,304 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-10 00:41:56,569 loading model...
2021-08-10 00:41:56,601 done!
2021-08-10 00:41:56,601 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-10 00:41:56,601 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-10 00:41:59,454 ***** Running evaluation *****
2021-08-10 00:41:59,454   Num examples = 9815
2021-08-10 00:41:59,454   Batch size = 32
2021-08-10 00:42:23,312 ***** Eval results *****
2021-08-10 00:42:23,312   acc = 0.08894549159449822
2021-08-10 00:42:23,312   eval_loss = 2.924482062119226
2021-08-13 22:01:48,724 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:01:48,885 device: cuda n_gpu: 4
2021-08-13 22:01:49,708 Writing example 0 of 9815
2021-08-13 22:01:49,710 *** Example ***
2021-08-13 22:01:49,710 guid: dev_matched-0
2021-08-13 22:01:49,710 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:01:49,710 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:01:49,710 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:01:49,710 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:01:49,710 label: neutral
2021-08-13 22:01:49,710 label_id: 2
2021-08-13 22:01:54,398 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:01:57,124 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:02:12,152 loading model...
2021-08-13 22:02:12,190 done!
2021-08-13 22:02:12,190 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:02:12,190 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:02:18,167 ***** Running evaluation *****
2021-08-13 22:02:18,168   Num examples = 9815
2021-08-13 22:02:18,168   Batch size = 32
2021-08-13 22:02:42,532 ***** Eval results *****
2021-08-13 22:02:42,532   acc = 0.08894549159449822
2021-08-13 22:02:42,532   eval_loss = 2.924482062119226
2021-08-13 22:12:46,890 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:12:47,047 device: cuda n_gpu: 4
2021-08-13 22:12:47,241 Writing example 0 of 9815
2021-08-13 22:12:47,242 *** Example ***
2021-08-13 22:12:47,242 guid: dev_matched-0
2021-08-13 22:12:47,242 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:12:47,242 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:12:47,242 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:12:47,242 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:12:47,242 label: neutral
2021-08-13 22:12:47,242 label_id: 2
2021-08-13 22:12:51,606 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:12:54,008 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:12:54,254 loading model...
2021-08-13 22:12:54,280 done!
2021-08-13 22:12:54,280 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:12:54,280 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:21:55,860 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:21:56,001 device: cuda n_gpu: 4
2021-08-13 22:22:32,886 Writing example 0 of 9815
2021-08-13 22:22:32,887 *** Example ***
2021-08-13 22:22:32,887 guid: dev_matched-0
2021-08-13 22:22:32,887 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:22:32,887 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:22:32,887 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:22:32,887 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:22:32,887 label: neutral
2021-08-13 22:22:32,888 label_id: 2
2021-08-13 22:22:37,245 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:22:39,643 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:22:39,878 loading model...
2021-08-13 22:22:39,903 done!
2021-08-13 22:22:39,903 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:22:39,903 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:24:52,777 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:24:52,900 device: cuda n_gpu: 4
2021-08-13 22:24:53,045 Writing example 0 of 9815
2021-08-13 22:24:53,046 *** Example ***
2021-08-13 22:24:53,046 guid: dev_matched-0
2021-08-13 22:24:53,046 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:24:53,046 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:24:53,046 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:24:53,046 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:24:53,046 label: neutral
2021-08-13 22:24:53,047 label_id: 2
2021-08-13 22:24:57,398 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:24:59,818 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:25:00,053 loading model...
2021-08-13 22:25:00,086 done!
2021-08-13 22:25:00,086 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:25:00,086 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:26:12,598 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:26:12,723 device: cuda n_gpu: 4
2021-08-13 22:26:12,866 Writing example 0 of 9815
2021-08-13 22:26:12,867 *** Example ***
2021-08-13 22:26:12,867 guid: dev_matched-0
2021-08-13 22:26:12,867 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:26:12,867 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:26:12,867 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:26:12,867 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:26:12,867 label: neutral
2021-08-13 22:26:12,867 label_id: 2
2021-08-13 22:26:17,187 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:26:19,586 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:26:19,822 loading model...
2021-08-13 22:26:19,846 done!
2021-08-13 22:26:19,847 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:26:19,847 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:27:24,696 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:27:24,856 device: cuda n_gpu: 4
2021-08-13 22:27:24,999 Writing example 0 of 9815
2021-08-13 22:27:25,000 *** Example ***
2021-08-13 22:27:25,000 guid: dev_matched-0
2021-08-13 22:27:25,000 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:27:25,000 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:27:25,000 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:27:25,000 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:27:25,000 label: neutral
2021-08-13 22:27:25,000 label_id: 2
2021-08-13 22:27:29,494 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:27:31,907 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:27:32,144 loading model...
2021-08-13 22:27:32,169 done!
2021-08-13 22:27:32,169 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:27:32,169 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:27:35,075 ***** Running evaluation *****
2021-08-13 22:27:35,076   Num examples = 9815
2021-08-13 22:27:35,076   Batch size = 32
2021-08-13 22:27:58,007 ***** Eval results *****
2021-08-13 22:27:58,007   acc = 0.08894549159449822
2021-08-13 22:27:58,007   eval_loss = 2.924482083864243
2021-08-13 22:38:51,610 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:38:51,737 device: cuda n_gpu: 4
2021-08-13 22:38:51,935 Writing example 0 of 9815
2021-08-13 22:38:51,935 *** Example ***
2021-08-13 22:38:51,935 guid: dev_matched-0
2021-08-13 22:38:51,935 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:38:51,935 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:38:51,936 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:38:51,936 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:38:51,936 label: neutral
2021-08-13 22:38:51,936 label_id: 2
2021-08-13 22:38:56,257 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:38:58,660 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:38:58,901 loading model...
2021-08-13 22:38:58,926 done!
2021-08-13 22:38:58,926 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:38:58,926 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-13 22:39:01,869 ***** Running evaluation *****
2021-08-13 22:39:01,869   Num examples = 9815
2021-08-13 22:39:01,869   Batch size = 32
2021-08-13 22:39:24,801 ***** Eval results *****
2021-08-13 22:39:24,802   acc = 0.08894549159449822
2021-08-13 22:39:24,802   eval_loss = 2.924482083864243
2021-08-13 22:40:18,854 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-13 22:40:19,063 device: cuda n_gpu: 4
2021-08-13 22:40:19,207 Writing example 0 of 9815
2021-08-13 22:40:19,208 *** Example ***
2021-08-13 22:40:19,208 guid: dev_matched-0
2021-08-13 22:40:19,208 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-13 22:40:19,208 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:40:19,208 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:40:19,208 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-13 22:40:19,208 label: neutral
2021-08-13 22:40:19,208 label_id: 2
2021-08-13 22:40:23,555 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-13 22:40:25,970 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-13 22:40:26,207 loading model...
2021-08-13 22:40:26,241 done!
2021-08-13 22:40:26,241 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-13 22:40:26,241 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:15:38,278 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/cased/no_trainer', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:15:38,455 device: cuda n_gpu: 4
2021-08-14 01:18:13,306 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=300, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:18:13,430 device: cuda n_gpu: 4
2021-08-14 01:18:55,124 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/cased/no_trainer', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:18:55,250 device: cuda n_gpu: 4
2021-08-14 01:18:55,848 Writing example 0 of 9815
2021-08-14 01:18:55,848 *** Example ***
2021-08-14 01:18:55,849 guid: dev_matched-0
2021-08-14 01:18:55,849 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:18:55,849 input_ids: 101 1103 1207 2266 1132 3505 1536 102 2490 1541 7407 1103 15649 6245 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:18:55,849 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:18:55,849 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:18:55,849 label: neutral
2021-08-14 01:18:55,849 label_id: 2
2021-08-14 01:19:00,950 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-14 01:19:03,368 Loading model /home/mcao610/scratch/huggingface/MNLI/cased/no_trainer/pytorch_model.bin
2021-08-14 01:19:13,176 loading model...
2021-08-14 01:19:13,210 done!
2021-08-14 01:19:13,210 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:19:13,210 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:19:36,604 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/cased/no_trainer', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:19:36,731 device: cuda n_gpu: 4
2021-08-14 01:19:36,876 Writing example 0 of 9815
2021-08-14 01:19:36,877 *** Example ***
2021-08-14 01:19:36,877 guid: dev_matched-0
2021-08-14 01:19:36,877 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:19:36,877 input_ids: 101 1103 1207 2266 1132 3505 1536 102 2490 1541 7407 1103 15649 6245 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:19:36,877 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:19:36,877 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:19:36,877 label: neutral
2021-08-14 01:19:36,877 label_id: 2
2021-08-14 01:19:41,283 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-14 01:19:43,658 Loading model /home/mcao610/scratch/huggingface/MNLI/cased/no_trainer/pytorch_model.bin
2021-08-14 01:19:43,904 loading model...
2021-08-14 01:19:43,929 done!
2021-08-14 01:19:43,929 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:19:43,929 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:19:46,786 ***** Running evaluation *****
2021-08-14 01:19:46,786   Num examples = 9815
2021-08-14 01:19:46,787   Batch size = 32
2021-08-14 01:20:09,665 ***** Eval results *****
2021-08-14 01:20:09,665   acc = 0.097809475292919
2021-08-14 01:20:09,665   eval_loss = 2.592961578493398
2021-08-14 01:27:47,147 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/cased/no_trainer', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:27:47,295 device: cuda n_gpu: 4
2021-08-14 01:27:47,447 Writing example 0 of 9815
2021-08-14 01:27:47,448 *** Example ***
2021-08-14 01:27:47,448 guid: dev_matched-0
2021-08-14 01:27:47,448 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:27:47,448 input_ids: 101 1103 1207 2266 1132 3505 1536 102 2490 1541 7407 1103 15649 6245 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:27:47,448 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:27:47,448 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:27:47,448 label: neutral
2021-08-14 01:27:47,448 label_id: 2
2021-08-14 01:27:51,849 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-14 01:27:54,231 Loading model /home/mcao610/scratch/huggingface/MNLI/cased/no_trainer/pytorch_model.bin
2021-08-14 01:27:54,471 loading model...
2021-08-14 01:27:54,496 done!
2021-08-14 01:27:54,496 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:27:54,496 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:27:57,403 ***** Running evaluation *****
2021-08-14 01:27:57,403   Num examples = 9815
2021-08-14 01:27:57,403   Batch size = 32
2021-08-14 01:33:22,860 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/SST-2', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/SST-2/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/SST-2/cased', task_name='SST-2', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:33:23,004 device: cuda n_gpu: 4
2021-08-14 01:33:23,338 Writing example 0 of 872
2021-08-14 01:33:23,339 *** Example ***
2021-08-14 01:33:23,339 guid: dev-1
2021-08-14 01:33:23,339 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2021-08-14 01:33:23,339 input_ids: 101 1122 112 188 170 14186 1105 1510 12759 5012 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:33:23,339 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:33:23,339 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:33:23,339 label: 1
2021-08-14 01:33:23,339 label_id: 1
2021-08-14 01:33:23,587 Model config {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

2021-08-14 01:33:25,997 Loading model /home/mcao610/scratch/huggingface/SST-2/cased/pytorch_model.bin
2021-08-14 01:33:30,588 loading model...
2021-08-14 01:33:30,618 done!
2021-08-14 01:33:30,618 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:33:30,618 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:33:33,469 ***** Running evaluation *****
2021-08-14 01:33:33,470   Num examples = 872
2021-08-14 01:33:33,470   Batch size = 32
2021-08-14 01:33:35,509 ***** Eval results *****
2021-08-14 01:33:35,509   acc = 0.9197247706422018
2021-08-14 01:33:35,509   eval_loss = 0.22334408001708134
2021-08-14 01:37:59,461 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:37:59,601 device: cuda n_gpu: 4
2021-08-14 01:37:59,841 Writing example 0 of 9815
2021-08-14 01:37:59,842 *** Example ***
2021-08-14 01:37:59,842 guid: dev_matched-0
2021-08-14 01:37:59,842 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:37:59,842 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:37:59,842 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:37:59,842 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:37:59,842 label: neutral
2021-08-14 01:37:59,842 label_id: 2
2021-08-14 01:38:04,571 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:38:07,064 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:38:27,427 loading model...
2021-08-14 01:38:27,452 done!
2021-08-14 01:38:27,452 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:38:27,452 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:38:30,284 ***** Running evaluation *****
2021-08-14 01:38:30,284   Num examples = 9815
2021-08-14 01:38:30,284   Batch size = 32
2021-08-14 01:43:13,001 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:43:13,157 device: cuda n_gpu: 4
2021-08-14 01:43:13,305 Writing example 0 of 9815
2021-08-14 01:43:13,306 *** Example ***
2021-08-14 01:43:13,306 guid: dev_matched-0
2021-08-14 01:43:13,306 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:43:13,306 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:43:13,306 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:43:13,306 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:43:13,306 label: neutral
2021-08-14 01:43:13,306 label_id: 2
2021-08-14 01:43:17,699 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:43:20,107 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:43:20,346 loading model...
2021-08-14 01:43:20,371 done!
2021-08-14 01:43:20,371 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:43:20,371 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:43:23,249 ***** Running evaluation *****
2021-08-14 01:43:23,249   Num examples = 9815
2021-08-14 01:43:23,249   Batch size = 32
2021-08-14 01:43:46,079 ***** Eval results *****
2021-08-14 01:43:46,080   acc = 0.08894549159449822
2021-08-14 01:43:46,080   eval_loss = 2.924482083864243
2021-08-14 01:44:43,947 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:44:44,087 device: cuda n_gpu: 4
2021-08-14 01:44:44,233 Writing example 0 of 9815
2021-08-14 01:44:44,234 *** Example ***
2021-08-14 01:44:44,234 guid: dev_matched-0
2021-08-14 01:44:44,234 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:44:44,234 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:44:44,234 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:44:44,234 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:44:44,234 label: neutral
2021-08-14 01:44:44,235 label_id: 2
2021-08-14 01:44:48,636 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:44:51,057 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:44:51,287 loading model...
2021-08-14 01:44:51,319 done!
2021-08-14 01:44:51,319 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:44:51,319 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:44:54,158 ***** Running evaluation *****
2021-08-14 01:44:54,158   Num examples = 9815
2021-08-14 01:44:54,159   Batch size = 32
2021-08-14 01:45:04,673 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:45:04,798 device: cuda n_gpu: 4
2021-08-14 01:45:04,943 Writing example 0 of 9815
2021-08-14 01:45:04,943 *** Example ***
2021-08-14 01:45:04,943 guid: dev_matched-0
2021-08-14 01:45:04,944 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:45:04,944 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:45:04,944 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:45:04,944 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:45:04,944 label: neutral
2021-08-14 01:45:04,944 label_id: 2
2021-08-14 01:45:09,290 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:45:11,742 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:45:11,970 loading model...
2021-08-14 01:45:12,003 done!
2021-08-14 01:45:12,003 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:45:12,003 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:45:14,913 ***** Running evaluation *****
2021-08-14 01:45:14,913   Num examples = 9815
2021-08-14 01:45:14,913   Batch size = 32
2021-08-14 01:45:37,792 ***** Eval results *****
2021-08-14 01:45:37,792   acc = 0.08894549159449822
2021-08-14 01:45:37,792   eval_loss = 0.4246328189143917
2021-08-14 01:46:05,203 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:46:05,353 device: cuda n_gpu: 4
2021-08-14 01:46:05,498 Writing example 0 of 9815
2021-08-14 01:46:05,499 *** Example ***
2021-08-14 01:46:05,499 guid: dev_matched-0
2021-08-14 01:46:05,499 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:46:05,499 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:46:05,499 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:46:05,499 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:46:05,499 label: neutral
2021-08-14 01:46:05,499 label_id: 2
2021-08-14 01:46:09,840 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:46:12,263 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:46:12,494 loading model...
2021-08-14 01:46:12,520 done!
2021-08-14 01:46:12,520 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:46:12,520 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:46:15,360 ***** Running evaluation *****
2021-08-14 01:46:15,361   Num examples = 9815
2021-08-14 01:46:15,361   Batch size = 32
2021-08-14 01:46:38,289 ***** Eval results *****
2021-08-14 01:46:38,289   acc = 0.08894549159449822
2021-08-14 01:46:38,289   eval_loss = 0.4246328189143917
2021-08-14 01:47:03,391 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:47:03,539 device: cuda n_gpu: 4
2021-08-14 01:47:03,686 Writing example 0 of 9815
2021-08-14 01:47:03,687 *** Example ***
2021-08-14 01:47:03,687 guid: dev_matched-0
2021-08-14 01:47:03,687 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:47:03,687 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:03,687 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:03,687 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:03,687 label: neutral
2021-08-14 01:47:03,687 label_id: 2
2021-08-14 01:47:08,172 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:47:10,635 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:47:10,866 loading model...
2021-08-14 01:47:10,899 done!
2021-08-14 01:47:10,899 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:47:10,899 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:47:13,767 ***** Running evaluation *****
2021-08-14 01:47:13,767   Num examples = 9815
2021-08-14 01:47:13,767   Batch size = 32
2021-08-14 01:47:40,359 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:47:40,503 device: cuda n_gpu: 4
2021-08-14 01:47:40,651 Writing example 0 of 9815
2021-08-14 01:47:40,652 *** Example ***
2021-08-14 01:47:40,652 guid: dev_matched-0
2021-08-14 01:47:40,652 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:47:40,652 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:40,652 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:40,652 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:47:40,652 label: neutral
2021-08-14 01:47:40,652 label_id: 2
2021-08-14 01:47:45,021 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:47:47,431 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:47:47,659 loading model...
2021-08-14 01:47:47,684 done!
2021-08-14 01:47:47,684 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:47:47,684 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:47:50,546 ***** Running evaluation *****
2021-08-14 01:47:50,546   Num examples = 9815
2021-08-14 01:47:50,547   Batch size = 32
2021-08-14 01:50:26,881 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:50:27,124 device: cuda n_gpu: 4
2021-08-14 01:50:27,274 Writing example 0 of 9815
2021-08-14 01:50:27,274 *** Example ***
2021-08-14 01:50:27,274 guid: dev_matched-0
2021-08-14 01:50:27,274 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:50:27,274 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:50:27,275 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:50:27,275 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:50:27,275 label: neutral
2021-08-14 01:50:27,275 label_id: 2
2021-08-14 01:50:31,625 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:50:34,032 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:50:34,260 loading model...
2021-08-14 01:50:34,285 done!
2021-08-14 01:50:34,285 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:50:34,285 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:50:37,122 ***** Running evaluation *****
2021-08-14 01:50:37,122   Num examples = 9815
2021-08-14 01:50:37,123   Batch size = 32
2021-08-14 01:51:33,016 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 01:51:33,169 device: cuda n_gpu: 4
2021-08-14 01:51:33,317 Writing example 0 of 9815
2021-08-14 01:51:33,317 *** Example ***
2021-08-14 01:51:33,317 guid: dev_matched-0
2021-08-14 01:51:33,317 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 01:51:33,318 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:51:33,318 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:51:33,318 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 01:51:33,318 label: neutral
2021-08-14 01:51:33,318 label_id: 2
2021-08-14 01:51:37,668 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 01:51:40,077 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 01:51:40,313 loading model...
2021-08-14 01:51:40,339 done!
2021-08-14 01:51:40,340 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 01:51:40,340 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 01:51:43,229 ***** Running evaluation *****
2021-08-14 01:51:43,230   Num examples = 9815
2021-08-14 01:51:43,230   Batch size = 32
2021-08-14 01:52:06,144 ***** Eval results *****
2021-08-14 01:52:06,144   acc = 0.8445236882322975
2021-08-14 01:52:06,144   eval_loss = 0.4246328189143917
2021-08-14 02:00:41,331 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 02:00:41,480 device: cuda n_gpu: 4
2021-08-14 02:00:41,678 Writing example 0 of 9815
2021-08-14 02:00:41,679 *** Example ***
2021-08-14 02:00:41,679 guid: dev_matched-0
2021-08-14 02:00:41,679 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 02:00:41,679 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:00:41,679 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:00:41,679 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:00:41,679 label: neutral
2021-08-14 02:00:41,680 label_id: 1
2021-08-14 02:00:46,005 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 02:00:48,448 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 02:00:48,676 loading model...
2021-08-14 02:00:48,709 done!
2021-08-14 02:00:48,709 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 02:00:48,709 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 02:00:51,577 ***** Running evaluation *****
2021-08-14 02:00:51,577   Num examples = 9815
2021-08-14 02:00:51,577   Batch size = 32
2021-08-14 02:01:14,441 ***** Eval results *****
2021-08-14 02:01:14,442   acc = 0.8445236882322975
2021-08-14 02:01:14,442   eval_loss = 0.4246328189143917
2021-08-14 02:06:14,485 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 02:06:14,629 device: cuda n_gpu: 4
2021-08-14 02:06:23,734 Writing example 0 of 505555
2021-08-14 02:06:23,735 *** Example ***
2021-08-14 02:06:23,735 guid: aug-0
2021-08-14 02:06:23,735 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 02:06:23,735 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:06:23,735 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:06:23,735 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:06:23,735 label: neutral
2021-08-14 02:06:23,735 label_id: 1
2021-08-14 02:06:28,477 Writing example 10000 of 505555
2021-08-14 02:06:33,108 Writing example 20000 of 505555
2021-08-14 02:06:37,619 Writing example 30000 of 505555
2021-08-14 02:06:42,402 Writing example 40000 of 505555
2021-08-14 02:06:47,162 Writing example 50000 of 505555
2021-08-14 02:06:51,625 Writing example 60000 of 505555
2021-08-14 02:06:56,219 Writing example 70000 of 505555
2021-08-14 02:07:00,757 Writing example 80000 of 505555
2021-08-14 02:07:05,678 Writing example 90000 of 505555
2021-08-14 02:07:10,236 Writing example 100000 of 505555
2021-08-14 02:07:14,821 Writing example 110000 of 505555
2021-08-14 02:07:19,149 Writing example 120000 of 505555
2021-08-14 02:07:23,773 Writing example 130000 of 505555
2021-08-14 02:07:29,048 Writing example 140000 of 505555
2021-08-14 02:07:33,516 Writing example 150000 of 505555
2021-08-14 02:07:38,265 Writing example 160000 of 505555
2021-08-14 02:07:42,890 Writing example 170000 of 505555
2021-08-14 02:07:47,435 Writing example 180000 of 505555
2021-08-14 02:07:52,002 Writing example 190000 of 505555
2021-08-14 02:07:56,536 Writing example 200000 of 505555
2021-08-14 02:08:02,006 Writing example 210000 of 505555
2021-08-14 02:08:06,430 Writing example 220000 of 505555
2021-08-14 02:08:11,051 Writing example 230000 of 505555
2021-08-14 02:08:15,556 Writing example 240000 of 505555
2021-08-14 02:08:20,351 Writing example 250000 of 505555
2021-08-14 02:08:24,975 Writing example 260000 of 505555
2021-08-14 02:08:29,437 Writing example 270000 of 505555
2021-08-14 02:08:33,862 Writing example 280000 of 505555
2021-08-14 02:08:39,477 Writing example 290000 of 505555
2021-08-14 02:08:44,185 Writing example 300000 of 505555
2021-08-14 02:08:48,680 Writing example 310000 of 505555
2021-08-14 02:08:53,059 Writing example 320000 of 505555
2021-08-14 02:08:57,694 Writing example 330000 of 505555
2021-08-14 02:09:02,158 Writing example 340000 of 505555
2021-08-14 02:09:06,631 Writing example 350000 of 505555
2021-08-14 02:09:11,129 Writing example 360000 of 505555
2021-08-14 02:09:15,498 Writing example 370000 of 505555
2021-08-14 02:09:20,041 Writing example 380000 of 505555
2021-08-14 02:09:26,074 Writing example 390000 of 505555
2021-08-14 02:09:30,645 Writing example 400000 of 505555
2021-08-14 02:09:35,397 Writing example 410000 of 505555
2021-08-14 02:09:39,935 Writing example 420000 of 505555
2021-08-14 02:09:44,396 Writing example 430000 of 505555
2021-08-14 02:09:48,768 Writing example 440000 of 505555
2021-08-14 02:09:53,532 Writing example 450000 of 505555
2021-08-14 02:09:55,436 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/6L_768D_FinalModel/MNLI', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 02:09:55,606 device: cuda n_gpu: 4
2021-08-14 02:09:58,181 Writing example 460000 of 505555
2021-08-14 02:10:02,741 Writing example 470000 of 505555
2021-08-14 02:10:04,258 Writing example 0 of 505555
2021-08-14 02:10:04,261 *** Example ***
2021-08-14 02:10:04,261 guid: aug-0
2021-08-14 02:10:04,261 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 02:10:04,261 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:04,261 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:04,261 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:04,262 label: neutral
2021-08-14 02:10:04,262 label_id: 1
2021-08-14 02:10:07,356 Writing example 480000 of 505555
2021-08-14 02:10:08,915 Writing example 10000 of 505555
2021-08-14 02:10:11,947 Writing example 490000 of 505555
2021-08-14 02:10:13,599 Writing example 20000 of 505555
2021-08-14 02:10:16,484 Writing example 500000 of 505555
2021-08-14 02:10:18,139 Writing example 30000 of 505555
2021-08-14 02:10:22,809 Writing example 40000 of 505555
2021-08-14 02:10:23,662 Writing example 0 of 9815
2021-08-14 02:10:23,677 *** Example ***
2021-08-14 02:10:23,677 guid: dev_matched-0
2021-08-14 02:10:23,677 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 02:10:23,678 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:23,678 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:23,678 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:10:23,678 label: neutral
2021-08-14 02:10:23,678 label_id: 1
2021-08-14 02:10:27,608 Writing example 50000 of 505555
2021-08-14 02:10:28,083 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 02:10:30,554 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 02:10:30,782 loading model...
2021-08-14 02:10:30,815 done!
2021-08-14 02:10:30,815 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 02:10:30,815 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 02:10:32,241 Writing example 60000 of 505555
2021-08-14 02:10:36,062 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-14 02:10:36,906 Writing example 70000 of 505555
2021-08-14 02:10:37,598 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-08-14 02:10:38,675 loading model...
2021-08-14 02:10:38,688 done!
2021-08-14 02:10:38,689 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-14 02:10:38,689 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-14 02:10:38,757 ***** Running training *****
2021-08-14 02:10:38,757   Num examples = 505555
2021-08-14 02:10:38,757   Batch size = 32
2021-08-14 02:10:38,757   Num steps = 157980
2021-08-14 02:10:38,758 n: module.bert.embeddings.word_embeddings.weight
2021-08-14 02:10:38,758 n: module.bert.embeddings.position_embeddings.weight
2021-08-14 02:10:38,758 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-14 02:10:38,758 n: module.bert.embeddings.LayerNorm.weight
2021-08-14 02:10:38,758 n: module.bert.embeddings.LayerNorm.bias
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-14 02:10:38,758 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-14 02:10:38,759 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-14 02:10:38,760 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-14 02:10:38,761 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-14 02:10:38,762 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-14 02:10:38,763 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-14 02:10:38,763 n: module.bert.pooler.dense.weight
2021-08-14 02:10:38,763 n: module.bert.pooler.dense.bias
2021-08-14 02:10:38,764 n: module.classifier.weight
2021-08-14 02:10:38,764 n: module.classifier.bias
2021-08-14 02:10:38,764 n: module.fit_dense.weight
2021-08-14 02:10:38,764 n: module.fit_dense.bias
2021-08-14 02:10:38,764 Total parameters: 67547907
2021-08-14 02:10:41,496 Writing example 80000 of 505555
2021-08-14 02:10:46,330 Writing example 90000 of 505555
2021-08-14 02:10:50,889 Writing example 100000 of 505555
2021-08-14 02:10:55,656 Writing example 110000 of 505555
2021-08-14 02:11:00,122 Writing example 120000 of 505555
2021-08-14 02:11:04,756 Writing example 130000 of 505555
2021-08-14 02:12:55,152 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=3000, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 02:12:55,295 device: cuda n_gpu: 4
2021-08-14 02:13:03,257 Writing example 0 of 505555
2021-08-14 02:13:03,258 *** Example ***
2021-08-14 02:13:03,258 guid: aug-0
2021-08-14 02:13:03,258 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 02:13:03,259 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:13:03,259 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:13:03,259 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:13:03,259 label: neutral
2021-08-14 02:13:03,259 label_id: 1
2021-08-14 02:13:07,933 Writing example 10000 of 505555
2021-08-14 02:13:12,579 Writing example 20000 of 505555
2021-08-14 02:13:17,088 Writing example 30000 of 505555
2021-08-14 02:13:21,809 Writing example 40000 of 505555
2021-08-14 02:13:26,639 Writing example 50000 of 505555
2021-08-14 02:13:31,137 Writing example 60000 of 505555
2021-08-14 02:13:35,801 Writing example 70000 of 505555
2021-08-14 02:13:40,368 Writing example 80000 of 505555
2021-08-14 02:13:45,211 Writing example 90000 of 505555
2021-08-14 02:13:49,837 Writing example 100000 of 505555
2021-08-14 02:13:54,421 Writing example 110000 of 505555
2021-08-14 02:13:58,824 Writing example 120000 of 505555
2021-08-14 02:14:03,496 Writing example 130000 of 505555
2021-08-14 02:14:08,802 Writing example 140000 of 505555
2021-08-14 02:14:13,200 Writing example 150000 of 505555
2021-08-14 02:14:18,005 Writing example 160000 of 505555
2021-08-14 02:14:22,636 Writing example 170000 of 505555
2021-08-14 02:14:27,209 Writing example 180000 of 505555
2021-08-14 02:14:31,816 Writing example 190000 of 505555
2021-08-14 02:14:36,369 Writing example 200000 of 505555
2021-08-14 02:14:41,961 Writing example 210000 of 505555
2021-08-14 02:14:46,433 Writing example 220000 of 505555
2021-08-14 02:14:51,050 Writing example 230000 of 505555
2021-08-14 02:14:55,643 Writing example 240000 of 505555
2021-08-14 02:15:00,277 Writing example 250000 of 505555
2021-08-14 02:15:04,963 Writing example 260000 of 505555
2021-08-14 02:15:09,522 Writing example 270000 of 505555
2021-08-14 02:15:13,957 Writing example 280000 of 505555
2021-08-14 02:15:19,567 Writing example 290000 of 505555
2021-08-14 02:15:24,269 Writing example 300000 of 505555
2021-08-14 02:15:28,848 Writing example 310000 of 505555
2021-08-14 02:15:33,476 Writing example 320000 of 505555
2021-08-14 02:15:38,124 Writing example 330000 of 505555
2021-08-14 02:15:42,540 Writing example 340000 of 505555
2021-08-14 02:15:47,024 Writing example 350000 of 505555
2021-08-14 02:15:51,496 Writing example 360000 of 505555
2021-08-14 02:15:56,095 Writing example 370000 of 505555
2021-08-14 02:16:00,819 Writing example 380000 of 505555
2021-08-14 02:16:06,655 Writing example 390000 of 505555
2021-08-14 02:16:11,239 Writing example 400000 of 505555
2021-08-14 02:16:15,770 Writing example 410000 of 505555
2021-08-14 02:16:20,322 Writing example 420000 of 505555
2021-08-14 02:16:24,742 Writing example 430000 of 505555
2021-08-14 02:16:29,180 Writing example 440000 of 505555
2021-08-14 02:16:33,776 Writing example 450000 of 505555
2021-08-14 02:16:38,400 Writing example 460000 of 505555
2021-08-14 02:16:42,939 Writing example 470000 of 505555
2021-08-14 02:16:47,678 Writing example 480000 of 505555
2021-08-14 02:16:52,238 Writing example 490000 of 505555
2021-08-14 02:16:56,761 Writing example 500000 of 505555
2021-08-14 02:17:04,354 Writing example 0 of 9815
2021-08-14 02:17:04,355 *** Example ***
2021-08-14 02:17:04,355 guid: dev_matched-0
2021-08-14 02:17:04,355 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 02:17:04,355 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:17:04,355 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:17:04,355 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 02:17:04,355 label: neutral
2021-08-14 02:17:04,355 label_id: 1
2021-08-14 02:17:09,198 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 02:17:11,735 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 02:17:39,797 loading model...
2021-08-14 02:17:39,828 done!
2021-08-14 02:17:39,828 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 02:17:39,828 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 02:17:45,355 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-14 02:17:46,841 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-08-14 02:17:48,223 loading model...
2021-08-14 02:17:48,238 done!
2021-08-14 02:17:48,239 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-14 02:17:48,239 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-14 02:17:48,306 ***** Running training *****
2021-08-14 02:17:48,306   Num examples = 505555
2021-08-14 02:17:48,306   Batch size = 32
2021-08-14 02:17:48,306   Num steps = 157980
2021-08-14 02:17:48,307 n: module.bert.embeddings.word_embeddings.weight
2021-08-14 02:17:48,308 n: module.bert.embeddings.position_embeddings.weight
2021-08-14 02:17:48,308 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-14 02:17:48,308 n: module.bert.embeddings.LayerNorm.weight
2021-08-14 02:17:48,308 n: module.bert.embeddings.LayerNorm.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-14 02:17:48,308 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-14 02:17:48,309 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-14 02:17:48,310 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-14 02:17:48,311 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-14 02:17:48,312 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-14 02:17:48,313 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-14 02:17:48,313 n: module.bert.pooler.dense.weight
2021-08-14 02:17:48,313 n: module.bert.pooler.dense.bias
2021-08-14 02:17:48,313 n: module.classifier.weight
2021-08-14 02:17:48,313 n: module.classifier.bias
2021-08-14 02:17:48,313 n: module.fit_dense.weight
2021-08-14 02:17:48,313 n: module.fit_dense.bias
2021-08-14 02:17:48,314 Total parameters: 67547907
2021-08-14 02:26:43,271 ***** Running evaluation *****
2021-08-14 02:26:43,272   Epoch = 0 iter 2999 step
2021-08-14 02:26:43,272   Num examples = 9815
2021-08-14 02:26:43,272   Batch size = 32
2021-08-14 02:26:43,346 ***** Eval results *****
2021-08-14 02:26:43,346   att_loss = 2.041318840684792
2021-08-14 02:26:43,346   cls_loss = 0.0
2021-08-14 02:26:43,346   global_step = 2999
2021-08-14 02:26:43,346   loss = 2.9986329173278556
2021-08-14 02:26:43,346   rep_loss = 0.9573140777759291
2021-08-14 02:26:43,347 ***** Save model *****
2021-08-14 02:35:12,694 ***** Running evaluation *****
2021-08-14 02:35:12,694   Epoch = 0 iter 5999 step
2021-08-14 02:35:12,694   Num examples = 9815
2021-08-14 02:35:12,694   Batch size = 32
2021-08-14 02:35:12,696 ***** Eval results *****
2021-08-14 02:35:12,696   att_loss = 1.981861229081177
2021-08-14 02:35:12,696   cls_loss = 0.0
2021-08-14 02:35:12,696   global_step = 5999
2021-08-14 02:35:12,696   loss = 2.920272613728239
2021-08-14 02:35:12,696   rep_loss = 0.9384113845278331
2021-08-14 02:35:12,697 ***** Save model *****
2021-08-14 02:43:48,689 ***** Running evaluation *****
2021-08-14 02:43:48,689   Epoch = 0 iter 8999 step
2021-08-14 02:43:48,690   Num examples = 9815
2021-08-14 02:43:48,690   Batch size = 32
2021-08-14 02:43:48,691 ***** Eval results *****
2021-08-14 02:43:48,691   att_loss = 1.9529792332148497
2021-08-14 02:43:48,691   cls_loss = 0.0
2021-08-14 02:43:48,691   global_step = 8999
2021-08-14 02:43:48,691   loss = 2.8810387173815957
2021-08-14 02:43:48,691   rep_loss = 0.928059483299071
2021-08-14 02:43:48,692 ***** Save model *****
2021-08-14 02:52:34,473 ***** Running evaluation *****
2021-08-14 02:52:34,473   Epoch = 0 iter 11999 step
2021-08-14 02:52:34,473   Num examples = 9815
2021-08-14 02:52:34,473   Batch size = 32
2021-08-14 02:52:34,524 ***** Eval results *****
2021-08-14 02:52:34,524   att_loss = 1.9325415511977346
2021-08-14 02:52:34,524   cls_loss = 0.0
2021-08-14 02:52:34,524   global_step = 11999
2021-08-14 02:52:34,524   loss = 2.8531555426062143
2021-08-14 02:52:34,524   rep_loss = 0.9206139907378715
2021-08-14 02:52:34,525 ***** Save model *****
2021-08-14 03:01:03,230 ***** Running evaluation *****
2021-08-14 03:01:03,231   Epoch = 0 iter 14999 step
2021-08-14 03:01:03,231   Num examples = 9815
2021-08-14 03:01:03,231   Batch size = 32
2021-08-14 03:01:03,296 ***** Eval results *****
2021-08-14 03:01:03,296   att_loss = 1.917297746954111
2021-08-14 03:01:03,296   cls_loss = 0.0
2021-08-14 03:01:03,296   global_step = 14999
2021-08-14 03:01:03,296   loss = 2.832207039313536
2021-08-14 03:01:03,296   rep_loss = 0.9149092918706342
2021-08-14 03:01:03,297 ***** Save model *****
2021-08-14 03:09:41,482 ***** Running evaluation *****
2021-08-14 03:09:41,483   Epoch = 1 iter 17999 step
2021-08-14 03:09:41,483   Num examples = 9815
2021-08-14 03:09:41,483   Batch size = 32
2021-08-14 03:09:41,484 ***** Eval results *****
2021-08-14 03:09:41,484   att_loss = 1.8400453031198483
2021-08-14 03:09:41,484   cls_loss = 0.0
2021-08-14 03:09:41,484   global_step = 17999
2021-08-14 03:09:41,484   loss = 2.725894397846518
2021-08-14 03:09:41,485   rep_loss = 0.8858490945100242
2021-08-14 03:09:41,485 ***** Save model *****
2021-08-14 03:18:28,608 ***** Running evaluation *****
2021-08-14 03:18:28,608   Epoch = 1 iter 20999 step
2021-08-14 03:18:28,608   Num examples = 9815
2021-08-14 03:18:28,608   Batch size = 32
2021-08-14 03:18:28,660 ***** Eval results *****
2021-08-14 03:18:28,660   att_loss = 1.834106725331706
2021-08-14 03:18:28,661   cls_loss = 0.0
2021-08-14 03:18:28,661   global_step = 20999
2021-08-14 03:18:28,661   loss = 2.717950967659608
2021-08-14 03:18:28,661   rep_loss = 0.8838442426831693
2021-08-14 03:18:28,661 ***** Save model *****
2021-08-14 03:27:19,546 ***** Running evaluation *****
2021-08-14 03:27:19,546   Epoch = 1 iter 23999 step
2021-08-14 03:27:19,546   Num examples = 9815
2021-08-14 03:27:19,546   Batch size = 32
2021-08-14 03:27:19,548 ***** Eval results *****
2021-08-14 03:27:19,548   att_loss = 1.8286882862494118
2021-08-14 03:27:19,548   cls_loss = 0.0
2021-08-14 03:27:19,548   global_step = 23999
2021-08-14 03:27:19,548   loss = 2.7106735652546345
2021-08-14 03:27:19,548   rep_loss = 0.881985279259602
2021-08-14 03:27:19,549 ***** Save model *****
2021-08-14 03:35:48,750 ***** Running evaluation *****
2021-08-14 03:35:48,750   Epoch = 1 iter 26999 step
2021-08-14 03:35:48,750   Num examples = 9815
2021-08-14 03:35:48,750   Batch size = 32
2021-08-14 03:35:48,752 ***** Eval results *****
2021-08-14 03:35:48,752   att_loss = 1.8253285944243647
2021-08-14 03:35:48,752   cls_loss = 0.0
2021-08-14 03:35:48,752   global_step = 26999
2021-08-14 03:35:48,752   loss = 2.705777196328179
2021-08-14 03:35:48,752   rep_loss = 0.8804486024838433
2021-08-14 03:35:48,753 ***** Save model *****
2021-08-14 03:44:22,017 ***** Running evaluation *****
2021-08-14 03:44:22,017   Epoch = 1 iter 29999 step
2021-08-14 03:44:22,017   Num examples = 9815
2021-08-14 03:44:22,017   Batch size = 32
2021-08-14 03:44:22,065 ***** Eval results *****
2021-08-14 03:44:22,065   att_loss = 1.820712124090045
2021-08-14 03:44:22,065   cls_loss = 0.0
2021-08-14 03:44:22,065   global_step = 29999
2021-08-14 03:44:22,065   loss = 2.699574964666491
2021-08-14 03:44:22,065   rep_loss = 0.8788628407107567
2021-08-14 03:44:22,065 ***** Save model *****
2021-08-14 03:52:56,215 ***** Running evaluation *****
2021-08-14 03:52:56,215   Epoch = 2 iter 32999 step
2021-08-14 03:52:56,215   Num examples = 9815
2021-08-14 03:52:56,215   Batch size = 32
2021-08-14 03:52:56,217 ***** Eval results *****
2021-08-14 03:52:56,217   att_loss = 1.7955191855080537
2021-08-14 03:52:56,217   cls_loss = 0.0
2021-08-14 03:52:56,217   global_step = 32999
2021-08-14 03:52:56,217   loss = 2.6651763507322337
2021-08-14 03:52:56,217   rep_loss = 0.8696571626751574
2021-08-14 03:52:56,218 ***** Save model *****
2021-08-14 04:01:35,111 ***** Running evaluation *****
2021-08-14 04:01:35,111   Epoch = 2 iter 35999 step
2021-08-14 04:01:35,111   Num examples = 9815
2021-08-14 04:01:35,111   Batch size = 32
2021-08-14 04:01:35,113 ***** Eval results *****
2021-08-14 04:01:35,113   att_loss = 1.7948438050988964
2021-08-14 04:01:35,113   cls_loss = 0.0
2021-08-14 04:01:35,113   global_step = 35999
2021-08-14 04:01:35,114   loss = 2.663554917322948
2021-08-14 04:01:35,114   rep_loss = 0.8687111120751412
2021-08-14 04:01:35,114 ***** Save model *****
2021-08-14 04:10:28,830 ***** Running evaluation *****
2021-08-14 04:10:28,830   Epoch = 2 iter 38999 step
2021-08-14 04:10:28,830   Num examples = 9815
2021-08-14 04:10:28,831   Batch size = 32
2021-08-14 04:10:28,884 ***** Eval results *****
2021-08-14 04:10:28,884   att_loss = 1.7927796637449944
2021-08-14 04:10:28,884   cls_loss = 0.0
2021-08-14 04:10:28,884   global_step = 38999
2021-08-14 04:10:28,884   loss = 2.6606624061861956
2021-08-14 04:10:28,884   rep_loss = 0.867882742513664
2021-08-14 04:10:28,884 ***** Save model *****
2021-08-14 04:18:55,039 ***** Running evaluation *****
2021-08-14 04:18:55,039   Epoch = 2 iter 41999 step
2021-08-14 04:18:55,039   Num examples = 9815
2021-08-14 04:18:55,039   Batch size = 32
2021-08-14 04:18:55,041 ***** Eval results *****
2021-08-14 04:18:55,041   att_loss = 1.792178067601347
2021-08-14 04:18:55,041   cls_loss = 0.0
2021-08-14 04:18:55,041   global_step = 41999
2021-08-14 04:18:55,041   loss = 2.659250560373363
2021-08-14 04:18:55,041   rep_loss = 0.8670724927892046
2021-08-14 04:18:55,041 ***** Save model *****
2021-08-14 04:27:42,069 ***** Running evaluation *****
2021-08-14 04:27:42,069   Epoch = 2 iter 44999 step
2021-08-14 04:27:42,070   Num examples = 9815
2021-08-14 04:27:42,070   Batch size = 32
2021-08-14 04:27:42,133 ***** Eval results *****
2021-08-14 04:27:42,134   att_loss = 1.791579980703643
2021-08-14 04:27:42,134   cls_loss = 0.0
2021-08-14 04:27:42,134   global_step = 44999
2021-08-14 04:27:42,134   loss = 2.6578805595247244
2021-08-14 04:27:42,134   rep_loss = 0.8663005793236053
2021-08-14 04:27:42,134 ***** Save model *****
2021-08-14 04:36:10,341 ***** Running evaluation *****
2021-08-14 04:36:10,341   Epoch = 3 iter 47999 step
2021-08-14 04:36:10,341   Num examples = 9815
2021-08-14 04:36:10,341   Batch size = 32
2021-08-14 04:36:10,394 ***** Eval results *****
2021-08-14 04:36:10,394   att_loss = 1.7607008167534821
2021-08-14 04:36:10,394   cls_loss = 0.0
2021-08-14 04:36:10,394   global_step = 47999
2021-08-14 04:36:10,394   loss = 2.6210198059554926
2021-08-14 04:36:10,394   rep_loss = 0.8603189896946111
2021-08-14 04:36:10,395 ***** Save model *****
2021-08-14 04:44:42,422 ***** Running evaluation *****
2021-08-14 04:44:42,422   Epoch = 3 iter 50999 step
2021-08-14 04:44:42,423   Num examples = 9815
2021-08-14 04:44:42,423   Batch size = 32
2021-08-14 04:44:42,424 ***** Eval results *****
2021-08-14 04:44:42,424   att_loss = 1.7735242170368253
2021-08-14 04:44:42,424   cls_loss = 0.0
2021-08-14 04:44:42,424   global_step = 50999
2021-08-14 04:44:42,424   loss = 2.6337399099140986
2021-08-14 04:44:42,424   rep_loss = 0.8602156926954008
2021-08-14 04:44:42,425 ***** Save model *****
2021-08-14 04:53:16,923 ***** Running evaluation *****
2021-08-14 04:53:16,923   Epoch = 3 iter 53999 step
2021-08-14 04:53:16,923   Num examples = 9815
2021-08-14 04:53:16,923   Batch size = 32
2021-08-14 04:53:16,925 ***** Eval results *****
2021-08-14 04:53:16,925   att_loss = 1.7758013986259766
2021-08-14 04:53:16,925   cls_loss = 0.0
2021-08-14 04:53:16,925   global_step = 53999
2021-08-14 04:53:16,925   loss = 2.635732626102803
2021-08-14 04:53:16,925   rep_loss = 0.859931226781965
2021-08-14 04:53:16,926 ***** Save model *****
2021-08-14 05:01:55,704 ***** Running evaluation *****
2021-08-14 05:01:55,704   Epoch = 3 iter 56999 step
2021-08-14 05:01:55,704   Num examples = 9815
2021-08-14 05:01:55,704   Batch size = 32
2021-08-14 05:01:55,856 ***** Eval results *****
2021-08-14 05:01:55,856   att_loss = 1.7750749839090667
2021-08-14 05:01:55,856   cls_loss = 0.0
2021-08-14 05:01:55,856   global_step = 56999
2021-08-14 05:01:55,856   loss = 2.6343586136809494
2021-08-14 05:01:55,856   rep_loss = 0.859283629045829
2021-08-14 05:01:55,856 ***** Save model *****
2021-08-14 05:10:34,910 ***** Running evaluation *****
2021-08-14 05:10:34,910   Epoch = 3 iter 59999 step
2021-08-14 05:10:34,910   Num examples = 9815
2021-08-14 05:10:34,910   Batch size = 32
2021-08-14 05:10:34,912 ***** Eval results *****
2021-08-14 05:10:34,912   att_loss = 1.773695625288531
2021-08-14 05:10:34,912   cls_loss = 0.0
2021-08-14 05:10:34,912   global_step = 59999
2021-08-14 05:10:34,912   loss = 2.6324494586110068
2021-08-14 05:10:34,912   rep_loss = 0.858753832589535
2021-08-14 05:10:34,913 ***** Save model *****
2021-08-14 05:19:29,706 ***** Running evaluation *****
2021-08-14 05:19:29,707   Epoch = 3 iter 62999 step
2021-08-14 05:19:29,707   Num examples = 9815
2021-08-14 05:19:29,707   Batch size = 32
2021-08-14 05:19:29,708 ***** Eval results *****
2021-08-14 05:19:29,708   att_loss = 1.7729092855859896
2021-08-14 05:19:29,708   cls_loss = 0.0
2021-08-14 05:19:29,708   global_step = 62999
2021-08-14 05:19:29,708   loss = 2.6311258229182646
2021-08-14 05:19:29,708   rep_loss = 0.8582165366905845
2021-08-14 05:19:29,709 ***** Save model *****
2021-08-14 05:28:02,113 ***** Running evaluation *****
2021-08-14 05:28:02,113   Epoch = 4 iter 65999 step
2021-08-14 05:28:02,114   Num examples = 9815
2021-08-14 05:28:02,114   Batch size = 32
2021-08-14 05:28:02,166 ***** Eval results *****
2021-08-14 05:28:02,167   att_loss = 1.7593548717471599
2021-08-14 05:28:02,167   cls_loss = 0.0
2021-08-14 05:28:02,167   global_step = 65999
2021-08-14 05:28:02,167   loss = 2.6136539625365582
2021-08-14 05:28:02,167   rep_loss = 0.8542990901948381
2021-08-14 05:28:02,167 ***** Save model *****
2021-08-14 05:36:37,974 ***** Running evaluation *****
2021-08-14 05:36:37,975   Epoch = 4 iter 68999 step
2021-08-14 05:36:37,975   Num examples = 9815
2021-08-14 05:36:37,975   Batch size = 32
2021-08-14 05:36:38,039 ***** Eval results *****
2021-08-14 05:36:38,039   att_loss = 1.7605229101924984
2021-08-14 05:36:38,039   cls_loss = 0.0
2021-08-14 05:36:38,039   global_step = 68999
2021-08-14 05:36:38,039   loss = 2.614625482586302
2021-08-14 05:36:38,039   rep_loss = 0.854102572157725
2021-08-14 05:36:38,039 ***** Save model *****
2021-08-14 05:45:15,235 ***** Running evaluation *****
2021-08-14 05:45:15,235   Epoch = 4 iter 71999 step
2021-08-14 05:45:15,235   Num examples = 9815
2021-08-14 05:45:15,236   Batch size = 32
2021-08-14 05:45:15,237 ***** Eval results *****
2021-08-14 05:45:15,237   att_loss = 1.762975107057419
2021-08-14 05:45:15,237   cls_loss = 0.0
2021-08-14 05:45:15,237   global_step = 71999
2021-08-14 05:45:15,237   loss = 2.616975652098155
2021-08-14 05:45:15,237   rep_loss = 0.8540005449459855
2021-08-14 05:45:15,237 ***** Save model *****
2021-08-14 05:53:59,330 ***** Running evaluation *****
2021-08-14 05:53:59,330   Epoch = 4 iter 74999 step
2021-08-14 05:53:59,330   Num examples = 9815
2021-08-14 05:53:59,330   Batch size = 32
2021-08-14 05:53:59,383 ***** Eval results *****
2021-08-14 05:53:59,383   att_loss = 1.7611886801339633
2021-08-14 05:53:59,384   cls_loss = 0.0
2021-08-14 05:53:59,384   global_step = 74999
2021-08-14 05:53:59,384   loss = 2.614796390306882
2021-08-14 05:53:59,384   rep_loss = 0.853607710324366
2021-08-14 05:53:59,384 ***** Save model *****
2021-08-14 06:02:39,150 ***** Running evaluation *****
2021-08-14 06:02:39,150   Epoch = 4 iter 77999 step
2021-08-14 06:02:39,150   Num examples = 9815
2021-08-14 06:02:39,150   Batch size = 32
2021-08-14 06:02:39,152 ***** Eval results *****
2021-08-14 06:02:39,152   att_loss = 1.7610684324342973
2021-08-14 06:02:39,152   cls_loss = 0.0
2021-08-14 06:02:39,152   global_step = 77999
2021-08-14 06:02:39,152   loss = 2.6143347568673625
2021-08-14 06:02:39,152   rep_loss = 0.8532663248758631
2021-08-14 06:02:39,152 ***** Save model *****
2021-08-14 06:11:23,000 ***** Running evaluation *****
2021-08-14 06:11:23,000   Epoch = 5 iter 80999 step
2021-08-14 06:11:23,000   Num examples = 9815
2021-08-14 06:11:23,001   Batch size = 32
2021-08-14 06:11:23,002 ***** Eval results *****
2021-08-14 06:11:23,002   att_loss = 1.7537161610861096
2021-08-14 06:11:23,002   cls_loss = 0.0
2021-08-14 06:11:23,002   global_step = 80999
2021-08-14 06:11:23,002   loss = 2.604166459301445
2021-08-14 06:11:23,003   rep_loss = 0.8504503013898983
2021-08-14 06:11:23,003 ***** Save model *****
2021-08-14 06:19:58,729 ***** Running evaluation *****
2021-08-14 06:19:58,729   Epoch = 5 iter 83999 step
2021-08-14 06:19:58,729   Num examples = 9815
2021-08-14 06:19:58,729   Batch size = 32
2021-08-14 06:19:58,779 ***** Eval results *****
2021-08-14 06:19:58,779   att_loss = 1.754225248085284
2021-08-14 06:19:58,779   cls_loss = 0.0
2021-08-14 06:19:58,779   global_step = 83999
2021-08-14 06:19:58,779   loss = 2.6043886699988756
2021-08-14 06:19:58,779   rep_loss = 0.8501634224371701
2021-08-14 06:19:58,780 ***** Save model *****
2021-08-14 06:28:31,897 ***** Running evaluation *****
2021-08-14 06:28:31,897   Epoch = 5 iter 86999 step
2021-08-14 06:28:31,898   Num examples = 9815
2021-08-14 06:28:31,898   Batch size = 32
2021-08-14 06:28:31,899 ***** Eval results *****
2021-08-14 06:28:31,899   att_loss = 1.7537281953048611
2021-08-14 06:28:31,899   cls_loss = 0.0
2021-08-14 06:28:31,899   global_step = 86999
2021-08-14 06:28:31,899   loss = 2.6036077203755372
2021-08-14 06:28:31,899   rep_loss = 0.8498795252716159
2021-08-14 06:28:31,900 ***** Save model *****
2021-08-14 06:37:12,786 ***** Running evaluation *****
2021-08-14 06:37:12,787   Epoch = 5 iter 89999 step
2021-08-14 06:37:12,787   Num examples = 9815
2021-08-14 06:37:12,787   Batch size = 32
2021-08-14 06:37:12,789 ***** Eval results *****
2021-08-14 06:37:12,789   att_loss = 1.7546230365517723
2021-08-14 06:37:12,789   cls_loss = 0.0
2021-08-14 06:37:12,789   global_step = 89999
2021-08-14 06:37:12,789   loss = 2.60428656927733
2021-08-14 06:37:12,789   rep_loss = 0.8496635333157028
2021-08-14 06:37:12,789 ***** Save model *****
2021-08-14 06:46:07,960 ***** Running evaluation *****
2021-08-14 06:46:07,960   Epoch = 5 iter 92999 step
2021-08-14 06:46:07,960   Num examples = 9815
2021-08-14 06:46:07,960   Batch size = 32
2021-08-14 06:46:08,135 ***** Eval results *****
2021-08-14 06:46:08,135   att_loss = 1.7525974913577433
2021-08-14 06:46:08,135   cls_loss = 0.0
2021-08-14 06:46:08,135   global_step = 92999
2021-08-14 06:46:08,135   loss = 2.601853457080262
2021-08-14 06:46:08,135   rep_loss = 0.8492559662033041
2021-08-14 06:46:08,136 ***** Save model *****
2021-08-14 06:54:33,822 ***** Running evaluation *****
2021-08-14 06:54:33,822   Epoch = 6 iter 95999 step
2021-08-14 06:54:33,822   Num examples = 9815
2021-08-14 06:54:33,822   Batch size = 32
2021-08-14 06:54:33,823 ***** Eval results *****
2021-08-14 06:54:33,823   att_loss = 1.7341926848957305
2021-08-14 06:54:33,824   cls_loss = 0.0
2021-08-14 06:54:33,824   global_step = 95999
2021-08-14 06:54:33,824   loss = 2.5799468735049915
2021-08-14 06:54:33,824   rep_loss = 0.8457541834904476
2021-08-14 06:54:33,824 ***** Save model *****
2021-08-14 07:03:12,524 ***** Running evaluation *****
2021-08-14 07:03:12,525   Epoch = 6 iter 98999 step
2021-08-14 07:03:12,525   Num examples = 9815
2021-08-14 07:03:12,525   Batch size = 32
2021-08-14 07:03:12,527 ***** Eval results *****
2021-08-14 07:03:12,527   att_loss = 1.744382220955366
2021-08-14 07:03:12,527   cls_loss = 0.0
2021-08-14 07:03:12,527   global_step = 98999
2021-08-14 07:03:12,527   loss = 2.590935880796441
2021-08-14 07:03:12,527   rep_loss = 0.8465536590059589
2021-08-14 07:03:12,528 ***** Save model *****
2021-08-14 07:12:04,902 ***** Running evaluation *****
2021-08-14 07:12:04,902   Epoch = 6 iter 101999 step
2021-08-14 07:12:04,902   Num examples = 9815
2021-08-14 07:12:04,902   Batch size = 32
2021-08-14 07:12:04,954 ***** Eval results *****
2021-08-14 07:12:04,954   att_loss = 1.7463634318972143
2021-08-14 07:12:04,955   cls_loss = 0.0
2021-08-14 07:12:04,955   global_step = 101999
2021-08-14 07:12:04,955   loss = 2.5929396991309175
2021-08-14 07:12:04,955   rep_loss = 0.8465762669609317
2021-08-14 07:12:04,955 ***** Save model *****
2021-08-14 07:20:28,139 ***** Running evaluation *****
2021-08-14 07:20:28,139   Epoch = 6 iter 104999 step
2021-08-14 07:20:28,139   Num examples = 9815
2021-08-14 07:20:28,139   Batch size = 32
2021-08-14 07:20:28,141 ***** Eval results *****
2021-08-14 07:20:28,141   att_loss = 1.7461483429746336
2021-08-14 07:20:28,141   cls_loss = 0.0
2021-08-14 07:20:28,141   global_step = 104999
2021-08-14 07:20:28,141   loss = 2.592579625201802
2021-08-14 07:20:28,141   rep_loss = 0.8464312824664977
2021-08-14 07:20:28,142 ***** Save model *****
2021-08-14 07:29:04,299 ***** Running evaluation *****
2021-08-14 07:29:04,300   Epoch = 6 iter 107999 step
2021-08-14 07:29:04,300   Num examples = 9815
2021-08-14 07:29:04,300   Batch size = 32
2021-08-14 07:29:04,301 ***** Eval results *****
2021-08-14 07:29:04,301   att_loss = 1.7450394724817806
2021-08-14 07:29:04,301   cls_loss = 0.0
2021-08-14 07:29:04,301   global_step = 107999
2021-08-14 07:29:04,301   loss = 2.5912691158400505
2021-08-14 07:29:04,301   rep_loss = 0.8462296434033875
2021-08-14 07:29:04,301 ***** Save model *****
2021-08-14 07:37:44,655 ***** Running evaluation *****
2021-08-14 07:37:44,655   Epoch = 7 iter 110999 step
2021-08-14 07:37:44,655   Num examples = 9815
2021-08-14 07:37:44,655   Batch size = 32
2021-08-14 07:37:44,708 ***** Eval results *****
2021-08-14 07:37:44,708   att_loss = 1.7377843178502006
2021-08-14 07:37:44,708   cls_loss = 0.0
2021-08-14 07:37:44,708   global_step = 110999
2021-08-14 07:37:44,708   loss = 2.5812525726115156
2021-08-14 07:37:44,708   rep_loss = 0.843468255771563
2021-08-14 07:37:44,708 ***** Save model *****
2021-08-14 07:46:21,329 ***** Running evaluation *****
2021-08-14 07:46:21,330   Epoch = 7 iter 113999 step
2021-08-14 07:46:21,330   Num examples = 9815
2021-08-14 07:46:21,330   Batch size = 32
2021-08-14 07:46:21,331 ***** Eval results *****
2021-08-14 07:46:21,331   att_loss = 1.7344325086849022
2021-08-14 07:46:21,331   cls_loss = 0.0
2021-08-14 07:46:21,331   global_step = 113999
2021-08-14 07:46:21,331   loss = 2.5780067471091503
2021-08-14 07:46:21,331   rep_loss = 0.8435742386163521
2021-08-14 07:46:21,332 ***** Save model *****
2021-08-14 07:55:15,502 ***** Running evaluation *****
2021-08-14 07:55:15,503   Epoch = 7 iter 116999 step
2021-08-14 07:55:15,503   Num examples = 9815
2021-08-14 07:55:15,503   Batch size = 32
2021-08-14 07:55:15,634 ***** Eval results *****
2021-08-14 07:55:15,634   att_loss = 1.7361563083516924
2021-08-14 07:55:15,634   cls_loss = 0.0
2021-08-14 07:55:15,634   global_step = 116999
2021-08-14 07:55:15,634   loss = 2.5797520160637872
2021-08-14 07:55:15,634   rep_loss = 0.8435957070336073
2021-08-14 07:55:15,634 ***** Save model *****
2021-08-14 08:03:39,850 ***** Running evaluation *****
2021-08-14 08:03:39,850   Epoch = 7 iter 119999 step
2021-08-14 08:03:39,850   Num examples = 9815
2021-08-14 08:03:39,850   Batch size = 32
2021-08-14 08:03:39,904 ***** Eval results *****
2021-08-14 08:03:39,904   att_loss = 1.7366149782260623
2021-08-14 08:03:39,904   cls_loss = 0.0
2021-08-14 08:03:39,904   global_step = 119999
2021-08-14 08:03:39,904   loss = 2.5801748331530523
2021-08-14 08:03:39,904   rep_loss = 0.8435598545027352
2021-08-14 08:03:39,904 ***** Save model *****
2021-08-14 08:12:22,592 ***** Running evaluation *****
2021-08-14 08:12:22,592   Epoch = 7 iter 122999 step
2021-08-14 08:12:22,592   Num examples = 9815
2021-08-14 08:12:22,592   Batch size = 32
2021-08-14 08:12:22,594 ***** Eval results *****
2021-08-14 08:12:22,594   att_loss = 1.7378044477700323
2021-08-14 08:12:22,594   cls_loss = 0.0
2021-08-14 08:12:22,594   global_step = 122999
2021-08-14 08:12:22,594   loss = 2.581329832239903
2021-08-14 08:12:22,595   rep_loss = 0.8435253842537902
2021-08-14 08:12:22,595 ***** Save model *****
2021-08-14 08:20:47,545 ***** Running evaluation *****
2021-08-14 08:20:47,546   Epoch = 7 iter 125999 step
2021-08-14 08:20:47,546   Num examples = 9815
2021-08-14 08:20:47,546   Batch size = 32
2021-08-14 08:20:47,548 ***** Eval results *****
2021-08-14 08:20:47,548   att_loss = 1.738451825012403
2021-08-14 08:20:47,548   cls_loss = 0.0
2021-08-14 08:20:47,548   global_step = 125999
2021-08-14 08:20:47,548   loss = 2.5818768071299174
2021-08-14 08:20:47,548   rep_loss = 0.8434249820092338
2021-08-14 08:20:47,549 ***** Save model *****
2021-08-14 08:29:18,555 ***** Running evaluation *****
2021-08-14 08:29:18,555   Epoch = 8 iter 128999 step
2021-08-14 08:29:18,555   Num examples = 9815
2021-08-14 08:29:18,555   Batch size = 32
2021-08-14 08:29:18,606 ***** Eval results *****
2021-08-14 08:29:18,607   att_loss = 1.733429422615591
2021-08-14 08:29:18,607   cls_loss = 0.0
2021-08-14 08:29:18,607   global_step = 128999
2021-08-14 08:29:18,607   loss = 2.5752523081252496
2021-08-14 08:29:18,607   rep_loss = 0.8418228850082048
2021-08-14 08:29:18,607 ***** Save model *****
2021-08-14 08:37:47,860 ***** Running evaluation *****
2021-08-14 08:37:47,861   Epoch = 8 iter 131999 step
2021-08-14 08:37:47,861   Num examples = 9815
2021-08-14 08:37:47,861   Batch size = 32
2021-08-14 08:37:47,862 ***** Eval results *****
2021-08-14 08:37:47,862   att_loss = 1.7334902648085053
2021-08-14 08:37:47,863   cls_loss = 0.0
2021-08-14 08:37:47,863   global_step = 131999
2021-08-14 08:37:47,863   loss = 2.575274618204852
2021-08-14 08:37:47,863   rep_loss = 0.8417843536723435
2021-08-14 08:37:47,863 ***** Save model *****
2021-08-14 08:46:30,287 ***** Running evaluation *****
2021-08-14 08:46:30,288   Epoch = 8 iter 134999 step
2021-08-14 08:46:30,288   Num examples = 9815
2021-08-14 08:46:30,288   Batch size = 32
2021-08-14 08:46:30,289 ***** Eval results *****
2021-08-14 08:46:30,289   att_loss = 1.7327735457666655
2021-08-14 08:46:30,289   cls_loss = 0.0
2021-08-14 08:46:30,289   global_step = 134999
2021-08-14 08:46:30,289   loss = 2.574303555986733
2021-08-14 08:46:30,289   rep_loss = 0.8415300104898972
2021-08-14 08:46:30,290 ***** Save model *****
2021-08-14 08:55:22,556 ***** Running evaluation *****
2021-08-14 08:55:22,556   Epoch = 8 iter 137999 step
2021-08-14 08:55:22,556   Num examples = 9815
2021-08-14 08:55:22,556   Batch size = 32
2021-08-14 08:55:22,609 ***** Eval results *****
2021-08-14 08:55:22,609   att_loss = 1.7325923157210397
2021-08-14 08:55:22,609   cls_loss = 0.0
2021-08-14 08:55:22,609   global_step = 137999
2021-08-14 08:55:22,609   loss = 2.573993566022882
2021-08-14 08:55:22,609   rep_loss = 0.8414012505122418
2021-08-14 08:55:22,610 ***** Save model *****
2021-08-14 09:03:55,633 ***** Running evaluation *****
2021-08-14 09:03:55,633   Epoch = 8 iter 140999 step
2021-08-14 09:03:55,634   Num examples = 9815
2021-08-14 09:03:55,634   Batch size = 32
2021-08-14 09:03:55,764 ***** Eval results *****
2021-08-14 09:03:55,764   att_loss = 1.7330465477376698
2021-08-14 09:03:55,764   cls_loss = 0.0
2021-08-14 09:03:55,764   global_step = 140999
2021-08-14 09:03:55,764   loss = 2.574382501312449
2021-08-14 09:03:55,764   rep_loss = 0.8413359535829359
2021-08-14 09:03:55,765 ***** Save model *****
2021-08-14 09:12:24,594 ***** Running evaluation *****
2021-08-14 09:12:24,595   Epoch = 9 iter 143999 step
2021-08-14 09:12:24,595   Num examples = 9815
2021-08-14 09:12:24,595   Batch size = 32
2021-08-14 09:12:24,597 ***** Eval results *****
2021-08-14 09:12:24,597   att_loss = 1.7291454576698055
2021-08-14 09:12:24,597   cls_loss = 0.0
2021-08-14 09:12:24,597   global_step = 143999
2021-08-14 09:12:24,597   loss = 2.568727015005604
2021-08-14 09:12:24,597   rep_loss = 0.8395815564828977
2021-08-14 09:12:24,597 ***** Save model *****
2021-08-14 09:21:07,541 ***** Running evaluation *****
2021-08-14 09:21:07,542   Epoch = 9 iter 146999 step
2021-08-14 09:21:07,542   Num examples = 9815
2021-08-14 09:21:07,542   Batch size = 32
2021-08-14 09:21:07,598 ***** Eval results *****
2021-08-14 09:21:07,598   att_loss = 1.7280435814312045
2021-08-14 09:21:07,598   cls_loss = 0.0
2021-08-14 09:21:07,598   global_step = 146999
2021-08-14 09:21:07,598   loss = 2.5678844840441943
2021-08-14 09:21:07,598   rep_loss = 0.8398409010662635
2021-08-14 09:21:07,598 ***** Save model *****
2021-08-14 09:29:36,839 ***** Running evaluation *****
2021-08-14 09:29:36,840   Epoch = 9 iter 149999 step
2021-08-14 09:29:36,840   Num examples = 9815
2021-08-14 09:29:36,840   Batch size = 32
2021-08-14 09:29:36,841 ***** Eval results *****
2021-08-14 09:29:36,841   att_loss = 1.7276233501472147
2021-08-14 09:29:36,841   cls_loss = 0.0
2021-08-14 09:29:36,841   global_step = 149999
2021-08-14 09:29:36,841   loss = 2.5672846004239154
2021-08-14 09:29:36,841   rep_loss = 0.8396612498649508
2021-08-14 09:29:36,842 ***** Save model *****
2021-08-14 09:38:07,739 ***** Running evaluation *****
2021-08-14 09:38:07,740   Epoch = 9 iter 152999 step
2021-08-14 09:38:07,740   Num examples = 9815
2021-08-14 09:38:07,740   Batch size = 32
2021-08-14 09:38:07,741 ***** Eval results *****
2021-08-14 09:38:07,741   att_loss = 1.7294965321314337
2021-08-14 09:38:07,741   cls_loss = 0.0
2021-08-14 09:38:07,742   global_step = 152999
2021-08-14 09:38:07,742   loss = 2.5691954881060286
2021-08-14 09:38:07,742   rep_loss = 0.8396989556054063
2021-08-14 09:38:07,742 ***** Save model *****
2021-08-14 09:46:58,178 ***** Running evaluation *****
2021-08-14 09:46:58,178   Epoch = 9 iter 155999 step
2021-08-14 09:46:58,178   Num examples = 9815
2021-08-14 09:46:58,178   Batch size = 32
2021-08-14 09:46:58,228 ***** Eval results *****
2021-08-14 09:46:58,228   att_loss = 1.729627429002479
2021-08-14 09:46:58,228   cls_loss = 0.0
2021-08-14 09:46:58,228   global_step = 155999
2021-08-14 09:46:58,228   loss = 2.569230086190695
2021-08-14 09:46:58,228   rep_loss = 0.8396026567050635
2021-08-14 09:46:58,229 ***** Save model *****
2021-08-14 10:02:41,800 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=300, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', pred_distill=True, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 10:02:41,973 device: cuda n_gpu: 4
2021-08-14 10:02:52,195 Writing example 0 of 505555
2021-08-14 10:02:52,198 *** Example ***
2021-08-14 10:02:52,198 guid: aug-0
2021-08-14 10:02:52,198 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 10:02:52,198 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:02:52,198 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:02:52,198 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:02:52,198 label: neutral
2021-08-14 10:02:52,198 label_id: 1
2021-08-14 10:02:56,852 Writing example 10000 of 505555
2021-08-14 10:03:01,494 Writing example 20000 of 505555
2021-08-14 10:03:05,977 Writing example 30000 of 505555
2021-08-14 10:03:10,676 Writing example 40000 of 505555
2021-08-14 10:03:15,451 Writing example 50000 of 505555
2021-08-14 10:03:20,156 Writing example 60000 of 505555
2021-08-14 10:03:24,797 Writing example 70000 of 505555
2021-08-14 10:03:29,494 Writing example 80000 of 505555
2021-08-14 10:03:34,441 Writing example 90000 of 505555
2021-08-14 10:03:39,039 Writing example 100000 of 505555
2021-08-14 10:03:43,581 Writing example 110000 of 505555
2021-08-14 10:03:48,039 Writing example 120000 of 505555
2021-08-14 10:03:52,812 Writing example 130000 of 505555
2021-08-14 10:03:58,075 Writing example 140000 of 505555
2021-08-14 10:04:02,451 Writing example 150000 of 505555
2021-08-14 10:04:07,187 Writing example 160000 of 505555
2021-08-14 10:04:11,981 Writing example 170000 of 505555
2021-08-14 10:04:16,663 Writing example 180000 of 505555
2021-08-14 10:04:21,251 Writing example 190000 of 505555
2021-08-14 10:04:25,777 Writing example 200000 of 505555
2021-08-14 10:04:31,162 Writing example 210000 of 505555
2021-08-14 10:04:35,581 Writing example 220000 of 505555
2021-08-14 10:04:40,217 Writing example 230000 of 505555
2021-08-14 10:04:44,910 Writing example 240000 of 505555
2021-08-14 10:04:49,526 Writing example 250000 of 505555
2021-08-14 10:04:54,050 Writing example 260000 of 505555
2021-08-14 10:04:58,529 Writing example 270000 of 505555
2021-08-14 10:05:02,970 Writing example 280000 of 505555
2021-08-14 10:05:08,862 Writing example 290000 of 505555
2021-08-14 10:05:13,644 Writing example 300000 of 505555
2021-08-14 10:05:18,152 Writing example 310000 of 505555
2021-08-14 10:05:22,522 Writing example 320000 of 505555
2021-08-14 10:05:27,156 Writing example 330000 of 505555
2021-08-14 10:05:31,547 Writing example 340000 of 505555
2021-08-14 10:05:36,011 Writing example 350000 of 505555
2021-08-14 10:05:40,454 Writing example 360000 of 505555
2021-08-14 10:05:44,809 Writing example 370000 of 505555
2021-08-14 10:05:49,351 Writing example 380000 of 505555
2021-08-14 10:05:55,234 Writing example 390000 of 505555
2021-08-14 10:05:59,802 Writing example 400000 of 505555
2021-08-14 10:06:04,310 Writing example 410000 of 505555
2021-08-14 10:06:08,710 Writing example 420000 of 505555
2021-08-14 10:06:13,051 Writing example 430000 of 505555
2021-08-14 10:06:17,431 Writing example 440000 of 505555
2021-08-14 10:06:22,054 Writing example 450000 of 505555
2021-08-14 10:06:26,808 Writing example 460000 of 505555
2021-08-14 10:06:31,336 Writing example 470000 of 505555
2021-08-14 10:06:36,067 Writing example 480000 of 505555
2021-08-14 10:06:40,602 Writing example 490000 of 505555
2021-08-14 10:06:45,108 Writing example 500000 of 505555
2021-08-14 10:06:52,910 Writing example 0 of 9815
2021-08-14 10:06:52,911 *** Example ***
2021-08-14 10:06:52,911 guid: dev_matched-0
2021-08-14 10:06:52,911 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 10:06:52,911 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:06:52,911 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:06:52,911 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:06:52,911 label: neutral
2021-08-14 10:06:52,911 label_id: 1
2021-08-14 10:06:57,312 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 10:06:59,794 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 10:07:29,486 loading model...
2021-08-14 10:07:29,521 done!
2021-08-14 10:07:29,521 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 10:07:29,521 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 10:07:37,954 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-14 10:07:39,441 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate/pytorch_model.bin
2021-08-14 10:07:49,126 loading model...
2021-08-14 10:07:49,141 done!
2021-08-14 10:07:49,209 ***** Running training *****
2021-08-14 10:07:49,209   Num examples = 505555
2021-08-14 10:07:49,209   Batch size = 32
2021-08-14 10:07:49,210   Num steps = 47394
2021-08-14 10:07:49,211 n: module.bert.embeddings.word_embeddings.weight
2021-08-14 10:07:49,211 n: module.bert.embeddings.position_embeddings.weight
2021-08-14 10:07:49,211 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-14 10:07:49,211 n: module.bert.embeddings.LayerNorm.weight
2021-08-14 10:07:49,211 n: module.bert.embeddings.LayerNorm.bias
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-14 10:07:49,211 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-14 10:07:49,212 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-14 10:07:49,213 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-14 10:07:49,214 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-14 10:07:49,215 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-14 10:07:49,216 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-14 10:07:49,217 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-14 10:07:49,217 n: module.bert.pooler.dense.weight
2021-08-14 10:07:49,217 n: module.bert.pooler.dense.bias
2021-08-14 10:07:49,217 n: module.classifier.weight
2021-08-14 10:07:49,217 n: module.classifier.bias
2021-08-14 10:07:49,217 n: module.fit_dense.weight
2021-08-14 10:07:49,217 n: module.fit_dense.bias
2021-08-14 10:07:49,217 Total parameters: 67547907
2021-08-14 10:08:45,975 ***** Running evaluation *****
2021-08-14 10:08:45,976   Epoch = 0 iter 299 step
2021-08-14 10:08:45,976   Num examples = 9815
2021-08-14 10:08:45,976   Batch size = 32
2021-08-14 10:08:57,840 ***** Eval results *****
2021-08-14 10:08:57,841   acc = 0.8146714212939379
2021-08-14 10:08:57,841   att_loss = 0.0
2021-08-14 10:08:57,841   cls_loss = 0.2946193078190188
2021-08-14 10:08:57,841   eval_loss = 0.577503136392525
2021-08-14 10:08:57,841   global_step = 299
2021-08-14 10:08:57,841   loss = 0.2946193078190188
2021-08-14 10:08:57,841   rep_loss = 0.0
2021-08-14 10:08:57,842 ***** Save model *****
2021-08-14 10:09:00,562 Writing example 0 of 9832
2021-08-14 10:09:00,562 *** Example ***
2021-08-14 10:09:00,563 guid: dev_matched-0
2021-08-14 10:09:00,563 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 10:09:00,563 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:09:00,563 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:09:00,563 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:09:00,563 label: contradiction
2021-08-14 10:09:00,563 label_id: 2
2021-08-14 10:09:05,101 ***** Running mm evaluation *****
2021-08-14 10:09:05,102   Num examples = 9832
2021-08-14 10:09:05,102   Batch size = 32
2021-08-14 10:09:14,839 ***** Eval results *****
2021-08-14 10:09:14,839   acc = 0.8279088689991864
2021-08-14 10:09:14,839   eval_loss = 0.5598793318132301
2021-08-14 10:09:14,839   global_step = 299
2021-08-14 10:10:05,471 ***** Running evaluation *****
2021-08-14 10:10:05,471   Epoch = 0 iter 599 step
2021-08-14 10:10:05,471   Num examples = 9832
2021-08-14 10:10:05,471   Batch size = 32
2021-08-14 10:10:17,122 ***** Eval results *****
2021-08-14 10:10:17,122   acc = 0.8323840520748577
2021-08-14 10:10:17,122   att_loss = 0.0
2021-08-14 10:10:17,123   cls_loss = 0.2558815186976988
2021-08-14 10:10:17,123   eval_loss = 0.4787452957266337
2021-08-14 10:10:17,123   global_step = 599
2021-08-14 10:10:17,123   loss = 0.2558815186976988
2021-08-14 10:10:17,123   rep_loss = 0.0
2021-08-14 10:10:17,123 ***** Save model *****
2021-08-14 10:10:18,139 Writing example 0 of 9832
2021-08-14 10:10:18,139 *** Example ***
2021-08-14 10:10:18,140 guid: dev_matched-0
2021-08-14 10:10:18,140 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 10:10:18,140 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:10:18,140 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:10:18,140 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:10:18,140 label: contradiction
2021-08-14 10:10:18,140 label_id: 2
2021-08-14 10:10:22,704 ***** Running mm evaluation *****
2021-08-14 10:10:22,704   Num examples = 9832
2021-08-14 10:10:22,704   Batch size = 32
2021-08-14 10:10:32,412 ***** Eval results *****
2021-08-14 10:10:32,412   acc = 0.8323840520748577
2021-08-14 10:10:32,412   eval_loss = 0.4787452957266337
2021-08-14 10:10:32,412   global_step = 599
2021-08-14 10:11:21,068 ***** Running evaluation *****
2021-08-14 10:11:21,068   Epoch = 0 iter 899 step
2021-08-14 10:11:21,068   Num examples = 9832
2021-08-14 10:11:21,068   Batch size = 32
2021-08-14 10:11:30,714 ***** Eval results *****
2021-08-14 10:11:30,714   acc = 0.8258746948738812
2021-08-14 10:11:30,714   att_loss = 0.0
2021-08-14 10:11:30,714   cls_loss = 0.24184817592877567
2021-08-14 10:11:30,714   eval_loss = 0.47460660023929235
2021-08-14 10:11:30,714   global_step = 899
2021-08-14 10:11:30,714   loss = 0.24184817592877567
2021-08-14 10:11:30,714   rep_loss = 0.0
2021-08-14 10:12:19,090 ***** Running evaluation *****
2021-08-14 10:12:19,090   Epoch = 0 iter 1199 step
2021-08-14 10:12:19,091   Num examples = 9832
2021-08-14 10:12:19,091   Batch size = 32
2021-08-14 10:12:28,728 ***** Eval results *****
2021-08-14 10:12:28,728   acc = 0.8347233523189586
2021-08-14 10:12:28,728   att_loss = 0.0
2021-08-14 10:12:28,728   cls_loss = 0.23469620560287335
2021-08-14 10:12:28,728   eval_loss = 0.45328823757636083
2021-08-14 10:12:28,729   global_step = 1199
2021-08-14 10:12:28,729   loss = 0.23469620560287335
2021-08-14 10:12:28,729   rep_loss = 0.0
2021-08-14 10:12:28,729 ***** Save model *****
2021-08-14 10:12:29,841 Writing example 0 of 9832
2021-08-14 10:12:29,842 *** Example ***
2021-08-14 10:12:29,842 guid: dev_matched-0
2021-08-14 10:12:29,842 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 10:12:29,842 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:12:29,842 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:12:29,842 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:12:29,842 label: contradiction
2021-08-14 10:12:29,842 label_id: 2
2021-08-14 10:12:34,387 ***** Running mm evaluation *****
2021-08-14 10:12:34,387   Num examples = 9832
2021-08-14 10:12:34,387   Batch size = 32
2021-08-14 10:12:44,042 ***** Eval results *****
2021-08-14 10:12:44,042   acc = 0.8347233523189586
2021-08-14 10:12:44,042   eval_loss = 0.45328823757636083
2021-08-14 10:12:44,042   global_step = 1199
2021-08-14 10:13:34,276 ***** Running evaluation *****
2021-08-14 10:13:34,276   Epoch = 0 iter 1499 step
2021-08-14 10:13:34,276   Num examples = 9832
2021-08-14 10:13:34,277   Batch size = 32
2021-08-14 10:13:43,938 ***** Eval results *****
2021-08-14 10:13:43,938   acc = 0.8300447518307568
2021-08-14 10:13:43,938   att_loss = 0.0
2021-08-14 10:13:43,938   cls_loss = 0.2303708052042725
2021-08-14 10:13:43,938   eval_loss = 0.4581037528038799
2021-08-14 10:13:43,938   global_step = 1499
2021-08-14 10:13:43,938   loss = 0.2303708052042725
2021-08-14 10:13:43,938   rep_loss = 0.0
2021-08-14 10:14:32,664 ***** Running evaluation *****
2021-08-14 10:14:32,664   Epoch = 0 iter 1799 step
2021-08-14 10:14:32,664   Num examples = 9832
2021-08-14 10:14:32,665   Batch size = 32
2021-08-14 10:14:42,342 ***** Eval results *****
2021-08-14 10:14:42,342   acc = 0.8322823433685923
2021-08-14 10:14:42,342   att_loss = 0.0
2021-08-14 10:14:42,342   cls_loss = 0.2275168490930025
2021-08-14 10:14:42,343   eval_loss = 0.4514339811035565
2021-08-14 10:14:42,343   global_step = 1799
2021-08-14 10:14:42,343   loss = 0.2275168490930025
2021-08-14 10:14:42,343   rep_loss = 0.0
2021-08-14 10:15:30,774 ***** Running evaluation *****
2021-08-14 10:15:30,774   Epoch = 0 iter 2099 step
2021-08-14 10:15:30,774   Num examples = 9832
2021-08-14 10:15:30,774   Batch size = 32
2021-08-14 10:15:42,296 ***** Eval results *****
2021-08-14 10:15:42,296   acc = 0.8259764035801465
2021-08-14 10:15:42,296   att_loss = 0.0
2021-08-14 10:15:42,296   cls_loss = 0.22495951604678438
2021-08-14 10:15:42,296   eval_loss = 0.46509995969471996
2021-08-14 10:15:42,297   global_step = 2099
2021-08-14 10:15:42,297   loss = 0.22495951604678438
2021-08-14 10:15:42,297   rep_loss = 0.0
2021-08-14 10:16:30,620 ***** Running evaluation *****
2021-08-14 10:16:30,621   Epoch = 0 iter 2399 step
2021-08-14 10:16:30,621   Num examples = 9832
2021-08-14 10:16:30,621   Batch size = 32
2021-08-14 10:16:41,894 ***** Eval results *****
2021-08-14 10:16:41,894   acc = 0.8247558991049634
2021-08-14 10:16:41,894   att_loss = 0.0
2021-08-14 10:16:41,894   cls_loss = 0.22318468400963945
2021-08-14 10:16:41,894   eval_loss = 0.45186107233166695
2021-08-14 10:16:41,895   global_step = 2399
2021-08-14 10:16:41,895   loss = 0.22318468400963945
2021-08-14 10:16:41,895   rep_loss = 0.0
2021-08-14 10:17:28,492 ***** Running evaluation *****
2021-08-14 10:17:28,492   Epoch = 0 iter 2699 step
2021-08-14 10:17:28,492   Num examples = 9832
2021-08-14 10:17:28,492   Batch size = 32
2021-08-14 10:17:40,051 ***** Eval results *****
2021-08-14 10:17:40,051   acc = 0.8268917819365338
2021-08-14 10:17:40,051   att_loss = 0.0
2021-08-14 10:17:40,051   cls_loss = 0.2220466040591038
2021-08-14 10:17:40,051   eval_loss = 0.44629728315132006
2021-08-14 10:17:40,051   global_step = 2699
2021-08-14 10:17:40,051   loss = 0.2220466040591038
2021-08-14 10:17:40,051   rep_loss = 0.0
2021-08-14 10:18:28,318 ***** Running evaluation *****
2021-08-14 10:18:28,319   Epoch = 0 iter 2999 step
2021-08-14 10:18:28,319   Num examples = 9832
2021-08-14 10:18:28,319   Batch size = 32
2021-08-14 10:18:37,971 ***** Eval results *****
2021-08-14 10:18:37,971   acc = 0.822620016273393
2021-08-14 10:18:37,971   att_loss = 0.0
2021-08-14 10:18:37,971   cls_loss = 0.22113553366787475
2021-08-14 10:18:37,971   eval_loss = 0.45674827815843866
2021-08-14 10:18:37,971   global_step = 2999
2021-08-14 10:18:37,971   loss = 0.22113553366787475
2021-08-14 10:18:37,971   rep_loss = 0.0
2021-08-14 10:19:28,122 ***** Running evaluation *****
2021-08-14 10:19:28,122   Epoch = 0 iter 3299 step
2021-08-14 10:19:28,122   Num examples = 9832
2021-08-14 10:19:28,122   Batch size = 32
2021-08-14 10:19:37,792 ***** Eval results *****
2021-08-14 10:19:37,792   acc = 0.8193653376729048
2021-08-14 10:19:37,792   att_loss = 0.0
2021-08-14 10:19:37,792   cls_loss = 0.22053133918321938
2021-08-14 10:19:37,792   eval_loss = 0.4643823936484851
2021-08-14 10:19:37,792   global_step = 3299
2021-08-14 10:19:37,792   loss = 0.22053133918321938
2021-08-14 10:19:37,792   rep_loss = 0.0
2021-08-14 10:20:26,033 ***** Running evaluation *****
2021-08-14 10:20:26,034   Epoch = 0 iter 3599 step
2021-08-14 10:20:26,034   Num examples = 9832
2021-08-14 10:20:26,034   Batch size = 32
2021-08-14 10:20:35,666 ***** Eval results *****
2021-08-14 10:20:35,666   acc = 0.8216029292107404
2021-08-14 10:20:35,666   att_loss = 0.0
2021-08-14 10:20:35,666   cls_loss = 0.21984421359346654
2021-08-14 10:20:35,666   eval_loss = 0.4564911488008189
2021-08-14 10:20:35,666   global_step = 3599
2021-08-14 10:20:35,666   loss = 0.21984421359346654
2021-08-14 10:20:35,666   rep_loss = 0.0
2021-08-14 10:21:23,966 ***** Running evaluation *****
2021-08-14 10:21:23,966   Epoch = 0 iter 3899 step
2021-08-14 10:21:23,966   Num examples = 9832
2021-08-14 10:21:23,966   Batch size = 32
2021-08-14 10:21:33,602 ***** Eval results *****
2021-08-14 10:21:33,602   acc = 0.8277054515866559
2021-08-14 10:21:33,602   att_loss = 0.0
2021-08-14 10:21:33,603   cls_loss = 0.2193583173189569
2021-08-14 10:21:33,603   eval_loss = 0.456508704787725
2021-08-14 10:21:33,603   global_step = 3899
2021-08-14 10:21:33,603   loss = 0.2193583173189569
2021-08-14 10:21:33,603   rep_loss = 0.0
2021-08-14 10:22:23,608 ***** Running evaluation *****
2021-08-14 10:22:23,608   Epoch = 0 iter 4199 step
2021-08-14 10:22:23,608   Num examples = 9832
2021-08-14 10:22:23,608   Batch size = 32
2021-08-14 10:22:33,275 ***** Eval results *****
2021-08-14 10:22:33,275   acc = 0.825772986167616
2021-08-14 10:22:33,275   att_loss = 0.0
2021-08-14 10:22:33,275   cls_loss = 0.21891083773274342
2021-08-14 10:22:33,275   eval_loss = 0.454328883874726
2021-08-14 10:22:33,276   global_step = 4199
2021-08-14 10:22:33,276   loss = 0.21891083773274342
2021-08-14 10:22:33,276   rep_loss = 0.0
2021-08-14 10:23:21,604 ***** Running evaluation *****
2021-08-14 10:23:21,604   Epoch = 0 iter 4499 step
2021-08-14 10:23:21,604   Num examples = 9832
2021-08-14 10:23:21,604   Batch size = 32
2021-08-14 10:23:32,887 ***** Eval results *****
2021-08-14 10:23:32,887   acc = 0.8248576078112286
2021-08-14 10:23:32,887   att_loss = 0.0
2021-08-14 10:23:32,888   cls_loss = 0.21869170542861865
2021-08-14 10:23:32,888   eval_loss = 0.45773899826136505
2021-08-14 10:23:32,888   global_step = 4499
2021-08-14 10:23:32,888   loss = 0.21869170542861865
2021-08-14 10:23:32,888   rep_loss = 0.0
2021-08-14 10:24:19,569 ***** Running evaluation *****
2021-08-14 10:24:19,570   Epoch = 0 iter 4799 step
2021-08-14 10:24:19,570   Num examples = 9832
2021-08-14 10:24:19,570   Batch size = 32
2021-08-14 10:24:30,916 ***** Eval results *****
2021-08-14 10:24:30,916   acc = 0.826586655817738
2021-08-14 10:24:30,916   att_loss = 0.0
2021-08-14 10:24:30,916   cls_loss = 0.21835988740432657
2021-08-14 10:24:30,916   eval_loss = 0.4569905272551945
2021-08-14 10:24:30,916   global_step = 4799
2021-08-14 10:24:30,916   loss = 0.21835988740432657
2021-08-14 10:24:30,916   rep_loss = 0.0
2021-08-14 10:25:20,935 ***** Running evaluation *****
2021-08-14 10:25:20,936   Epoch = 0 iter 5099 step
2021-08-14 10:25:20,936   Num examples = 9832
2021-08-14 10:25:20,936   Batch size = 32
2021-08-14 10:25:30,613 ***** Eval results *****
2021-08-14 10:25:30,614   acc = 0.8262815296989422
2021-08-14 10:25:30,614   att_loss = 0.0
2021-08-14 10:25:30,614   cls_loss = 0.21828079187395433
2021-08-14 10:25:30,614   eval_loss = 0.45130349536027226
2021-08-14 10:25:30,614   global_step = 5099
2021-08-14 10:25:30,614   loss = 0.21828079187395433
2021-08-14 10:25:30,614   rep_loss = 0.0
2021-08-14 10:26:18,871 ***** Running evaluation *****
2021-08-14 10:26:18,871   Epoch = 0 iter 5399 step
2021-08-14 10:26:18,871   Num examples = 9832
2021-08-14 10:26:18,871   Batch size = 32
2021-08-14 10:26:28,565 ***** Eval results *****
2021-08-14 10:26:28,566   acc = 0.8165174938974776
2021-08-14 10:26:28,566   att_loss = 0.0
2021-08-14 10:26:28,566   cls_loss = 0.21808439216076786
2021-08-14 10:26:28,566   eval_loss = 0.47399426425819274
2021-08-14 10:26:28,566   global_step = 5399
2021-08-14 10:26:28,566   loss = 0.21808439216076786
2021-08-14 10:26:28,566   rep_loss = 0.0
2021-08-14 10:27:16,979 ***** Running evaluation *****
2021-08-14 10:27:16,980   Epoch = 0 iter 5699 step
2021-08-14 10:27:16,980   Num examples = 9832
2021-08-14 10:27:16,980   Batch size = 32
2021-08-14 10:27:26,730 ***** Eval results *****
2021-08-14 10:27:26,730   acc = 0.8232302685109846
2021-08-14 10:27:26,730   att_loss = 0.0
2021-08-14 10:27:26,730   cls_loss = 0.2178788555674562
2021-08-14 10:27:26,730   eval_loss = 0.4657691590875
2021-08-14 10:27:26,730   global_step = 5699
2021-08-14 10:27:26,730   loss = 0.2178788555674562
2021-08-14 10:27:26,730   rep_loss = 0.0
2021-08-14 10:28:16,718 ***** Running evaluation *****
2021-08-14 10:28:16,718   Epoch = 0 iter 5999 step
2021-08-14 10:28:16,718   Num examples = 9832
2021-08-14 10:28:16,718   Batch size = 32
2021-08-14 10:28:26,418 ***** Eval results *****
2021-08-14 10:28:26,418   acc = 0.823840520748576
2021-08-14 10:28:26,418   att_loss = 0.0
2021-08-14 10:28:26,418   cls_loss = 0.21772585868596991
2021-08-14 10:28:26,418   eval_loss = 0.4601266233177928
2021-08-14 10:28:26,418   global_step = 5999
2021-08-14 10:28:26,418   loss = 0.21772585868596991
2021-08-14 10:28:26,418   rep_loss = 0.0
2021-08-14 10:29:14,640 ***** Running evaluation *****
2021-08-14 10:29:14,647   Epoch = 0 iter 6299 step
2021-08-14 10:29:14,647   Num examples = 9832
2021-08-14 10:29:14,647   Batch size = 32
2021-08-14 10:29:24,354 ***** Eval results *****
2021-08-14 10:29:24,354   acc = 0.8259764035801465
2021-08-14 10:29:24,354   att_loss = 0.0
2021-08-14 10:29:24,354   cls_loss = 0.21753683494166506
2021-08-14 10:29:24,354   eval_loss = 0.46646757269060457
2021-08-14 10:29:24,355   global_step = 6299
2021-08-14 10:29:24,355   loss = 0.21753683494166506
2021-08-14 10:29:24,355   rep_loss = 0.0
2021-08-14 10:30:12,850 ***** Running evaluation *****
2021-08-14 10:30:12,850   Epoch = 0 iter 6599 step
2021-08-14 10:30:12,850   Num examples = 9832
2021-08-14 10:30:12,850   Batch size = 32
2021-08-14 10:30:22,545 ***** Eval results *****
2021-08-14 10:30:22,545   acc = 0.8156021155410903
2021-08-14 10:30:22,545   att_loss = 0.0
2021-08-14 10:30:22,545   cls_loss = 0.21732474375780939
2021-08-14 10:30:22,545   eval_loss = 0.48432599380612373
2021-08-14 10:30:22,545   global_step = 6599
2021-08-14 10:30:22,545   loss = 0.21732474375780939
2021-08-14 10:30:22,545   rep_loss = 0.0
2021-08-14 10:31:12,608 ***** Running evaluation *****
2021-08-14 10:31:12,608   Epoch = 0 iter 6899 step
2021-08-14 10:31:12,608   Num examples = 9832
2021-08-14 10:31:12,608   Batch size = 32
2021-08-14 10:31:23,877 ***** Eval results *****
2021-08-14 10:31:23,877   acc = 0.8262815296989422
2021-08-14 10:31:23,877   att_loss = 0.0
2021-08-14 10:31:23,877   cls_loss = 0.21721755218930927
2021-08-14 10:31:23,877   eval_loss = 0.4604319381442937
2021-08-14 10:31:23,877   global_step = 6899
2021-08-14 10:31:23,877   loss = 0.21721755218930927
2021-08-14 10:31:23,877   rep_loss = 0.0
2021-08-14 10:32:10,398 ***** Running evaluation *****
2021-08-14 10:32:10,398   Epoch = 0 iter 7199 step
2021-08-14 10:32:10,398   Num examples = 9832
2021-08-14 10:32:10,398   Batch size = 32
2021-08-14 10:32:21,525 ***** Eval results *****
2021-08-14 10:32:21,526   acc = 0.8287225386493083
2021-08-14 10:32:21,526   att_loss = 0.0
2021-08-14 10:32:21,526   cls_loss = 0.2171927903911971
2021-08-14 10:32:21,526   eval_loss = 0.4600606519099954
2021-08-14 10:32:21,526   global_step = 7199
2021-08-14 10:32:21,526   loss = 0.2171927903911971
2021-08-14 10:32:21,526   rep_loss = 0.0
2021-08-14 10:33:08,004 ***** Running evaluation *****
2021-08-14 10:33:08,004   Epoch = 0 iter 7499 step
2021-08-14 10:33:08,004   Num examples = 9832
2021-08-14 10:33:08,005   Batch size = 32
2021-08-14 10:33:19,312 ***** Eval results *****
2021-08-14 10:33:19,312   acc = 0.8268917819365338
2021-08-14 10:33:19,312   att_loss = 0.0
2021-08-14 10:33:19,312   cls_loss = 0.21701025453340628
2021-08-14 10:33:19,312   eval_loss = 0.45658553227201687
2021-08-14 10:33:19,312   global_step = 7499
2021-08-14 10:33:19,312   loss = 0.21701025453340628
2021-08-14 10:33:19,312   rep_loss = 0.0
2021-08-14 10:34:09,161 ***** Running evaluation *****
2021-08-14 10:34:09,161   Epoch = 0 iter 7799 step
2021-08-14 10:34:09,161   Num examples = 9832
2021-08-14 10:34:09,161   Batch size = 32
2021-08-14 10:34:18,793 ***** Eval results *****
2021-08-14 10:34:18,793   acc = 0.8277054515866559
2021-08-14 10:34:18,793   att_loss = 0.0
2021-08-14 10:34:18,793   cls_loss = 0.21694798877238006
2021-08-14 10:34:18,793   eval_loss = 0.45134535252854424
2021-08-14 10:34:18,793   global_step = 7799
2021-08-14 10:34:18,793   loss = 0.21694798877238006
2021-08-14 10:34:18,793   rep_loss = 0.0
2021-08-14 10:35:06,918 ***** Running evaluation *****
2021-08-14 10:35:06,918   Epoch = 0 iter 8099 step
2021-08-14 10:35:06,919   Num examples = 9832
2021-08-14 10:35:06,919   Batch size = 32
2021-08-14 10:35:16,574 ***** Eval results *****
2021-08-14 10:35:16,575   acc = 0.8256712774613507
2021-08-14 10:35:16,575   att_loss = 0.0
2021-08-14 10:35:16,575   cls_loss = 0.21687293788072046
2021-08-14 10:35:16,575   eval_loss = 0.4631326152132703
2021-08-14 10:35:16,575   global_step = 8099
2021-08-14 10:35:16,575   loss = 0.21687293788072046
2021-08-14 10:35:16,575   rep_loss = 0.0
2021-08-14 10:36:04,812 ***** Running evaluation *****
2021-08-14 10:36:04,813   Epoch = 0 iter 8399 step
2021-08-14 10:36:04,813   Num examples = 9832
2021-08-14 10:36:04,813   Batch size = 32
2021-08-14 10:36:14,503 ***** Eval results *****
2021-08-14 10:36:14,503   acc = 0.8263832384052074
2021-08-14 10:36:14,503   att_loss = 0.0
2021-08-14 10:36:14,503   cls_loss = 0.21671891414185254
2021-08-14 10:36:14,503   eval_loss = 0.46040123904293234
2021-08-14 10:36:14,503   global_step = 8399
2021-08-14 10:36:14,503   loss = 0.21671891414185254
2021-08-14 10:36:14,504   rep_loss = 0.0
2021-08-14 10:37:04,448 ***** Running evaluation *****
2021-08-14 10:37:04,448   Epoch = 0 iter 8699 step
2021-08-14 10:37:04,448   Num examples = 9832
2021-08-14 10:37:04,448   Batch size = 32
2021-08-14 10:37:14,096 ***** Eval results *****
2021-08-14 10:37:14,096   acc = 0.8242473555736372
2021-08-14 10:37:14,096   att_loss = 0.0
2021-08-14 10:37:14,096   cls_loss = 0.2165572771324839
2021-08-14 10:37:14,096   eval_loss = 0.4587960157107997
2021-08-14 10:37:14,096   global_step = 8699
2021-08-14 10:37:14,096   loss = 0.2165572771324839
2021-08-14 10:37:14,096   rep_loss = 0.0
2021-08-14 10:38:02,408 ***** Running evaluation *****
2021-08-14 10:38:02,408   Epoch = 0 iter 8999 step
2021-08-14 10:38:02,408   Num examples = 9832
2021-08-14 10:38:02,408   Batch size = 32
2021-08-14 10:38:12,087 ***** Eval results *****
2021-08-14 10:38:12,087   acc = 0.825366151342555
2021-08-14 10:38:12,087   att_loss = 0.0
2021-08-14 10:38:12,087   cls_loss = 0.21630496262563495
2021-08-14 10:38:12,087   eval_loss = 0.4544612823368667
2021-08-14 10:38:12,087   global_step = 8999
2021-08-14 10:38:12,087   loss = 0.21630496262563495
2021-08-14 10:38:12,087   rep_loss = 0.0
2021-08-14 10:39:00,484 ***** Running evaluation *****
2021-08-14 10:39:00,484   Epoch = 0 iter 9299 step
2021-08-14 10:39:00,484   Num examples = 9832
2021-08-14 10:39:00,484   Batch size = 32
2021-08-14 10:39:10,190 ***** Eval results *****
2021-08-14 10:39:10,190   acc = 0.8245524816924329
2021-08-14 10:39:10,190   att_loss = 0.0
2021-08-14 10:39:10,191   cls_loss = 0.21619773321779123
2021-08-14 10:39:10,191   eval_loss = 0.45898752978869845
2021-08-14 10:39:10,191   global_step = 9299
2021-08-14 10:39:10,191   loss = 0.21619773321779123
2021-08-14 10:39:10,191   rep_loss = 0.0
2021-08-14 10:40:00,139 ***** Running evaluation *****
2021-08-14 10:40:00,139   Epoch = 0 iter 9599 step
2021-08-14 10:40:00,139   Num examples = 9832
2021-08-14 10:40:00,139   Batch size = 32
2021-08-14 10:40:11,306 ***** Eval results *****
2021-08-14 10:40:11,307   acc = 0.8267900732302685
2021-08-14 10:40:11,307   att_loss = 0.0
2021-08-14 10:40:11,307   cls_loss = 0.2160873054008159
2021-08-14 10:40:11,307   eval_loss = 0.45380731033427374
2021-08-14 10:40:11,307   global_step = 9599
2021-08-14 10:40:11,307   loss = 0.2160873054008159
2021-08-14 10:40:11,307   rep_loss = 0.0
2021-08-14 10:40:57,795 ***** Running evaluation *****
2021-08-14 10:40:57,795   Epoch = 0 iter 9899 step
2021-08-14 10:40:57,795   Num examples = 9832
2021-08-14 10:40:57,795   Batch size = 32
2021-08-14 10:41:09,116 ***** Eval results *****
2021-08-14 10:41:09,117   acc = 0.8244507729861676
2021-08-14 10:41:09,117   att_loss = 0.0
2021-08-14 10:41:09,117   cls_loss = 0.21590945586608823
2021-08-14 10:41:09,117   eval_loss = 0.4552403788094397
2021-08-14 10:41:09,117   global_step = 9899
2021-08-14 10:41:09,117   loss = 0.21590945586608823
2021-08-14 10:41:09,117   rep_loss = 0.0
2021-08-14 10:41:57,184 ***** Running evaluation *****
2021-08-14 10:41:57,185   Epoch = 0 iter 10199 step
2021-08-14 10:41:57,185   Num examples = 9832
2021-08-14 10:41:57,185   Batch size = 32
2021-08-14 10:42:06,814 ***** Eval results *****
2021-08-14 10:42:06,815   acc = 0.8255695687550855
2021-08-14 10:42:06,815   att_loss = 0.0
2021-08-14 10:42:06,815   cls_loss = 0.21581266986258954
2021-08-14 10:42:06,815   eval_loss = 0.45981003924623715
2021-08-14 10:42:06,815   global_step = 10199
2021-08-14 10:42:06,815   loss = 0.21581266986258954
2021-08-14 10:42:06,815   rep_loss = 0.0
2021-08-14 10:42:56,576 ***** Running evaluation *****
2021-08-14 10:42:56,576   Epoch = 0 iter 10499 step
2021-08-14 10:42:56,577   Num examples = 9832
2021-08-14 10:42:56,577   Batch size = 32
2021-08-14 10:43:06,176 ***** Eval results *****
2021-08-14 10:43:06,177   acc = 0.8295362082994304
2021-08-14 10:43:06,177   att_loss = 0.0
2021-08-14 10:43:06,177   cls_loss = 0.21561384407273132
2021-08-14 10:43:06,177   eval_loss = 0.44614491756860314
2021-08-14 10:43:06,177   global_step = 10499
2021-08-14 10:43:06,177   loss = 0.21561384407273132
2021-08-14 10:43:06,177   rep_loss = 0.0
2021-08-14 10:43:54,405 ***** Running evaluation *****
2021-08-14 10:43:54,406   Epoch = 0 iter 10799 step
2021-08-14 10:43:54,406   Num examples = 9832
2021-08-14 10:43:54,406   Batch size = 32
2021-08-14 10:44:04,078 ***** Eval results *****
2021-08-14 10:44:04,078   acc = 0.8293327908868999
2021-08-14 10:44:04,078   att_loss = 0.0
2021-08-14 10:44:04,078   cls_loss = 0.21553366214205286
2021-08-14 10:44:04,078   eval_loss = 0.4599711248143153
2021-08-14 10:44:04,078   global_step = 10799
2021-08-14 10:44:04,078   loss = 0.21553366214205286
2021-08-14 10:44:04,078   rep_loss = 0.0
2021-08-14 10:44:52,241 ***** Running evaluation *****
2021-08-14 10:44:52,242   Epoch = 0 iter 11099 step
2021-08-14 10:44:52,242   Num examples = 9832
2021-08-14 10:44:52,242   Batch size = 32
2021-08-14 10:45:01,836 ***** Eval results *****
2021-08-14 10:45:01,836   acc = 0.8293327908868999
2021-08-14 10:45:01,836   att_loss = 0.0
2021-08-14 10:45:01,836   cls_loss = 0.2154644978042121
2021-08-14 10:45:01,836   eval_loss = 0.45000178173377914
2021-08-14 10:45:01,836   global_step = 11099
2021-08-14 10:45:01,836   loss = 0.2154644978042121
2021-08-14 10:45:01,836   rep_loss = 0.0
2021-08-14 10:45:51,564 ***** Running evaluation *****
2021-08-14 10:45:51,564   Epoch = 0 iter 11399 step
2021-08-14 10:45:51,564   Num examples = 9832
2021-08-14 10:45:51,564   Batch size = 32
2021-08-14 10:46:01,227 ***** Eval results *****
2021-08-14 10:46:01,227   acc = 0.8277054515866559
2021-08-14 10:46:01,227   att_loss = 0.0
2021-08-14 10:46:01,227   cls_loss = 0.21535236347593836
2021-08-14 10:46:01,227   eval_loss = 0.45180380571778717
2021-08-14 10:46:01,227   global_step = 11399
2021-08-14 10:46:01,227   loss = 0.21535236347593836
2021-08-14 10:46:01,227   rep_loss = 0.0
2021-08-14 10:46:47,680 ***** Running evaluation *****
2021-08-14 10:46:47,681   Epoch = 0 iter 11699 step
2021-08-14 10:46:47,681   Num examples = 9832
2021-08-14 10:46:47,681   Batch size = 32
2021-08-14 10:46:58,852 ***** Eval results *****
2021-08-14 10:46:58,852   acc = 0.8267900732302685
2021-08-14 10:46:58,852   att_loss = 0.0
2021-08-14 10:46:58,852   cls_loss = 0.21530121021244666
2021-08-14 10:46:58,852   eval_loss = 0.45056562759465985
2021-08-14 10:46:58,852   global_step = 11699
2021-08-14 10:46:58,852   loss = 0.21530121021244666
2021-08-14 10:46:58,852   rep_loss = 0.0
2021-08-14 10:47:45,339 ***** Running evaluation *****
2021-08-14 10:47:45,340   Epoch = 0 iter 11999 step
2021-08-14 10:47:45,340   Num examples = 9832
2021-08-14 10:47:45,340   Batch size = 32
2021-08-14 10:47:54,996 ***** Eval results *****
2021-08-14 10:47:54,996   acc = 0.8267900732302685
2021-08-14 10:47:54,996   att_loss = 0.0
2021-08-14 10:47:54,996   cls_loss = 0.21518554001567225
2021-08-14 10:47:54,996   eval_loss = 0.44896992963629884
2021-08-14 10:47:54,996   global_step = 11999
2021-08-14 10:47:54,996   loss = 0.21518554001567225
2021-08-14 10:47:54,996   rep_loss = 0.0
2021-08-14 10:48:44,800 ***** Running evaluation *****
2021-08-14 10:48:44,800   Epoch = 0 iter 12299 step
2021-08-14 10:48:44,800   Num examples = 9832
2021-08-14 10:48:44,800   Batch size = 32
2021-08-14 10:48:54,417 ***** Eval results *****
2021-08-14 10:48:54,417   acc = 0.8262815296989422
2021-08-14 10:48:54,417   att_loss = 0.0
2021-08-14 10:48:54,418   cls_loss = 0.21505225876577794
2021-08-14 10:48:54,418   eval_loss = 0.4540616066328117
2021-08-14 10:48:54,418   global_step = 12299
2021-08-14 10:48:54,418   loss = 0.21505225876577794
2021-08-14 10:48:54,418   rep_loss = 0.0
2021-08-14 10:49:42,446 ***** Running evaluation *****
2021-08-14 10:49:42,447   Epoch = 0 iter 12599 step
2021-08-14 10:49:42,447   Num examples = 9832
2021-08-14 10:49:42,447   Batch size = 32
2021-08-14 10:49:52,065 ***** Eval results *****
2021-08-14 10:49:52,065   acc = 0.8267900732302685
2021-08-14 10:49:52,065   att_loss = 0.0
2021-08-14 10:49:52,065   cls_loss = 0.214934894464557
2021-08-14 10:49:52,065   eval_loss = 0.45749819815739406
2021-08-14 10:49:52,065   global_step = 12599
2021-08-14 10:49:52,065   loss = 0.214934894464557
2021-08-14 10:49:52,065   rep_loss = 0.0
2021-08-14 10:50:38,658 ***** Running evaluation *****
2021-08-14 10:50:38,658   Epoch = 0 iter 12899 step
2021-08-14 10:50:38,658   Num examples = 9832
2021-08-14 10:50:38,658   Batch size = 32
2021-08-14 10:50:49,964 ***** Eval results *****
2021-08-14 10:50:49,964   acc = 0.8288242473555736
2021-08-14 10:50:49,964   att_loss = 0.0
2021-08-14 10:50:49,964   cls_loss = 0.21475494043320054
2021-08-14 10:50:49,964   eval_loss = 0.448455325872093
2021-08-14 10:50:49,964   global_step = 12899
2021-08-14 10:50:49,964   loss = 0.21475494043320054
2021-08-14 10:50:49,964   rep_loss = 0.0
2021-08-14 10:51:38,222 ***** Running evaluation *****
2021-08-14 10:51:38,223   Epoch = 0 iter 13199 step
2021-08-14 10:51:38,223   Num examples = 9832
2021-08-14 10:51:38,223   Batch size = 32
2021-08-14 10:51:47,872 ***** Eval results *****
2021-08-14 10:51:47,872   acc = 0.8277054515866559
2021-08-14 10:51:47,872   att_loss = 0.0
2021-08-14 10:51:47,872   cls_loss = 0.2146906739690555
2021-08-14 10:51:47,872   eval_loss = 0.4495814453762073
2021-08-14 10:51:47,872   global_step = 13199
2021-08-14 10:51:47,872   loss = 0.2146906739690555
2021-08-14 10:51:47,872   rep_loss = 0.0
2021-08-14 10:52:36,054 ***** Running evaluation *****
2021-08-14 10:52:36,054   Epoch = 0 iter 13499 step
2021-08-14 10:52:36,054   Num examples = 9832
2021-08-14 10:52:36,054   Batch size = 32
2021-08-14 10:52:45,655 ***** Eval results *****
2021-08-14 10:52:45,655   acc = 0.8270951993490643
2021-08-14 10:52:45,655   att_loss = 0.0
2021-08-14 10:52:45,655   cls_loss = 0.2145620880491054
2021-08-14 10:52:45,655   eval_loss = 0.45095009356737137
2021-08-14 10:52:45,655   global_step = 13499
2021-08-14 10:52:45,655   loss = 0.2145620880491054
2021-08-14 10:52:45,656   rep_loss = 0.0
2021-08-14 10:53:33,730 ***** Running evaluation *****
2021-08-14 10:53:33,730   Epoch = 0 iter 13799 step
2021-08-14 10:53:33,731   Num examples = 9832
2021-08-14 10:53:33,731   Batch size = 32
2021-08-14 10:53:43,368 ***** Eval results *****
2021-08-14 10:53:43,368   acc = 0.8293327908868999
2021-08-14 10:53:43,368   att_loss = 0.0
2021-08-14 10:53:43,368   cls_loss = 0.21449174681269817
2021-08-14 10:53:43,369   eval_loss = 0.45169767650303905
2021-08-14 10:53:43,369   global_step = 13799
2021-08-14 10:53:43,369   loss = 0.21449174681269817
2021-08-14 10:53:43,369   rep_loss = 0.0
2021-08-14 10:54:33,097 ***** Running evaluation *****
2021-08-14 10:54:33,097   Epoch = 0 iter 14099 step
2021-08-14 10:54:33,097   Num examples = 9832
2021-08-14 10:54:33,097   Batch size = 32
2021-08-14 10:54:42,719 ***** Eval results *****
2021-08-14 10:54:42,719   acc = 0.8285191212367778
2021-08-14 10:54:42,719   att_loss = 0.0
2021-08-14 10:54:42,720   cls_loss = 0.21441769439477906
2021-08-14 10:54:42,720   eval_loss = 0.44677121119646285
2021-08-14 10:54:42,720   global_step = 14099
2021-08-14 10:54:42,720   loss = 0.21441769439477906
2021-08-14 10:54:42,720   rep_loss = 0.0
2021-08-14 10:55:29,150 ***** Running evaluation *****
2021-08-14 10:55:29,150   Epoch = 0 iter 14399 step
2021-08-14 10:55:29,150   Num examples = 9832
2021-08-14 10:55:29,150   Batch size = 32
2021-08-14 10:55:40,353 ***** Eval results *****
2021-08-14 10:55:40,354   acc = 0.8277054515866559
2021-08-14 10:55:40,354   att_loss = 0.0
2021-08-14 10:55:40,354   cls_loss = 0.21431956759727913
2021-08-14 10:55:40,354   eval_loss = 0.4526391807798441
2021-08-14 10:55:40,354   global_step = 14399
2021-08-14 10:55:40,354   loss = 0.21431956759727913
2021-08-14 10:55:40,354   rep_loss = 0.0
2021-08-14 10:56:26,762 ***** Running evaluation *****
2021-08-14 10:56:26,762   Epoch = 0 iter 14699 step
2021-08-14 10:56:26,762   Num examples = 9832
2021-08-14 10:56:26,762   Batch size = 32
2021-08-14 10:56:36,358 ***** Eval results *****
2021-08-14 10:56:36,358   acc = 0.8329943043124491
2021-08-14 10:56:36,358   att_loss = 0.0
2021-08-14 10:56:36,358   cls_loss = 0.21423097423075235
2021-08-14 10:56:36,358   eval_loss = 0.4481941752813079
2021-08-14 10:56:36,358   global_step = 14699
2021-08-14 10:56:36,358   loss = 0.21423097423075235
2021-08-14 10:56:36,358   rep_loss = 0.0
2021-08-14 10:57:26,031 ***** Running evaluation *****
2021-08-14 10:57:26,032   Epoch = 0 iter 14999 step
2021-08-14 10:57:26,032   Num examples = 9832
2021-08-14 10:57:26,032   Batch size = 32
2021-08-14 10:57:35,659 ***** Eval results *****
2021-08-14 10:57:35,660   acc = 0.8234336859235151
2021-08-14 10:57:35,660   att_loss = 0.0
2021-08-14 10:57:35,660   cls_loss = 0.2141565658945395
2021-08-14 10:57:35,660   eval_loss = 0.4660081686814884
2021-08-14 10:57:35,660   global_step = 14999
2021-08-14 10:57:35,660   loss = 0.2141565658945395
2021-08-14 10:57:35,660   rep_loss = 0.0
2021-08-14 10:57:37,430 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/General_TinyBERT_6L_768D', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=48, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-14 10:57:37,595 device: cuda n_gpu: 4
2021-08-14 10:57:47,623 Writing example 0 of 505555
2021-08-14 10:57:47,624 *** Example ***
2021-08-14 10:57:47,624 guid: aug-0
2021-08-14 10:57:47,624 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 10:57:47,624 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:57:47,624 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:57:47,624 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 10:57:47,624 label: neutral
2021-08-14 10:57:47,624 label_id: 1
2021-08-14 10:57:52,298 Writing example 10000 of 505555
2021-08-14 10:57:56,865 Writing example 20000 of 505555
2021-08-14 10:58:01,345 Writing example 30000 of 505555
2021-08-14 10:58:06,011 Writing example 40000 of 505555
2021-08-14 10:58:10,814 Writing example 50000 of 505555
2021-08-14 10:58:15,285 Writing example 60000 of 505555
2021-08-14 10:58:19,880 Writing example 70000 of 505555
2021-08-14 10:58:23,666 ***** Running evaluation *****
2021-08-14 10:58:23,668   Epoch = 0 iter 15299 step
2021-08-14 10:58:23,669   Num examples = 9832
2021-08-14 10:58:23,669   Batch size = 32
2021-08-14 10:58:24,457 Writing example 80000 of 505555
2021-08-14 10:58:29,333 Writing example 90000 of 505555
2021-08-14 10:58:33,308 ***** Eval results *****
2021-08-14 10:58:33,310   acc = 0.8299430431244914
2021-08-14 10:58:33,310   att_loss = 0.0
2021-08-14 10:58:33,311   cls_loss = 0.2140276990214846
2021-08-14 10:58:33,311   eval_loss = 0.45006869017303763
2021-08-14 10:58:33,311   global_step = 15299
2021-08-14 10:58:33,311   loss = 0.2140276990214846
2021-08-14 10:58:33,311   rep_loss = 0.0
2021-08-14 10:58:33,890 Writing example 100000 of 505555
2021-08-14 10:58:38,472 Writing example 110000 of 505555
2021-08-14 10:58:42,851 Writing example 120000 of 505555
2021-08-14 10:58:47,484 Writing example 130000 of 505555
2021-08-14 10:58:52,736 Writing example 140000 of 505555
2021-08-14 10:58:57,133 Writing example 150000 of 505555
2021-08-14 10:59:02,103 Writing example 160000 of 505555
2021-08-14 10:59:06,738 Writing example 170000 of 505555
2021-08-14 10:59:11,326 Writing example 180000 of 505555
2021-08-14 10:59:16,011 Writing example 190000 of 505555
2021-08-14 10:59:19,822 ***** Running evaluation *****
2021-08-14 10:59:19,824   Epoch = 0 iter 15599 step
2021-08-14 10:59:19,825   Num examples = 9832
2021-08-14 10:59:19,825   Batch size = 32
2021-08-14 10:59:20,546 Writing example 200000 of 505555
2021-08-14 10:59:25,930 Writing example 210000 of 505555
2021-08-14 10:59:30,354 Writing example 220000 of 505555
2021-08-14 10:59:31,053 ***** Eval results *****
2021-08-14 10:59:31,054   acc = 0.8289259560618389
2021-08-14 10:59:31,054   att_loss = 0.0
2021-08-14 10:59:31,054   cls_loss = 0.2138919756929342
2021-08-14 10:59:31,054   eval_loss = 0.44952706000828124
2021-08-14 10:59:31,054   global_step = 15599
2021-08-14 10:59:31,054   loss = 0.2138919756929342
2021-08-14 10:59:31,054   rep_loss = 0.0
2021-08-14 10:59:34,940 Writing example 230000 of 505555
2021-08-14 10:59:39,642 Writing example 240000 of 505555
2021-08-14 10:59:44,285 Writing example 250000 of 505555
2021-08-14 10:59:48,850 Writing example 260000 of 505555
2021-08-14 10:59:53,323 Writing example 270000 of 505555
2021-08-14 10:59:57,756 Writing example 280000 of 505555
2021-08-14 11:00:03,330 Writing example 290000 of 505555
2021-08-14 11:00:08,006 Writing example 300000 of 505555
2021-08-14 11:00:12,548 Writing example 310000 of 505555
2021-08-14 11:00:16,915 Writing example 320000 of 505555
2021-08-14 11:00:19,363 ***** Running evaluation *****
2021-08-14 11:00:19,365   Epoch = 1 iter 15899 step
2021-08-14 11:00:19,365   Num examples = 9832
2021-08-14 11:00:19,365   Batch size = 32
2021-08-14 11:00:21,546 Writing example 330000 of 505555
2021-08-14 11:00:25,948 Writing example 340000 of 505555
2021-08-14 11:00:29,008 ***** Eval results *****
2021-08-14 11:00:29,010   acc = 0.8316720911310008
2021-08-14 11:00:29,010   att_loss = 0.0
2021-08-14 11:00:29,010   cls_loss = 0.20851220544612054
2021-08-14 11:00:29,010   eval_loss = 0.44597225016974784
2021-08-14 11:00:29,010   global_step = 15899
2021-08-14 11:00:29,010   loss = 0.20851220544612054
2021-08-14 11:00:29,010   rep_loss = 0.0
2021-08-14 11:00:30,417 Writing example 350000 of 505555
2021-08-14 11:00:34,967 Writing example 360000 of 505555
2021-08-14 11:00:39,358 Writing example 370000 of 505555
2021-08-14 11:00:43,800 Writing example 380000 of 505555
2021-08-14 11:00:49,765 Writing example 390000 of 505555
2021-08-14 11:00:54,379 Writing example 400000 of 505555
2021-08-14 11:00:58,920 Writing example 410000 of 505555
2021-08-14 11:01:03,330 Writing example 420000 of 505555
2021-08-14 11:01:07,737 Writing example 430000 of 505555
2021-08-14 11:01:12,141 Writing example 440000 of 505555
2021-08-14 11:01:16,795 Writing example 450000 of 505555
2021-08-14 11:01:17,141 ***** Running evaluation *****
2021-08-14 11:01:17,168   Epoch = 1 iter 16199 step
2021-08-14 11:01:17,168   Num examples = 9832
2021-08-14 11:01:17,168   Batch size = 32
2021-08-14 11:01:21,347 Writing example 460000 of 505555
2021-08-14 11:01:25,878 Writing example 470000 of 505555
2021-08-14 11:01:26,795 ***** Eval results *****
2021-08-14 11:01:26,814   acc = 0.826993490642799
2021-08-14 11:01:26,814   att_loss = 0.0
2021-08-14 11:01:26,814   cls_loss = 0.20760120076134317
2021-08-14 11:01:26,814   eval_loss = 0.4548986539147891
2021-08-14 11:01:26,814   global_step = 16199
2021-08-14 11:01:26,814   loss = 0.20760120076134317
2021-08-14 11:01:26,814   rep_loss = 0.0
2021-08-14 11:01:30,580 Writing example 480000 of 505555
2021-08-14 11:01:35,218 Writing example 490000 of 505555
2021-08-14 11:01:39,744 Writing example 500000 of 505555
2021-08-14 11:01:47,810 Writing example 0 of 9815
2021-08-14 11:01:47,811 *** Example ***
2021-08-14 11:01:47,811 guid: dev_matched-0
2021-08-14 11:01:47,811 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-14 11:01:47,811 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:01:47,811 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:01:47,811 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:01:47,811 label: neutral
2021-08-14 11:01:47,811 label_id: 1
2021-08-14 11:01:52,148 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 11:01:54,598 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 11:02:05,951 loading model...
2021-08-14 11:02:05,985 done!
2021-08-14 11:02:05,986 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 11:02:05,986 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 11:02:11,834 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-14 11:02:13,252 ***** Running evaluation *****
2021-08-14 11:02:13,255   Epoch = 1 iter 16499 step
2021-08-14 11:02:13,255   Num examples = 9832
2021-08-14 11:02:13,255   Batch size = 32
2021-08-14 11:02:13,324 Loading model /home/mcao610/scratch/General_TinyBERT_6L_768D/pytorch_model.bin
2021-08-14 11:02:14,613 loading model...
2021-08-14 11:02:14,626 done!
2021-08-14 11:02:14,626 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-08-14 11:02:14,626 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
2021-08-14 11:02:14,694 ***** Running training *****
2021-08-14 11:02:14,694   Num examples = 505555
2021-08-14 11:02:14,694   Batch size = 48
2021-08-14 11:02:14,694   Num steps = 52660
2021-08-14 11:02:14,695 n: module.bert.embeddings.word_embeddings.weight
2021-08-14 11:02:14,695 n: module.bert.embeddings.position_embeddings.weight
2021-08-14 11:02:14,695 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-14 11:02:14,695 n: module.bert.embeddings.LayerNorm.weight
2021-08-14 11:02:14,695 n: module.bert.embeddings.LayerNorm.bias
2021-08-14 11:02:14,695 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-14 11:02:14,695 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-14 11:02:14,695 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-14 11:02:14,695 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-14 11:02:14,695 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-14 11:02:14,696 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-14 11:02:14,697 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-14 11:02:14,698 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-14 11:02:14,699 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-14 11:02:14,700 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-14 11:02:14,701 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-14 11:02:14,701 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-14 11:02:14,701 n: module.bert.pooler.dense.weight
2021-08-14 11:02:14,701 n: module.bert.pooler.dense.bias
2021-08-14 11:02:14,701 n: module.classifier.weight
2021-08-14 11:02:14,701 n: module.classifier.bias
2021-08-14 11:02:14,701 n: module.fit_dense.weight
2021-08-14 11:02:14,701 n: module.fit_dense.bias
2021-08-14 11:02:14,701 Total parameters: 67547907
2021-08-14 11:02:22,927 ***** Eval results *****
2021-08-14 11:02:22,944   acc = 0.8303498779495525
2021-08-14 11:02:22,944   att_loss = 0.0
2021-08-14 11:02:22,944   cls_loss = 0.20749157896735698
2021-08-14 11:02:22,944   eval_loss = 0.4527614229395018
2021-08-14 11:02:22,944   global_step = 16499
2021-08-14 11:02:22,944   loss = 0.20749157896735698
2021-08-14 11:02:22,944   rep_loss = 0.0
2021-08-14 11:03:11,586 ***** Running evaluation *****
2021-08-14 11:03:11,586   Epoch = 1 iter 16799 step
2021-08-14 11:03:11,586   Num examples = 9832
2021-08-14 11:03:11,586   Batch size = 32
2021-08-14 11:03:22,709 ***** Eval results *****
2021-08-14 11:03:22,710   acc = 0.830960130187144
2021-08-14 11:03:22,710   att_loss = 0.0
2021-08-14 11:03:22,710   cls_loss = 0.20757565104758943
2021-08-14 11:03:22,710   eval_loss = 0.4475779832377062
2021-08-14 11:03:22,710   global_step = 16799
2021-08-14 11:03:22,710   loss = 0.20757565104758943
2021-08-14 11:03:22,710   rep_loss = 0.0
2021-08-14 11:04:09,174 ***** Running evaluation *****
2021-08-14 11:04:09,174   Epoch = 1 iter 17099 step
2021-08-14 11:04:09,174   Num examples = 9832
2021-08-14 11:04:09,174   Batch size = 32
2021-08-14 11:04:18,787 ***** Eval results *****
2021-08-14 11:04:18,787   acc = 0.830146460537022
2021-08-14 11:04:18,787   att_loss = 0.0
2021-08-14 11:04:18,787   cls_loss = 0.20732199211105945
2021-08-14 11:04:18,787   eval_loss = 0.45300818984578184
2021-08-14 11:04:18,787   global_step = 17099
2021-08-14 11:04:18,787   loss = 0.20732199211105945
2021-08-14 11:04:18,787   rep_loss = 0.0
2021-08-14 11:05:05,276 ***** Running evaluation *****
2021-08-14 11:05:05,276   Epoch = 1 iter 17399 step
2021-08-14 11:05:05,276   Num examples = 9832
2021-08-14 11:05:05,276   Batch size = 32
2021-08-14 11:05:14,933 ***** Eval results *****
2021-08-14 11:05:14,933   acc = 0.8256712774613507
2021-08-14 11:05:14,934   att_loss = 0.0
2021-08-14 11:05:14,934   cls_loss = 0.20696480328369557
2021-08-14 11:05:14,934   eval_loss = 0.4532560347639895
2021-08-14 11:05:14,934   global_step = 17399
2021-08-14 11:05:14,934   loss = 0.20696480328369557
2021-08-14 11:05:14,934   rep_loss = 0.0
2021-08-14 11:06:04,903 ***** Running evaluation *****
2021-08-14 11:06:04,904   Epoch = 1 iter 17699 step
2021-08-14 11:06:04,904   Num examples = 9832
2021-08-14 11:06:04,904   Batch size = 32
2021-08-14 11:06:14,590 ***** Eval results *****
2021-08-14 11:06:14,590   acc = 0.8284174125305126
2021-08-14 11:06:14,590   att_loss = 0.0
2021-08-14 11:06:14,590   cls_loss = 0.20677412592505606
2021-08-14 11:06:14,590   eval_loss = 0.45591581008070475
2021-08-14 11:06:14,590   global_step = 17699
2021-08-14 11:06:14,590   loss = 0.20677412592505606
2021-08-14 11:06:14,591   rep_loss = 0.0
2021-08-14 11:07:01,194 ***** Running evaluation *****
2021-08-14 11:07:01,194   Epoch = 1 iter 17999 step
2021-08-14 11:07:01,194   Num examples = 9832
2021-08-14 11:07:01,194   Batch size = 32
2021-08-14 11:07:10,929 ***** Eval results *****
2021-08-14 11:07:10,930   acc = 0.8315703824247356
2021-08-14 11:07:10,930   att_loss = 0.0
2021-08-14 11:07:10,930   cls_loss = 0.20674794310469458
2021-08-14 11:07:10,930   eval_loss = 0.44751629049514796
2021-08-14 11:07:10,930   global_step = 17999
2021-08-14 11:07:10,930   loss = 0.20674794310469458
2021-08-14 11:07:10,930   rep_loss = 0.0
2021-08-14 11:07:57,364 ***** Running evaluation *****
2021-08-14 11:07:57,365   Epoch = 1 iter 18299 step
2021-08-14 11:07:57,365   Num examples = 9832
2021-08-14 11:07:57,365   Batch size = 32
2021-08-14 11:08:06,965 ***** Running evaluation *****
2021-08-14 11:08:06,994   Epoch = 0 iter 1999 step
2021-08-14 11:08:06,994   Num examples = 9815
2021-08-14 11:08:06,994   Batch size = 32
2021-08-14 11:08:08,738 ***** Eval results *****
2021-08-14 11:08:08,740   acc = 0.8287225386493083
2021-08-14 11:08:08,740   att_loss = 0.0
2021-08-14 11:08:08,740   cls_loss = 0.20676449032389416
2021-08-14 11:08:08,741   eval_loss = 0.45292669171830274
2021-08-14 11:08:08,741   global_step = 18299
2021-08-14 11:08:08,741   loss = 0.20676449032389416
2021-08-14 11:08:08,741   rep_loss = 0.0
2021-08-14 11:08:16,787 ***** Eval results *****
2021-08-14 11:08:16,809   acc = 0.1269485481406011
2021-08-14 11:08:16,809   att_loss = 0.0
2021-08-14 11:08:16,809   cls_loss = 0.07924328080978078
2021-08-14 11:08:16,809   eval_loss = 1.1235576798162554
2021-08-14 11:08:16,809   global_step = 1999
2021-08-14 11:08:16,809   loss = 0.07924328080978078
2021-08-14 11:08:16,809   rep_loss = 0.0
2021-08-14 11:08:16,810 ***** Save model *****
2021-08-14 11:08:46,275 Writing example 0 of 9832
2021-08-14 11:08:46,276 *** Example ***
2021-08-14 11:08:46,276 guid: dev_matched-0
2021-08-14 11:08:46,276 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:08:46,276 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:08:46,276 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:08:46,276 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:08:46,276 label: contradiction
2021-08-14 11:08:46,277 label_id: 2
2021-08-14 11:08:50,909 ***** Running mm evaluation *****
2021-08-14 11:08:50,909   Num examples = 9832
2021-08-14 11:08:50,909   Batch size = 32
2021-08-14 11:08:56,694 ***** Running evaluation *****
2021-08-14 11:08:56,695   Epoch = 1 iter 18599 step
2021-08-14 11:08:56,695   Num examples = 9832
2021-08-14 11:08:56,695   Batch size = 32
2021-08-14 11:09:02,379 ***** Eval results *****
2021-08-14 11:09:02,407   acc = 0.12672904800650936
2021-08-14 11:09:02,407   eval_loss = 1.125451940994758
2021-08-14 11:09:02,407   global_step = 1999
2021-08-14 11:09:06,298 ***** Eval results *****
2021-08-14 11:09:06,299   acc = 0.8291293734743694
2021-08-14 11:09:06,299   att_loss = 0.0
2021-08-14 11:09:06,300   cls_loss = 0.20683425109499654
2021-08-14 11:09:06,300   eval_loss = 0.4513753839320951
2021-08-14 11:09:06,300   global_step = 18599
2021-08-14 11:09:06,300   loss = 0.20683425109499654
2021-08-14 11:09:06,300   rep_loss = 0.0
2021-08-14 11:09:52,791 ***** Running evaluation *****
2021-08-14 11:09:52,792   Epoch = 1 iter 18899 step
2021-08-14 11:09:52,792   Num examples = 9832
2021-08-14 11:09:52,792   Batch size = 32
2021-08-14 11:10:02,420 ***** Eval results *****
2021-08-14 11:10:02,420   acc = 0.8287225386493083
2021-08-14 11:10:02,420   att_loss = 0.0
2021-08-14 11:10:02,421   cls_loss = 0.2066765626768111
2021-08-14 11:10:02,421   eval_loss = 0.4576749051739643
2021-08-14 11:10:02,421   global_step = 18899
2021-08-14 11:10:02,421   loss = 0.2066765626768111
2021-08-14 11:10:02,421   rep_loss = 0.0
2021-08-14 11:10:50,642 ***** Running evaluation *****
2021-08-14 11:10:50,643   Epoch = 1 iter 19199 step
2021-08-14 11:10:50,643   Num examples = 9832
2021-08-14 11:10:50,643   Batch size = 32
2021-08-14 11:11:00,266 ***** Eval results *****
2021-08-14 11:11:00,266   acc = 0.8318755085435313
2021-08-14 11:11:00,266   att_loss = 0.0
2021-08-14 11:11:00,266   cls_loss = 0.2065550023117545
2021-08-14 11:11:00,266   eval_loss = 0.44272902274093073
2021-08-14 11:11:00,266   global_step = 19199
2021-08-14 11:11:00,266   loss = 0.2065550023117545
2021-08-14 11:11:00,266   rep_loss = 0.0
2021-08-14 11:11:48,209 ***** Running evaluation *****
2021-08-14 11:11:48,209   Epoch = 1 iter 19499 step
2021-08-14 11:11:48,209   Num examples = 9832
2021-08-14 11:11:48,209   Batch size = 32
2021-08-14 11:11:57,768 ***** Eval results *****
2021-08-14 11:11:57,768   acc = 0.8307567127746135
2021-08-14 11:11:57,768   att_loss = 0.0
2021-08-14 11:11:57,768   cls_loss = 0.2064726192567581
2021-08-14 11:11:57,768   eval_loss = 0.4533508602868427
2021-08-14 11:11:57,768   global_step = 19499
2021-08-14 11:11:57,768   loss = 0.2064726192567581
2021-08-14 11:11:57,768   rep_loss = 0.0
2021-08-14 11:12:45,340 ***** Running evaluation *****
2021-08-14 11:12:45,341   Epoch = 1 iter 19799 step
2021-08-14 11:12:45,341   Num examples = 9832
2021-08-14 11:12:45,341   Batch size = 32
2021-08-14 11:12:54,903 ***** Eval results *****
2021-08-14 11:12:54,903   acc = 0.8285191212367778
2021-08-14 11:12:54,903   att_loss = 0.0
2021-08-14 11:12:54,903   cls_loss = 0.2064041125010145
2021-08-14 11:12:54,904   eval_loss = 0.45716546412992787
2021-08-14 11:12:54,904   global_step = 19799
2021-08-14 11:12:54,904   loss = 0.2064041125010145
2021-08-14 11:12:54,904   rep_loss = 0.0
2021-08-14 11:13:41,033 ***** Running evaluation *****
2021-08-14 11:13:41,033   Epoch = 1 iter 20099 step
2021-08-14 11:13:41,033   Num examples = 9832
2021-08-14 11:13:41,033   Batch size = 32
2021-08-14 11:13:50,566 ***** Eval results *****
2021-08-14 11:13:50,567   acc = 0.8300447518307568
2021-08-14 11:13:50,567   att_loss = 0.0
2021-08-14 11:13:50,567   cls_loss = 0.20626013721665293
2021-08-14 11:13:50,567   eval_loss = 0.45231704718687316
2021-08-14 11:13:50,567   global_step = 20099
2021-08-14 11:13:50,567   loss = 0.20626013721665293
2021-08-14 11:13:50,567   rep_loss = 0.0
2021-08-14 11:14:38,321 ***** Running evaluation *****
2021-08-14 11:14:38,321   Epoch = 1 iter 20399 step
2021-08-14 11:14:38,321   Num examples = 9832
2021-08-14 11:14:38,321   Batch size = 32
2021-08-14 11:14:44,847 ***** Running evaluation *****
2021-08-14 11:14:44,889   Epoch = 0 iter 3999 step
2021-08-14 11:14:44,889   Num examples = 9832
2021-08-14 11:14:44,889   Batch size = 32
2021-08-14 11:14:49,417 ***** Eval results *****
2021-08-14 11:14:49,420   acc = 0.8320789259560618
2021-08-14 11:14:49,420   att_loss = 0.0
2021-08-14 11:14:49,420   cls_loss = 0.20632035832409235
2021-08-14 11:14:49,420   eval_loss = 0.44714494093091456
2021-08-14 11:14:49,420   global_step = 20399
2021-08-14 11:14:49,420   loss = 0.20632035832409235
2021-08-14 11:14:49,420   rep_loss = 0.0
2021-08-14 11:14:54,519 ***** Eval results *****
2021-08-14 11:14:54,522   acc = 0.11920260374288039
2021-08-14 11:14:54,522   att_loss = 0.0
2021-08-14 11:14:54,522   cls_loss = 0.07902321484192337
2021-08-14 11:14:54,523   eval_loss = 1.1258450069210746
2021-08-14 11:14:54,523   global_step = 3999
2021-08-14 11:14:54,523   loss = 0.07902321484192337
2021-08-14 11:14:54,523   rep_loss = 0.0
2021-08-14 11:14:54,523 ***** Save model *****
2021-08-14 11:14:55,761 Writing example 0 of 9832
2021-08-14 11:14:55,761 *** Example ***
2021-08-14 11:14:55,761 guid: dev_matched-0
2021-08-14 11:14:55,761 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:14:55,761 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:14:55,762 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:14:55,762 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:14:55,762 label: contradiction
2021-08-14 11:14:55,762 label_id: 2
2021-08-14 11:15:00,359 ***** Running mm evaluation *****
2021-08-14 11:15:00,359   Num examples = 9832
2021-08-14 11:15:00,359   Batch size = 32
2021-08-14 11:15:10,012 ***** Eval results *****
2021-08-14 11:15:10,012   acc = 0.11920260374288039
2021-08-14 11:15:10,012   eval_loss = 1.1258450069210746
2021-08-14 11:15:10,012   global_step = 3999
2021-08-14 11:15:35,538 ***** Running evaluation *****
2021-08-14 11:15:35,557   Epoch = 1 iter 20699 step
2021-08-14 11:15:35,557   Num examples = 9832
2021-08-14 11:15:35,557   Batch size = 32
2021-08-14 11:15:45,066 ***** Eval results *****
2021-08-14 11:15:45,066   acc = 0.8311635475996745
2021-08-14 11:15:45,066   att_loss = 0.0
2021-08-14 11:15:45,066   cls_loss = 0.2062759068341334
2021-08-14 11:15:45,066   eval_loss = 0.45283407824380056
2021-08-14 11:15:45,066   global_step = 20699
2021-08-14 11:15:45,066   loss = 0.2062759068341334
2021-08-14 11:15:45,066   rep_loss = 0.0
2021-08-14 11:16:31,042 ***** Running evaluation *****
2021-08-14 11:16:31,043   Epoch = 1 iter 20999 step
2021-08-14 11:16:31,043   Num examples = 9832
2021-08-14 11:16:31,043   Batch size = 32
2021-08-14 11:16:40,556 ***** Eval results *****
2021-08-14 11:16:40,556   acc = 0.8322823433685923
2021-08-14 11:16:40,556   att_loss = 0.0
2021-08-14 11:16:40,556   cls_loss = 0.2062991811630474
2021-08-14 11:16:40,556   eval_loss = 0.44694305206467577
2021-08-14 11:16:40,556   global_step = 20999
2021-08-14 11:16:40,556   loss = 0.2062991811630474
2021-08-14 11:16:40,556   rep_loss = 0.0
2021-08-14 11:17:29,834 ***** Running evaluation *****
2021-08-14 11:17:29,834   Epoch = 1 iter 21299 step
2021-08-14 11:17:29,834   Num examples = 9832
2021-08-14 11:17:29,834   Batch size = 32
2021-08-14 11:17:39,305 ***** Eval results *****
2021-08-14 11:17:39,305   acc = 0.8316720911310008
2021-08-14 11:17:39,305   att_loss = 0.0
2021-08-14 11:17:39,306   cls_loss = 0.20631101624550896
2021-08-14 11:17:39,306   eval_loss = 0.44494737994361233
2021-08-14 11:17:39,306   global_step = 21299
2021-08-14 11:17:39,306   loss = 0.20631101624550896
2021-08-14 11:17:39,306   rep_loss = 0.0
2021-08-14 11:18:25,123 ***** Running evaluation *****
2021-08-14 11:18:25,123   Epoch = 1 iter 21599 step
2021-08-14 11:18:25,124   Num examples = 9832
2021-08-14 11:18:25,124   Batch size = 32
2021-08-14 11:18:34,594 ***** Eval results *****
2021-08-14 11:18:34,594   acc = 0.826993490642799
2021-08-14 11:18:34,594   att_loss = 0.0
2021-08-14 11:18:34,594   cls_loss = 0.20639738383744835
2021-08-14 11:18:34,594   eval_loss = 0.4587985296528061
2021-08-14 11:18:34,594   global_step = 21599
2021-08-14 11:18:34,594   loss = 0.20639738383744835
2021-08-14 11:18:34,594   rep_loss = 0.0
2021-08-14 11:19:21,972 ***** Running evaluation *****
2021-08-14 11:19:21,973   Epoch = 1 iter 21899 step
2021-08-14 11:19:21,973   Num examples = 9832
2021-08-14 11:19:21,973   Batch size = 32
2021-08-14 11:19:31,433 ***** Eval results *****
2021-08-14 11:19:31,433   acc = 0.8322823433685923
2021-08-14 11:19:31,433   att_loss = 0.0
2021-08-14 11:19:31,433   cls_loss = 0.20639945345615837
2021-08-14 11:19:31,433   eval_loss = 0.4451884146924917
2021-08-14 11:19:31,433   global_step = 21899
2021-08-14 11:19:31,434   loss = 0.20639945345615837
2021-08-14 11:19:31,434   rep_loss = 0.0
2021-08-14 11:20:19,091 ***** Running evaluation *****
2021-08-14 11:20:19,092   Epoch = 1 iter 22199 step
2021-08-14 11:20:19,092   Num examples = 9832
2021-08-14 11:20:19,092   Batch size = 32
2021-08-14 11:20:28,540 ***** Eval results *****
2021-08-14 11:20:28,541   acc = 0.8308584214808787
2021-08-14 11:20:28,541   att_loss = 0.0
2021-08-14 11:20:28,541   cls_loss = 0.2062409100910031
2021-08-14 11:20:28,541   eval_loss = 0.4440186405433463
2021-08-14 11:20:28,541   global_step = 22199
2021-08-14 11:20:28,541   loss = 0.2062409100910031
2021-08-14 11:20:28,541   rep_loss = 0.0
2021-08-14 11:20:59,033 ***** Running evaluation *****
2021-08-14 11:20:59,035   Epoch = 0 iter 5999 step
2021-08-14 11:20:59,036   Num examples = 9832
2021-08-14 11:20:59,036   Batch size = 32
2021-08-14 11:21:08,602 ***** Eval results *****
2021-08-14 11:21:08,603   acc = 0.13303498779495526
2021-08-14 11:21:08,603   att_loss = 0.0
2021-08-14 11:21:08,603   cls_loss = 0.0788967264249516
2021-08-14 11:21:08,603   eval_loss = 1.1205625104439723
2021-08-14 11:21:08,603   global_step = 5999
2021-08-14 11:21:08,603   loss = 0.0788967264249516
2021-08-14 11:21:08,603   rep_loss = 0.0
2021-08-14 11:21:08,603 ***** Save model *****
2021-08-14 11:21:09,711 Writing example 0 of 9832
2021-08-14 11:21:09,712 *** Example ***
2021-08-14 11:21:09,712 guid: dev_matched-0
2021-08-14 11:21:09,712 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:21:09,712 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:21:09,712 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:21:09,712 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:21:09,712 label: contradiction
2021-08-14 11:21:09,712 label_id: 2
2021-08-14 11:21:14,243 ***** Running mm evaluation *****
2021-08-14 11:21:14,243   Num examples = 9832
2021-08-14 11:21:14,243   Batch size = 32
2021-08-14 11:21:14,472 ***** Running evaluation *****
2021-08-14 11:21:14,494   Epoch = 1 iter 22499 step
2021-08-14 11:21:14,494   Num examples = 9832
2021-08-14 11:21:14,494   Batch size = 32
2021-08-14 11:21:23,838 ***** Eval results *****
2021-08-14 11:21:23,902   acc = 0.13303498779495526
2021-08-14 11:21:23,902   eval_loss = 1.1205625104439723
2021-08-14 11:21:23,902   global_step = 5999
2021-08-14 11:21:25,493 ***** Eval results *****
2021-08-14 11:21:25,494   acc = 0.830553295362083
2021-08-14 11:21:25,494   att_loss = 0.0
2021-08-14 11:21:25,494   cls_loss = 0.20610194309151647
2021-08-14 11:21:25,494   eval_loss = 0.44731313314337234
2021-08-14 11:21:25,494   global_step = 22499
2021-08-14 11:21:25,494   loss = 0.20610194309151647
2021-08-14 11:21:25,494   rep_loss = 0.0
2021-08-14 11:22:11,632 ***** Running evaluation *****
2021-08-14 11:22:11,656   Epoch = 1 iter 22799 step
2021-08-14 11:22:11,656   Num examples = 9832
2021-08-14 11:22:11,656   Batch size = 32
2021-08-14 11:22:21,154 ***** Eval results *****
2021-08-14 11:22:21,154   acc = 0.8315703824247356
2021-08-14 11:22:21,154   att_loss = 0.0
2021-08-14 11:22:21,154   cls_loss = 0.20602543617364186
2021-08-14 11:22:21,154   eval_loss = 0.44763525057729187
2021-08-14 11:22:21,154   global_step = 22799
2021-08-14 11:22:21,154   loss = 0.20602543617364186
2021-08-14 11:22:21,154   rep_loss = 0.0
2021-08-14 11:23:08,598 ***** Running evaluation *****
2021-08-14 11:23:08,624   Epoch = 1 iter 23099 step
2021-08-14 11:23:08,624   Num examples = 9832
2021-08-14 11:23:08,625   Batch size = 32
2021-08-14 11:23:18,097 ***** Eval results *****
2021-08-14 11:23:18,097   acc = 0.830960130187144
2021-08-14 11:23:18,097   att_loss = 0.0
2021-08-14 11:23:18,098   cls_loss = 0.20592803817149008
2021-08-14 11:23:18,098   eval_loss = 0.4513679966330528
2021-08-14 11:23:18,098   global_step = 23099
2021-08-14 11:23:18,098   loss = 0.20592803817149008
2021-08-14 11:23:18,098   rep_loss = 0.0
2021-08-14 11:24:05,538 ***** Running evaluation *****
2021-08-14 11:24:05,565   Epoch = 1 iter 23399 step
2021-08-14 11:24:05,565   Num examples = 9832
2021-08-14 11:24:05,565   Batch size = 32
2021-08-14 11:24:15,050 ***** Eval results *****
2021-08-14 11:24:15,050   acc = 0.8326891781936534
2021-08-14 11:24:15,050   att_loss = 0.0
2021-08-14 11:24:15,050   cls_loss = 0.20592243929839954
2021-08-14 11:24:15,050   eval_loss = 0.4473092813867253
2021-08-14 11:24:15,050   global_step = 23399
2021-08-14 11:24:15,050   loss = 0.20592243929839954
2021-08-14 11:24:15,050   rep_loss = 0.0
2021-08-14 11:25:00,888 ***** Running evaluation *****
2021-08-14 11:25:00,889   Epoch = 1 iter 23699 step
2021-08-14 11:25:00,889   Num examples = 9832
2021-08-14 11:25:00,889   Batch size = 32
2021-08-14 11:25:10,343 ***** Eval results *****
2021-08-14 11:25:10,343   acc = 0.8338079739625712
2021-08-14 11:25:10,343   att_loss = 0.0
2021-08-14 11:25:10,343   cls_loss = 0.20588554388178315
2021-08-14 11:25:10,343   eval_loss = 0.44493025004283177
2021-08-14 11:25:10,343   global_step = 23699
2021-08-14 11:25:10,343   loss = 0.20588554388178315
2021-08-14 11:25:10,343   rep_loss = 0.0
2021-08-14 11:25:59,514 ***** Running evaluation *****
2021-08-14 11:25:59,516   Epoch = 1 iter 23999 step
2021-08-14 11:25:59,516   Num examples = 9832
2021-08-14 11:25:59,516   Batch size = 32
2021-08-14 11:26:08,979 ***** Eval results *****
2021-08-14 11:26:08,979   acc = 0.8277054515866559
2021-08-14 11:26:08,979   att_loss = 0.0
2021-08-14 11:26:08,979   cls_loss = 0.2059099048979325
2021-08-14 11:26:08,979   eval_loss = 0.45033655194686606
2021-08-14 11:26:08,979   global_step = 23999
2021-08-14 11:26:08,979   loss = 0.2059099048979325
2021-08-14 11:26:08,979   rep_loss = 0.0
2021-08-14 11:26:54,948 ***** Running evaluation *****
2021-08-14 11:26:54,955   Epoch = 1 iter 24299 step
2021-08-14 11:26:54,955   Num examples = 9832
2021-08-14 11:26:54,955   Batch size = 32
2021-08-14 11:27:04,430 ***** Eval results *****
2021-08-14 11:27:04,430   acc = 0.8278071602929211
2021-08-14 11:27:04,430   att_loss = 0.0
2021-08-14 11:27:04,430   cls_loss = 0.20583860497098294
2021-08-14 11:27:04,430   eval_loss = 0.45025482952788276
2021-08-14 11:27:04,430   global_step = 24299
2021-08-14 11:27:04,430   loss = 0.20583860497098294
2021-08-14 11:27:04,430   rep_loss = 0.0
2021-08-14 11:27:08,651 ***** Running evaluation *****
2021-08-14 11:27:08,667   Epoch = 0 iter 7999 step
2021-08-14 11:27:08,667   Num examples = 9832
2021-08-14 11:27:08,667   Batch size = 32
2021-08-14 11:27:18,251 ***** Eval results *****
2021-08-14 11:27:18,252   acc = 0.1392392188771359
2021-08-14 11:27:18,252   att_loss = 0.0
2021-08-14 11:27:18,252   cls_loss = 0.07881249895742914
2021-08-14 11:27:18,252   eval_loss = 1.1146323390595325
2021-08-14 11:27:18,252   global_step = 7999
2021-08-14 11:27:18,252   loss = 0.07881249895742914
2021-08-14 11:27:18,252   rep_loss = 0.0
2021-08-14 11:27:18,252 ***** Save model *****
2021-08-14 11:27:19,497 Writing example 0 of 9832
2021-08-14 11:27:19,498 *** Example ***
2021-08-14 11:27:19,498 guid: dev_matched-0
2021-08-14 11:27:19,498 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:27:19,498 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:27:19,498 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:27:19,498 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:27:19,498 label: contradiction
2021-08-14 11:27:19,498 label_id: 2
2021-08-14 11:27:24,015 ***** Running mm evaluation *****
2021-08-14 11:27:24,015   Num examples = 9832
2021-08-14 11:27:24,015   Batch size = 32
2021-08-14 11:27:33,637 ***** Eval results *****
2021-08-14 11:27:33,637   acc = 0.1392392188771359
2021-08-14 11:27:33,637   eval_loss = 1.1146323390595325
2021-08-14 11:27:33,637   global_step = 7999
2021-08-14 11:27:50,363 ***** Running evaluation *****
2021-08-14 11:27:50,376   Epoch = 1 iter 24599 step
2021-08-14 11:27:50,376   Num examples = 9832
2021-08-14 11:27:50,376   Batch size = 32
2021-08-14 11:27:59,833 ***** Eval results *****
2021-08-14 11:27:59,833   acc = 0.8332994304312449
2021-08-14 11:27:59,833   att_loss = 0.0
2021-08-14 11:27:59,833   cls_loss = 0.20581723686145553
2021-08-14 11:27:59,833   eval_loss = 0.44470176057188543
2021-08-14 11:27:59,833   global_step = 24599
2021-08-14 11:27:59,833   loss = 0.20581723686145553
2021-08-14 11:27:59,833   rep_loss = 0.0
2021-08-14 11:28:49,026 ***** Running evaluation *****
2021-08-14 11:28:49,039   Epoch = 1 iter 24899 step
2021-08-14 11:28:49,039   Num examples = 9832
2021-08-14 11:28:49,039   Batch size = 32
2021-08-14 11:28:58,554 ***** Eval results *****
2021-08-14 11:28:58,554   acc = 0.8354353132628153
2021-08-14 11:28:58,554   att_loss = 0.0
2021-08-14 11:28:58,554   cls_loss = 0.20578669598568614
2021-08-14 11:28:58,554   eval_loss = 0.44346244855747596
2021-08-14 11:28:58,554   global_step = 24899
2021-08-14 11:28:58,554   loss = 0.20578669598568614
2021-08-14 11:28:58,554   rep_loss = 0.0
2021-08-14 11:28:58,555 ***** Save model *****
2021-08-14 11:29:03,355 Writing example 0 of 9832
2021-08-14 11:29:03,355 *** Example ***
2021-08-14 11:29:03,356 guid: dev_matched-0
2021-08-14 11:29:03,356 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:29:03,356 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:29:03,356 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:29:03,356 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:29:03,356 label: contradiction
2021-08-14 11:29:03,356 label_id: 2
2021-08-14 11:29:07,835 ***** Running mm evaluation *****
2021-08-14 11:29:07,836   Num examples = 9832
2021-08-14 11:29:07,836   Batch size = 32
2021-08-14 11:29:17,371 ***** Eval results *****
2021-08-14 11:29:17,371   acc = 0.8354353132628153
2021-08-14 11:29:17,371   eval_loss = 0.44346244855747596
2021-08-14 11:29:17,371   global_step = 24899
2021-08-14 11:30:05,101 ***** Running evaluation *****
2021-08-14 11:30:05,120   Epoch = 1 iter 25199 step
2021-08-14 11:30:05,120   Num examples = 9832
2021-08-14 11:30:05,120   Batch size = 32
2021-08-14 11:30:14,653 ***** Eval results *****
2021-08-14 11:30:14,654   acc = 0.8335028478437754
2021-08-14 11:30:14,654   att_loss = 0.0
2021-08-14 11:30:14,654   cls_loss = 0.20575044124465763
2021-08-14 11:30:14,654   eval_loss = 0.4482550523788124
2021-08-14 11:30:14,654   global_step = 25199
2021-08-14 11:30:14,654   loss = 0.20575044124465763
2021-08-14 11:30:14,654   rep_loss = 0.0
2021-08-14 11:31:03,691 ***** Running evaluation *****
2021-08-14 11:31:03,708   Epoch = 1 iter 25499 step
2021-08-14 11:31:03,708   Num examples = 9832
2021-08-14 11:31:03,708   Batch size = 32
2021-08-14 11:31:14,702 ***** Eval results *****
2021-08-14 11:31:14,702   acc = 0.8339096826688365
2021-08-14 11:31:14,702   att_loss = 0.0
2021-08-14 11:31:14,702   cls_loss = 0.2057262237336682
2021-08-14 11:31:14,702   eval_loss = 0.440918916473528
2021-08-14 11:31:14,702   global_step = 25499
2021-08-14 11:31:14,702   loss = 0.2057262237336682
2021-08-14 11:31:14,703   rep_loss = 0.0
2021-08-14 11:32:00,670 ***** Running evaluation *****
2021-08-14 11:32:00,670   Epoch = 1 iter 25799 step
2021-08-14 11:32:00,670   Num examples = 9832
2021-08-14 11:32:00,670   Batch size = 32
2021-08-14 11:32:11,699 ***** Eval results *****
2021-08-14 11:32:11,700   acc = 0.8321806346623271
2021-08-14 11:32:11,700   att_loss = 0.0
2021-08-14 11:32:11,700   cls_loss = 0.20564704032948394
2021-08-14 11:32:11,700   eval_loss = 0.4438674506041911
2021-08-14 11:32:11,700   global_step = 25799
2021-08-14 11:32:11,700   loss = 0.20564704032948394
2021-08-14 11:32:11,700   rep_loss = 0.0
2021-08-14 11:32:57,745 ***** Running evaluation *****
2021-08-14 11:32:57,745   Epoch = 1 iter 26099 step
2021-08-14 11:32:57,745   Num examples = 9832
2021-08-14 11:32:57,745   Batch size = 32
2021-08-14 11:33:08,765 ***** Eval results *****
2021-08-14 11:33:08,765   acc = 0.8295362082994304
2021-08-14 11:33:08,765   att_loss = 0.0
2021-08-14 11:33:08,766   cls_loss = 0.2055778170459977
2021-08-14 11:33:08,766   eval_loss = 0.4462047911599859
2021-08-14 11:33:08,766   global_step = 26099
2021-08-14 11:33:08,766   loss = 0.2055778170459977
2021-08-14 11:33:08,766   rep_loss = 0.0
2021-08-14 11:33:21,672 ***** Running evaluation *****
2021-08-14 11:33:21,674   Epoch = 0 iter 9999 step
2021-08-14 11:33:21,674   Num examples = 9832
2021-08-14 11:33:21,674   Batch size = 32
2021-08-14 11:33:31,254 ***** Eval results *****
2021-08-14 11:33:31,254   acc = 0.12093165174938975
2021-08-14 11:33:31,254   att_loss = 0.0
2021-08-14 11:33:31,254   cls_loss = 0.07875136137291817
2021-08-14 11:33:31,254   eval_loss = 1.1259675668431568
2021-08-14 11:33:31,254   global_step = 9999
2021-08-14 11:33:31,254   loss = 0.07875136137291817
2021-08-14 11:33:31,254   rep_loss = 0.0
2021-08-14 11:33:31,255 ***** Save model *****
2021-08-14 11:33:32,124 Writing example 0 of 9832
2021-08-14 11:33:32,124 *** Example ***
2021-08-14 11:33:32,124 guid: dev_matched-0
2021-08-14 11:33:32,124 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:33:32,124 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:33:32,125 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:33:32,125 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:33:32,125 label: contradiction
2021-08-14 11:33:32,125 label_id: 2
2021-08-14 11:33:36,657 ***** Running mm evaluation *****
2021-08-14 11:33:36,657   Num examples = 9832
2021-08-14 11:33:36,657   Batch size = 32
2021-08-14 11:33:46,260 ***** Eval results *****
2021-08-14 11:33:46,261   acc = 0.12093165174938975
2021-08-14 11:33:46,261   eval_loss = 1.1259675668431568
2021-08-14 11:33:46,261   global_step = 9999
2021-08-14 11:33:57,892 ***** Running evaluation *****
2021-08-14 11:33:57,908   Epoch = 1 iter 26399 step
2021-08-14 11:33:57,908   Num examples = 9832
2021-08-14 11:33:57,908   Batch size = 32
2021-08-14 11:34:07,406 ***** Eval results *****
2021-08-14 11:34:07,406   acc = 0.8343165174938975
2021-08-14 11:34:07,406   att_loss = 0.0
2021-08-14 11:34:07,406   cls_loss = 0.20556092741028137
2021-08-14 11:34:07,406   eval_loss = 0.43776239512802717
2021-08-14 11:34:07,406   global_step = 26399
2021-08-14 11:34:07,407   loss = 0.20556092741028137
2021-08-14 11:34:07,407   rep_loss = 0.0
2021-08-14 11:34:55,132 ***** Running evaluation *****
2021-08-14 11:34:55,132   Epoch = 1 iter 26699 step
2021-08-14 11:34:55,132   Num examples = 9832
2021-08-14 11:34:55,132   Batch size = 32
2021-08-14 11:35:04,647 ***** Eval results *****
2021-08-14 11:35:04,647   acc = 0.8295362082994304
2021-08-14 11:35:04,647   att_loss = 0.0
2021-08-14 11:35:04,647   cls_loss = 0.20557569801320769
2021-08-14 11:35:04,647   eval_loss = 0.45410931865116216
2021-08-14 11:35:04,647   global_step = 26699
2021-08-14 11:35:04,647   loss = 0.20557569801320769
2021-08-14 11:35:04,647   rep_loss = 0.0
2021-08-14 11:35:52,336 ***** Running evaluation *****
2021-08-14 11:35:52,336   Epoch = 1 iter 26999 step
2021-08-14 11:35:52,336   Num examples = 9832
2021-08-14 11:35:52,336   Batch size = 32
2021-08-14 11:36:01,879 ***** Eval results *****
2021-08-14 11:36:01,879   acc = 0.8311635475996745
2021-08-14 11:36:01,879   att_loss = 0.0
2021-08-14 11:36:01,879   cls_loss = 0.20554286842313838
2021-08-14 11:36:01,879   eval_loss = 0.4461061389899099
2021-08-14 11:36:01,879   global_step = 26999
2021-08-14 11:36:01,879   loss = 0.20554286842313838
2021-08-14 11:36:01,879   rep_loss = 0.0
2021-08-14 11:36:51,364 ***** Running evaluation *****
2021-08-14 11:36:51,364   Epoch = 1 iter 27299 step
2021-08-14 11:36:51,364   Num examples = 9832
2021-08-14 11:36:51,364   Batch size = 32
2021-08-14 11:37:00,906 ***** Eval results *****
2021-08-14 11:37:00,906   acc = 0.8339096826688365
2021-08-14 11:37:00,906   att_loss = 0.0
2021-08-14 11:37:00,906   cls_loss = 0.20550003625519245
2021-08-14 11:37:00,907   eval_loss = 0.4434285220000651
2021-08-14 11:37:00,907   global_step = 27299
2021-08-14 11:37:00,907   loss = 0.20550003625519245
2021-08-14 11:37:00,907   rep_loss = 0.0
2021-08-14 11:37:48,721 ***** Running evaluation *****
2021-08-14 11:37:48,721   Epoch = 1 iter 27599 step
2021-08-14 11:37:48,721   Num examples = 9832
2021-08-14 11:37:48,721   Batch size = 32
2021-08-14 11:37:58,268 ***** Eval results *****
2021-08-14 11:37:58,269   acc = 0.8348250610252238
2021-08-14 11:37:58,269   att_loss = 0.0
2021-08-14 11:37:58,269   cls_loss = 0.20540128008595626
2021-08-14 11:37:58,269   eval_loss = 0.4377567599733154
2021-08-14 11:37:58,269   global_step = 27599
2021-08-14 11:37:58,269   loss = 0.20540128008595626
2021-08-14 11:37:58,269   rep_loss = 0.0
2021-08-14 11:38:45,963 ***** Running evaluation *****
2021-08-14 11:38:45,964   Epoch = 1 iter 27899 step
2021-08-14 11:38:45,964   Num examples = 9832
2021-08-14 11:38:45,964   Batch size = 32
2021-08-14 11:38:55,464 ***** Eval results *****
2021-08-14 11:38:55,464   acc = 0.8335028478437754
2021-08-14 11:38:55,464   att_loss = 0.0
2021-08-14 11:38:55,464   cls_loss = 0.20536143986531186
2021-08-14 11:38:55,464   eval_loss = 0.44138226261386626
2021-08-14 11:38:55,464   global_step = 27899
2021-08-14 11:38:55,464   loss = 0.20536143986531186
2021-08-14 11:38:55,464   rep_loss = 0.0
2021-08-14 11:39:25,679 ***** Running evaluation *****
2021-08-14 11:39:25,701   Epoch = 1 iter 11999 step
2021-08-14 11:39:25,701   Num examples = 9832
2021-08-14 11:39:25,701   Batch size = 32
2021-08-14 11:39:35,271 ***** Eval results *****
2021-08-14 11:39:35,271   acc = 0.10709926769731488
2021-08-14 11:39:35,271   att_loss = 0.0
2021-08-14 11:39:35,271   cls_loss = 0.07838637392457522
2021-08-14 11:39:35,271   eval_loss = 1.1298129605008411
2021-08-14 11:39:35,271   global_step = 11999
2021-08-14 11:39:35,271   loss = 0.07838637392457522
2021-08-14 11:39:35,271   rep_loss = 0.0
2021-08-14 11:39:35,272 ***** Save model *****
2021-08-14 11:39:36,320 Writing example 0 of 9832
2021-08-14 11:39:36,321 *** Example ***
2021-08-14 11:39:36,321 guid: dev_matched-0
2021-08-14 11:39:36,321 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:39:36,321 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:39:36,321 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:39:36,321 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:39:36,321 label: contradiction
2021-08-14 11:39:36,321 label_id: 2
2021-08-14 11:39:40,864 ***** Running mm evaluation *****
2021-08-14 11:39:40,864   Num examples = 9832
2021-08-14 11:39:40,864   Batch size = 32
2021-08-14 11:39:44,697 ***** Running evaluation *****
2021-08-14 11:39:44,700   Epoch = 1 iter 28199 step
2021-08-14 11:39:44,700   Num examples = 9832
2021-08-14 11:39:44,700   Batch size = 32
2021-08-14 11:39:52,112 ***** Eval results *****
2021-08-14 11:39:52,147   acc = 0.10709926769731488
2021-08-14 11:39:52,147   eval_loss = 1.1298129605008411
2021-08-14 11:39:52,147   global_step = 11999
2021-08-14 11:39:55,757 ***** Eval results *****
2021-08-14 11:39:55,759   acc = 0.8326891781936534
2021-08-14 11:39:55,759   att_loss = 0.0
2021-08-14 11:39:55,759   cls_loss = 0.2053129048335361
2021-08-14 11:39:55,759   eval_loss = 0.4440702543436707
2021-08-14 11:39:55,759   global_step = 28199
2021-08-14 11:39:55,759   loss = 0.2053129048335361
2021-08-14 11:39:55,759   rep_loss = 0.0
2021-08-14 11:40:41,821 ***** Running evaluation *****
2021-08-14 11:40:41,821   Epoch = 1 iter 28499 step
2021-08-14 11:40:41,821   Num examples = 9832
2021-08-14 11:40:41,821   Batch size = 32
2021-08-14 11:40:52,861 ***** Eval results *****
2021-08-14 11:40:52,862   acc = 0.833706265256306
2021-08-14 11:40:52,862   att_loss = 0.0
2021-08-14 11:40:52,862   cls_loss = 0.20528945600401743
2021-08-14 11:40:52,862   eval_loss = 0.44109836501347555
2021-08-14 11:40:52,862   global_step = 28499
2021-08-14 11:40:52,862   loss = 0.20528945600401743
2021-08-14 11:40:52,862   rep_loss = 0.0
2021-08-14 11:41:40,397 ***** Running evaluation *****
2021-08-14 11:41:40,397   Epoch = 1 iter 28799 step
2021-08-14 11:41:40,397   Num examples = 9832
2021-08-14 11:41:40,397   Batch size = 32
2021-08-14 11:41:49,862 ***** Eval results *****
2021-08-14 11:41:49,862   acc = 0.8347233523189586
2021-08-14 11:41:49,862   att_loss = 0.0
2021-08-14 11:41:49,862   cls_loss = 0.20527423077427912
2021-08-14 11:41:49,863   eval_loss = 0.4389746733396858
2021-08-14 11:41:49,863   global_step = 28799
2021-08-14 11:41:49,863   loss = 0.20527423077427912
2021-08-14 11:41:49,863   rep_loss = 0.0
2021-08-14 11:42:38,958 ***** Running evaluation *****
2021-08-14 11:42:38,958   Epoch = 1 iter 29099 step
2021-08-14 11:42:38,958   Num examples = 9832
2021-08-14 11:42:38,958   Batch size = 32
2021-08-14 11:42:48,437 ***** Eval results *****
2021-08-14 11:42:48,437   acc = 0.8365541090317331
2021-08-14 11:42:48,437   att_loss = 0.0
2021-08-14 11:42:48,437   cls_loss = 0.20524963643884991
2021-08-14 11:42:48,437   eval_loss = 0.4403663770622247
2021-08-14 11:42:48,437   global_step = 29099
2021-08-14 11:42:48,437   loss = 0.20524963643884991
2021-08-14 11:42:48,437   rep_loss = 0.0
2021-08-14 11:42:48,438 ***** Save model *****
2021-08-14 11:42:50,054 Writing example 0 of 9832
2021-08-14 11:42:50,055 *** Example ***
2021-08-14 11:42:50,055 guid: dev_matched-0
2021-08-14 11:42:50,055 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:42:50,055 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:42:50,055 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:42:50,055 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:42:50,055 label: contradiction
2021-08-14 11:42:50,055 label_id: 2
2021-08-14 11:42:54,545 ***** Running mm evaluation *****
2021-08-14 11:42:54,545   Num examples = 9832
2021-08-14 11:42:54,545   Batch size = 32
2021-08-14 11:43:04,017 ***** Eval results *****
2021-08-14 11:43:04,017   acc = 0.8365541090317331
2021-08-14 11:43:04,017   eval_loss = 0.4403663770622247
2021-08-14 11:43:04,017   global_step = 29099
2021-08-14 11:43:53,150 ***** Running evaluation *****
2021-08-14 11:43:53,151   Epoch = 1 iter 29399 step
2021-08-14 11:43:53,151   Num examples = 9832
2021-08-14 11:43:53,151   Batch size = 32
2021-08-14 11:44:02,615 ***** Eval results *****
2021-08-14 11:44:02,615   acc = 0.8311635475996745
2021-08-14 11:44:02,615   att_loss = 0.0
2021-08-14 11:44:02,615   cls_loss = 0.205181878373716
2021-08-14 11:44:02,615   eval_loss = 0.4413571030675591
2021-08-14 11:44:02,615   global_step = 29399
2021-08-14 11:44:02,615   loss = 0.205181878373716
2021-08-14 11:44:02,615   rep_loss = 0.0
2021-08-14 11:44:48,420 ***** Running evaluation *****
2021-08-14 11:44:48,420   Epoch = 1 iter 29699 step
2021-08-14 11:44:48,420   Num examples = 9832
2021-08-14 11:44:48,420   Batch size = 32
2021-08-14 11:44:57,875 ***** Eval results *****
2021-08-14 11:44:57,875   acc = 0.834926769731489
2021-08-14 11:44:57,875   att_loss = 0.0
2021-08-14 11:44:57,875   cls_loss = 0.2051116007453683
2021-08-14 11:44:57,875   eval_loss = 0.4379794072214659
2021-08-14 11:44:57,875   global_step = 29699
2021-08-14 11:44:57,875   loss = 0.2051116007453683
2021-08-14 11:44:57,875   rep_loss = 0.0
2021-08-14 11:45:40,516 ***** Running evaluation *****
2021-08-14 11:45:40,537   Epoch = 1 iter 13999 step
2021-08-14 11:45:40,537   Num examples = 9832
2021-08-14 11:45:40,537   Batch size = 32
2021-08-14 11:45:43,645 ***** Running evaluation *****
2021-08-14 11:45:43,647   Epoch = 1 iter 29999 step
2021-08-14 11:45:43,647   Num examples = 9832
2021-08-14 11:45:43,647   Batch size = 32
2021-08-14 11:45:50,160 ***** Eval results *****
2021-08-14 11:45:50,165   acc = 0.10760781122864117
2021-08-14 11:45:50,165   att_loss = 0.0
2021-08-14 11:45:50,165   cls_loss = 0.07839403474686776
2021-08-14 11:45:50,165   eval_loss = 1.132450455581987
2021-08-14 11:45:50,165   global_step = 13999
2021-08-14 11:45:50,165   loss = 0.07839403474686776
2021-08-14 11:45:50,165   rep_loss = 0.0
2021-08-14 11:45:50,166 ***** Save model *****
2021-08-14 11:45:52,775 Writing example 0 of 9832
2021-08-14 11:45:52,776 *** Example ***
2021-08-14 11:45:52,776 guid: dev_matched-0
2021-08-14 11:45:52,776 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:45:52,776 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:45:52,776 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:45:52,776 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:45:52,776 label: contradiction
2021-08-14 11:45:52,777 label_id: 2
2021-08-14 11:45:53,095 ***** Eval results *****
2021-08-14 11:45:53,098   acc = 0.83533360455655
2021-08-14 11:45:53,099   att_loss = 0.0
2021-08-14 11:45:53,099   cls_loss = 0.20508233122183953
2021-08-14 11:45:53,099   eval_loss = 0.4404268167525917
2021-08-14 11:45:53,099   global_step = 29999
2021-08-14 11:45:53,099   loss = 0.20508233122183953
2021-08-14 11:45:53,099   rep_loss = 0.0
2021-08-14 11:45:57,325 ***** Running mm evaluation *****
2021-08-14 11:45:57,336   Num examples = 9832
2021-08-14 11:45:57,336   Batch size = 32
2021-08-14 11:46:06,941 ***** Eval results *****
2021-08-14 11:46:06,941   acc = 0.10760781122864117
2021-08-14 11:46:06,941   eval_loss = 1.132450455581987
2021-08-14 11:46:06,942   global_step = 13999
2021-08-14 11:46:41,991 ***** Running evaluation *****
2021-08-14 11:46:42,021   Epoch = 1 iter 30299 step
2021-08-14 11:46:42,022   Num examples = 9832
2021-08-14 11:46:42,022   Batch size = 32
2021-08-14 11:46:51,469 ***** Eval results *****
2021-08-14 11:46:51,469   acc = 0.8366558177379984
2021-08-14 11:46:51,469   att_loss = 0.0
2021-08-14 11:46:51,469   cls_loss = 0.20502570598781145
2021-08-14 11:46:51,469   eval_loss = 0.43807745052428987
2021-08-14 11:46:51,469   global_step = 30299
2021-08-14 11:46:51,469   loss = 0.20502570598781145
2021-08-14 11:46:51,469   rep_loss = 0.0
2021-08-14 11:46:51,470 ***** Save model *****
2021-08-14 11:46:52,858 Writing example 0 of 9832
2021-08-14 11:46:52,858 *** Example ***
2021-08-14 11:46:52,859 guid: dev_matched-0
2021-08-14 11:46:52,859 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:46:52,859 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:46:52,859 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:46:52,859 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:46:52,859 label: contradiction
2021-08-14 11:46:52,859 label_id: 2
2021-08-14 11:46:57,340 ***** Running mm evaluation *****
2021-08-14 11:46:57,340   Num examples = 9832
2021-08-14 11:46:57,340   Batch size = 32
2021-08-14 11:47:06,851 ***** Eval results *****
2021-08-14 11:47:06,852   acc = 0.8366558177379984
2021-08-14 11:47:06,852   eval_loss = 0.43807745052428987
2021-08-14 11:47:06,852   global_step = 30299
2021-08-14 11:47:54,384 ***** Running evaluation *****
2021-08-14 11:47:54,385   Epoch = 1 iter 30599 step
2021-08-14 11:47:54,385   Num examples = 9832
2021-08-14 11:47:54,385   Batch size = 32
2021-08-14 11:48:03,893 ***** Eval results *****
2021-08-14 11:48:03,894   acc = 0.8372660699755899
2021-08-14 11:48:03,894   att_loss = 0.0
2021-08-14 11:48:03,894   cls_loss = 0.20500938663204155
2021-08-14 11:48:03,894   eval_loss = 0.4396012157879092
2021-08-14 11:48:03,894   global_step = 30599
2021-08-14 11:48:03,894   loss = 0.20500938663204155
2021-08-14 11:48:03,894   rep_loss = 0.0
2021-08-14 11:48:03,894 ***** Save model *****
2021-08-14 11:48:05,357 Writing example 0 of 9832
2021-08-14 11:48:05,358 *** Example ***
2021-08-14 11:48:05,358 guid: dev_matched-0
2021-08-14 11:48:05,358 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:48:05,358 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:48:05,358 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:48:05,358 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:48:05,358 label: contradiction
2021-08-14 11:48:05,358 label_id: 2
2021-08-14 11:48:09,842 ***** Running mm evaluation *****
2021-08-14 11:48:09,842   Num examples = 9832
2021-08-14 11:48:09,842   Batch size = 32
2021-08-14 11:48:20,874 ***** Eval results *****
2021-08-14 11:48:20,874   acc = 0.8372660699755899
2021-08-14 11:48:20,875   eval_loss = 0.4396012157879092
2021-08-14 11:48:20,875   global_step = 30599
2021-08-14 11:49:06,763 ***** Running evaluation *****
2021-08-14 11:49:06,763   Epoch = 1 iter 30899 step
2021-08-14 11:49:06,763   Num examples = 9832
2021-08-14 11:49:06,763   Batch size = 32
2021-08-14 11:49:16,323 ***** Eval results *****
2021-08-14 11:49:16,323   acc = 0.8361472742066721
2021-08-14 11:49:16,323   att_loss = 0.0
2021-08-14 11:49:16,324   cls_loss = 0.204945623229319
2021-08-14 11:49:16,324   eval_loss = 0.4351408657702533
2021-08-14 11:49:16,324   global_step = 30899
2021-08-14 11:49:16,324   loss = 0.204945623229319
2021-08-14 11:49:16,324   rep_loss = 0.0
2021-08-14 11:50:03,888 ***** Running evaluation *****
2021-08-14 11:50:03,888   Epoch = 1 iter 31199 step
2021-08-14 11:50:03,888   Num examples = 9832
2021-08-14 11:50:03,888   Batch size = 32
2021-08-14 11:50:13,420 ***** Eval results *****
2021-08-14 11:50:13,420   acc = 0.8328925956061839
2021-08-14 11:50:13,421   att_loss = 0.0
2021-08-14 11:50:13,421   cls_loss = 0.2049117392485006
2021-08-14 11:50:13,421   eval_loss = 0.4375419036618301
2021-08-14 11:50:13,421   global_step = 31199
2021-08-14 11:50:13,421   loss = 0.2049117392485006
2021-08-14 11:50:13,421   rep_loss = 0.0
2021-08-14 11:51:00,849 ***** Running evaluation *****
2021-08-14 11:51:00,850   Epoch = 1 iter 31499 step
2021-08-14 11:51:00,850   Num examples = 9832
2021-08-14 11:51:00,850   Batch size = 32
2021-08-14 11:51:10,348 ***** Eval results *****
2021-08-14 11:51:10,348   acc = 0.8298413344182262
2021-08-14 11:51:10,348   att_loss = 0.0
2021-08-14 11:51:10,348   cls_loss = 0.20487598315214445
2021-08-14 11:51:10,348   eval_loss = 0.44432981438063957
2021-08-14 11:51:10,348   global_step = 31499
2021-08-14 11:51:10,348   loss = 0.20487598315214445
2021-08-14 11:51:10,348   rep_loss = 0.0
2021-08-14 11:51:55,130 ***** Running evaluation *****
2021-08-14 11:51:55,155   Epoch = 1 iter 15999 step
2021-08-14 11:51:55,155   Num examples = 9832
2021-08-14 11:51:55,155   Batch size = 32
2021-08-14 11:51:56,287 ***** Running evaluation *****
2021-08-14 11:51:56,305   Epoch = 2 iter 31799 step
2021-08-14 11:51:56,305   Num examples = 9832
2021-08-14 11:51:56,305   Batch size = 32
2021-08-14 11:52:04,775 ***** Eval results *****
2021-08-14 11:52:04,803   acc = 0.1119812855980472
2021-08-14 11:52:04,803   att_loss = 0.0
2021-08-14 11:52:04,803   cls_loss = 0.07838738269085899
2021-08-14 11:52:04,803   eval_loss = 1.1319371434775265
2021-08-14 11:52:04,803   global_step = 15999
2021-08-14 11:52:04,803   loss = 0.07838738269085899
2021-08-14 11:52:04,803   rep_loss = 0.0
2021-08-14 11:52:04,804 ***** Save model *****
2021-08-14 11:52:05,799 Writing example 0 of 9832
2021-08-14 11:52:05,799 *** Example ***
2021-08-14 11:52:05,799 guid: dev_matched-0
2021-08-14 11:52:05,800 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:52:05,800 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:52:05,800 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:52:05,800 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:52:05,800 label: contradiction
2021-08-14 11:52:05,800 label_id: 2
2021-08-14 11:52:05,801 ***** Eval results *****
2021-08-14 11:52:05,803   acc = 0.8330960130187144
2021-08-14 11:52:05,803   att_loss = 0.0
2021-08-14 11:52:05,803   cls_loss = 0.20179036545929652
2021-08-14 11:52:05,803   eval_loss = 0.44071861772568194
2021-08-14 11:52:05,803   global_step = 31799
2021-08-14 11:52:05,803   loss = 0.20179036545929652
2021-08-14 11:52:05,803   rep_loss = 0.0
2021-08-14 11:52:10,330 ***** Running mm evaluation *****
2021-08-14 11:52:10,333   Num examples = 9832
2021-08-14 11:52:10,333   Batch size = 32
2021-08-14 11:52:19,967 ***** Eval results *****
2021-08-14 11:52:19,967   acc = 0.1119812855980472
2021-08-14 11:52:19,967   eval_loss = 1.1319371434775265
2021-08-14 11:52:19,967   global_step = 15999
2021-08-14 11:52:54,889 ***** Running evaluation *****
2021-08-14 11:52:54,907   Epoch = 2 iter 32099 step
2021-08-14 11:52:54,907   Num examples = 9832
2021-08-14 11:52:54,907   Batch size = 32
2021-08-14 11:53:04,402 ***** Eval results *****
2021-08-14 11:53:04,402   acc = 0.8346216436126932
2021-08-14 11:53:04,402   att_loss = 0.0
2021-08-14 11:53:04,402   cls_loss = 0.20215800253463312
2021-08-14 11:53:04,402   eval_loss = 0.4392569880787428
2021-08-14 11:53:04,402   global_step = 32099
2021-08-14 11:53:04,402   loss = 0.20215800253463312
2021-08-14 11:53:04,402   rep_loss = 0.0
2021-08-14 11:53:50,199 ***** Running evaluation *****
2021-08-14 11:53:50,199   Epoch = 2 iter 32399 step
2021-08-14 11:53:50,199   Num examples = 9832
2021-08-14 11:53:50,199   Batch size = 32
2021-08-14 11:53:59,643 ***** Eval results *****
2021-08-14 11:53:59,643   acc = 0.8322823433685923
2021-08-14 11:53:59,644   att_loss = 0.0
2021-08-14 11:53:59,644   cls_loss = 0.2021865868664916
2021-08-14 11:53:59,644   eval_loss = 0.44454733461335105
2021-08-14 11:53:59,644   global_step = 32399
2021-08-14 11:53:59,644   loss = 0.2021865868664916
2021-08-14 11:53:59,644   rep_loss = 0.0
2021-08-14 11:54:45,449 ***** Running evaluation *****
2021-08-14 11:54:45,476   Epoch = 2 iter 32699 step
2021-08-14 11:54:45,476   Num examples = 9832
2021-08-14 11:54:45,476   Batch size = 32
2021-08-14 11:54:56,462 ***** Eval results *****
2021-08-14 11:54:56,462   acc = 0.83533360455655
2021-08-14 11:54:56,462   att_loss = 0.0
2021-08-14 11:54:56,462   cls_loss = 0.20145323282193836
2021-08-14 11:54:56,462   eval_loss = 0.44028745615830667
2021-08-14 11:54:56,462   global_step = 32699
2021-08-14 11:54:56,462   loss = 0.20145323282193836
2021-08-14 11:54:56,462   rep_loss = 0.0
2021-08-14 11:55:44,028 ***** Running evaluation *****
2021-08-14 11:55:44,057   Epoch = 2 iter 32999 step
2021-08-14 11:55:44,057   Num examples = 9832
2021-08-14 11:55:44,057   Batch size = 32
2021-08-14 11:55:53,559 ***** Eval results *****
2021-08-14 11:55:53,559   acc = 0.8325874694873882
2021-08-14 11:55:53,559   att_loss = 0.0
2021-08-14 11:55:53,559   cls_loss = 0.2008462794661437
2021-08-14 11:55:53,559   eval_loss = 0.44641228987799064
2021-08-14 11:55:53,559   global_step = 32999
2021-08-14 11:55:53,559   loss = 0.2008462794661437
2021-08-14 11:55:53,559   rep_loss = 0.0
2021-08-14 11:56:39,562 ***** Running evaluation *****
2021-08-14 11:56:39,569   Epoch = 2 iter 33299 step
2021-08-14 11:56:39,569   Num examples = 9832
2021-08-14 11:56:39,569   Batch size = 32
2021-08-14 11:56:49,080 ***** Eval results *****
2021-08-14 11:56:49,081   acc = 0.8352318958502848
2021-08-14 11:56:49,081   att_loss = 0.0
2021-08-14 11:56:49,081   cls_loss = 0.20071654892359772
2021-08-14 11:56:49,081   eval_loss = 0.4396160092059668
2021-08-14 11:56:49,081   global_step = 33299
2021-08-14 11:56:49,081   loss = 0.20071654892359772
2021-08-14 11:56:49,081   rep_loss = 0.0
2021-08-14 11:57:36,663 ***** Running evaluation *****
2021-08-14 11:57:36,685   Epoch = 2 iter 33599 step
2021-08-14 11:57:36,685   Num examples = 9832
2021-08-14 11:57:36,685   Batch size = 32
2021-08-14 11:57:46,205 ***** Eval results *****
2021-08-14 11:57:46,205   acc = 0.8344182262001627
2021-08-14 11:57:46,205   att_loss = 0.0
2021-08-14 11:57:46,205   cls_loss = 0.20111845931409303
2021-08-14 11:57:46,205   eval_loss = 0.43809146017997297
2021-08-14 11:57:46,205   global_step = 33599
2021-08-14 11:57:46,205   loss = 0.20111845931409303
2021-08-14 11:57:46,205   rep_loss = 0.0
2021-08-14 11:58:04,290 ***** Running evaluation *****
2021-08-14 11:58:04,292   Epoch = 1 iter 17999 step
2021-08-14 11:58:04,292   Num examples = 9832
2021-08-14 11:58:04,292   Batch size = 32
2021-08-14 11:58:13,872 ***** Eval results *****
2021-08-14 11:58:13,872   acc = 0.11604963384865745
2021-08-14 11:58:13,872   att_loss = 0.0
2021-08-14 11:58:13,872   cls_loss = 0.07837588223301296
2021-08-14 11:58:13,872   eval_loss = 1.1294102688114365
2021-08-14 11:58:13,872   global_step = 17999
2021-08-14 11:58:13,872   loss = 0.07837588223301296
2021-08-14 11:58:13,872   rep_loss = 0.0
2021-08-14 11:58:13,873 ***** Save model *****
2021-08-14 11:58:14,974 Writing example 0 of 9832
2021-08-14 11:58:14,975 *** Example ***
2021-08-14 11:58:14,975 guid: dev_matched-0
2021-08-14 11:58:14,975 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 11:58:14,975 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:58:14,975 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:58:14,975 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 11:58:14,975 label: contradiction
2021-08-14 11:58:14,975 label_id: 2
2021-08-14 11:58:19,507 ***** Running mm evaluation *****
2021-08-14 11:58:19,507   Num examples = 9832
2021-08-14 11:58:19,507   Batch size = 32
2021-08-14 11:58:30,636 ***** Eval results *****
2021-08-14 11:58:30,636   acc = 0.11604963384865745
2021-08-14 11:58:30,636   eval_loss = 1.1294102688114365
2021-08-14 11:58:30,636   global_step = 17999
2021-08-14 11:58:33,711 ***** Running evaluation *****
2021-08-14 11:58:33,786   Epoch = 2 iter 33899 step
2021-08-14 11:58:33,786   Num examples = 9832
2021-08-14 11:58:33,786   Batch size = 32
2021-08-14 11:58:43,298 ***** Eval results *****
2021-08-14 11:58:43,298   acc = 0.8319772172497966
2021-08-14 11:58:43,298   att_loss = 0.0
2021-08-14 11:58:43,298   cls_loss = 0.20123184380767348
2021-08-14 11:58:43,298   eval_loss = 0.4457312835888429
2021-08-14 11:58:43,299   global_step = 33899
2021-08-14 11:58:43,299   loss = 0.20123184380767348
2021-08-14 11:58:43,299   rep_loss = 0.0
2021-08-14 11:59:30,907 ***** Running evaluation *****
2021-08-14 11:59:30,937   Epoch = 2 iter 34199 step
2021-08-14 11:59:30,937   Num examples = 9832
2021-08-14 11:59:30,937   Batch size = 32
2021-08-14 11:59:40,444 ***** Eval results *****
2021-08-14 11:59:40,444   acc = 0.8347233523189586
2021-08-14 11:59:40,444   att_loss = 0.0
2021-08-14 11:59:40,445   cls_loss = 0.20112794596462857
2021-08-14 11:59:40,445   eval_loss = 0.4395252526677274
2021-08-14 11:59:40,445   global_step = 34199
2021-08-14 11:59:40,445   loss = 0.20112794596462857
2021-08-14 11:59:40,445   rep_loss = 0.0
2021-08-14 12:00:26,471 ***** Running evaluation *****
2021-08-14 12:00:26,501   Epoch = 2 iter 34499 step
2021-08-14 12:00:26,501   Num examples = 9832
2021-08-14 12:00:26,501   Batch size = 32
2021-08-14 12:00:35,997 ***** Eval results *****
2021-08-14 12:00:35,997   acc = 0.8339096826688365
2021-08-14 12:00:35,997   att_loss = 0.0
2021-08-14 12:00:35,997   cls_loss = 0.20099676643471615
2021-08-14 12:00:35,997   eval_loss = 0.4390172168322198
2021-08-14 12:00:35,997   global_step = 34499
2021-08-14 12:00:35,997   loss = 0.20099676643471615
2021-08-14 12:00:35,997   rep_loss = 0.0
2021-08-14 12:01:23,563 ***** Running evaluation *****
2021-08-14 12:01:23,563   Epoch = 2 iter 34799 step
2021-08-14 12:01:23,563   Num examples = 9832
2021-08-14 12:01:23,563   Batch size = 32
2021-08-14 12:01:34,509 ***** Eval results *****
2021-08-14 12:01:34,509   acc = 0.8356387306753458
2021-08-14 12:01:34,510   att_loss = 0.0
2021-08-14 12:01:34,510   cls_loss = 0.20092991727183024
2021-08-14 12:01:34,510   eval_loss = 0.43913122387481973
2021-08-14 12:01:34,510   global_step = 34799
2021-08-14 12:01:34,510   loss = 0.20092991727183024
2021-08-14 12:01:34,510   rep_loss = 0.0
2021-08-14 12:02:20,318 ***** Running evaluation *****
2021-08-14 12:02:20,341   Epoch = 2 iter 35099 step
2021-08-14 12:02:20,341   Num examples = 9832
2021-08-14 12:02:20,341   Batch size = 32
2021-08-14 12:02:23,169 Writing example 0 of 392702
2021-08-14 12:02:23,182 *** Example ***
2021-08-14 12:02:23,183 guid: train-0
2021-08-14 12:02:23,183 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-14 12:02:23,184 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:02:23,184 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:02:23,184 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:02:23,185 label: neutral
2021-08-14 12:02:23,185 label_id: 1
2021-08-14 12:02:28,002 Writing example 10000 of 392702
2021-08-14 12:02:29,783 ***** Eval results *****
2021-08-14 12:02:29,784   acc = 0.8343165174938975
2021-08-14 12:02:29,785   att_loss = 0.0
2021-08-14 12:02:29,785   cls_loss = 0.2008759781841615
2021-08-14 12:02:29,785   eval_loss = 0.4374669153581966
2021-08-14 12:02:29,785   global_step = 35099
2021-08-14 12:02:29,785   loss = 0.2008759781841615
2021-08-14 12:02:29,785   rep_loss = 0.0
2021-08-14 12:02:32,954 Writing example 20000 of 392702
2021-08-14 12:02:37,735 Writing example 30000 of 392702
2021-08-14 12:02:42,523 Writing example 40000 of 392702
2021-08-14 12:02:47,697 Writing example 50000 of 392702
2021-08-14 12:02:52,494 Writing example 60000 of 392702
2021-08-14 12:02:57,342 Writing example 70000 of 392702
2021-08-14 12:03:02,229 Writing example 80000 of 392702
2021-08-14 12:03:07,486 Writing example 90000 of 392702
2021-08-14 12:03:12,337 Writing example 100000 of 392702
2021-08-14 12:03:15,542 ***** Running evaluation *****
2021-08-14 12:03:15,544   Epoch = 2 iter 35399 step
2021-08-14 12:03:15,544   Num examples = 9832
2021-08-14 12:03:15,544   Batch size = 32
2021-08-14 12:03:17,189 Writing example 110000 of 392702
2021-08-14 12:03:22,092 Writing example 120000 of 392702
2021-08-14 12:03:25,010 ***** Eval results *****
2021-08-14 12:03:25,011   acc = 0.8363506916192026
2021-08-14 12:03:25,011   att_loss = 0.0
2021-08-14 12:03:25,011   cls_loss = 0.20089007799261155
2021-08-14 12:03:25,011   eval_loss = 0.4367026083848693
2021-08-14 12:03:25,011   global_step = 35399
2021-08-14 12:03:25,011   loss = 0.20089007799261155
2021-08-14 12:03:25,011   rep_loss = 0.0
2021-08-14 12:03:26,925 Writing example 130000 of 392702
2021-08-14 12:03:32,416 Writing example 140000 of 392702
2021-08-14 12:03:37,285 Writing example 150000 of 392702
2021-08-14 12:03:42,185 Writing example 160000 of 392702
2021-08-14 12:03:47,048 Writing example 170000 of 392702
2021-08-14 12:03:51,911 Writing example 180000 of 392702
2021-08-14 12:03:56,720 Writing example 190000 of 392702
2021-08-14 12:04:02,339 Writing example 200000 of 392702
2021-08-14 12:04:07,188 Writing example 210000 of 392702
2021-08-14 12:04:12,032 Writing example 220000 of 392702
2021-08-14 12:04:14,022 ***** Running evaluation *****
2021-08-14 12:04:14,024   Epoch = 2 iter 35699 step
2021-08-14 12:04:14,024   Num examples = 9832
2021-08-14 12:04:14,024   Batch size = 32
2021-08-14 12:04:16,907 Writing example 230000 of 392702
2021-08-14 12:04:20,947 ***** Running evaluation *****
2021-08-14 12:04:20,949   Epoch = 1 iter 19999 step
2021-08-14 12:04:20,950   Num examples = 9832
2021-08-14 12:04:20,950   Batch size = 32
2021-08-14 12:04:21,705 Writing example 240000 of 392702
2021-08-14 12:04:23,509 ***** Eval results *****
2021-08-14 12:04:23,510   acc = 0.8330960130187144
2021-08-14 12:04:23,511   att_loss = 0.0
2021-08-14 12:04:23,511   cls_loss = 0.2009842868857113
2021-08-14 12:04:23,511   eval_loss = 0.4418290853500366
2021-08-14 12:04:23,511   global_step = 35699
2021-08-14 12:04:23,511   loss = 0.2009842868857113
2021-08-14 12:04:23,511   rep_loss = 0.0
2021-08-14 12:04:26,527 Writing example 250000 of 392702
2021-08-14 12:04:30,675 ***** Eval results *****
2021-08-14 12:04:30,677   acc = 0.10058991049633849
2021-08-14 12:04:30,677   att_loss = 0.0
2021-08-14 12:04:30,677   cls_loss = 0.07837052368870302
2021-08-14 12:04:30,677   eval_loss = 1.1388910943037505
2021-08-14 12:04:30,677   global_step = 19999
2021-08-14 12:04:30,677   loss = 0.07837052368870302
2021-08-14 12:04:30,677   rep_loss = 0.0
2021-08-14 12:04:30,677 ***** Save model *****
2021-08-14 12:04:31,420 Writing example 260000 of 392702
2021-08-14 12:04:32,057 Writing example 0 of 9832
2021-08-14 12:04:32,060 *** Example ***
2021-08-14 12:04:32,060 guid: dev_matched-0
2021-08-14 12:04:32,060 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:04:32,060 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:04:32,060 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:04:32,060 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:04:32,060 label: contradiction
2021-08-14 12:04:32,060 label_id: 2
2021-08-14 12:04:37,383 Writing example 270000 of 392702
2021-08-14 12:04:38,224 ***** Running mm evaluation *****
2021-08-14 12:04:38,225   Num examples = 9832
2021-08-14 12:04:38,226   Batch size = 32
2021-08-14 12:04:42,174 Writing example 280000 of 392702
2021-08-14 12:04:47,000 Writing example 290000 of 392702
2021-08-14 12:04:47,855 ***** Eval results *****
2021-08-14 12:04:47,856   acc = 0.10058991049633849
2021-08-14 12:04:47,857   eval_loss = 1.1388910943037505
2021-08-14 12:04:47,857   global_step = 19999
2021-08-14 12:04:51,885 Writing example 300000 of 392702
2021-08-14 12:04:56,829 Writing example 310000 of 392702
2021-08-14 12:05:01,744 Writing example 320000 of 392702
2021-08-14 12:05:06,591 Writing example 330000 of 392702
2021-08-14 12:05:09,402 ***** Running evaluation *****
2021-08-14 12:05:09,404   Epoch = 2 iter 35999 step
2021-08-14 12:05:09,405   Num examples = 9832
2021-08-14 12:05:09,405   Batch size = 32
2021-08-14 12:05:11,475 Writing example 340000 of 392702
2021-08-14 12:05:16,342 Writing example 350000 of 392702
2021-08-14 12:05:18,904 ***** Eval results *****
2021-08-14 12:05:18,906   acc = 0.8340113913751017
2021-08-14 12:05:18,906   att_loss = 0.0
2021-08-14 12:05:18,906   cls_loss = 0.2010054497516564
2021-08-14 12:05:18,906   eval_loss = 0.440319954913545
2021-08-14 12:05:18,906   global_step = 35999
2021-08-14 12:05:18,906   loss = 0.2010054497516564
2021-08-14 12:05:18,906   rep_loss = 0.0
2021-08-14 12:05:21,212 Writing example 360000 of 392702
2021-08-14 12:05:27,515 Writing example 370000 of 392702
2021-08-14 12:05:32,398 Writing example 380000 of 392702
2021-08-14 12:05:37,263 Writing example 390000 of 392702
2021-08-14 12:06:04,713 ***** Running evaluation *****
2021-08-14 12:06:04,715   Epoch = 2 iter 36299 step
2021-08-14 12:06:04,715   Num examples = 9832
2021-08-14 12:06:04,715   Batch size = 32
2021-08-14 12:06:15,777 ***** Eval results *****
2021-08-14 12:06:15,777   acc = 0.8364524003254679
2021-08-14 12:06:15,777   att_loss = 0.0
2021-08-14 12:06:15,777   cls_loss = 0.2008831038446901
2021-08-14 12:06:15,777   eval_loss = 0.43696723288142836
2021-08-14 12:06:15,777   global_step = 36299
2021-08-14 12:06:15,777   loss = 0.2008831038446901
2021-08-14 12:06:15,777   rep_loss = 0.0
2021-08-14 12:07:03,329 ***** Running evaluation *****
2021-08-14 12:07:03,329   Epoch = 2 iter 36599 step
2021-08-14 12:07:03,329   Num examples = 9832
2021-08-14 12:07:03,329   Batch size = 32
2021-08-14 12:07:12,813 ***** Eval results *****
2021-08-14 12:07:12,813   acc = 0.834926769731489
2021-08-14 12:07:12,813   att_loss = 0.0
2021-08-14 12:07:12,813   cls_loss = 0.2009543361108398
2021-08-14 12:07:12,813   eval_loss = 0.4396415974509406
2021-08-14 12:07:12,813   global_step = 36599
2021-08-14 12:07:12,814   loss = 0.2009543361108398
2021-08-14 12:07:12,814   rep_loss = 0.0
2021-08-14 12:07:58,657 ***** Running evaluation *****
2021-08-14 12:07:58,657   Epoch = 2 iter 36899 step
2021-08-14 12:07:58,657   Num examples = 9832
2021-08-14 12:07:58,657   Batch size = 32
2021-08-14 12:08:08,161 ***** Eval results *****
2021-08-14 12:08:08,161   acc = 0.8350284784377543
2021-08-14 12:08:08,161   att_loss = 0.0
2021-08-14 12:08:08,161   cls_loss = 0.201052011759376
2021-08-14 12:08:08,161   eval_loss = 0.43897542420339275
2021-08-14 12:08:08,161   global_step = 36899
2021-08-14 12:08:08,161   loss = 0.201052011759376
2021-08-14 12:08:08,161   rep_loss = 0.0
2021-08-14 12:08:55,560 ***** Running evaluation *****
2021-08-14 12:08:55,561   Epoch = 2 iter 37199 step
2021-08-14 12:08:55,561   Num examples = 9832
2021-08-14 12:08:55,561   Batch size = 32
2021-08-14 12:09:05,060 ***** Eval results *****
2021-08-14 12:09:05,060   acc = 0.8321806346623271
2021-08-14 12:09:05,061   att_loss = 0.0
2021-08-14 12:09:05,061   cls_loss = 0.20099556676387872
2021-08-14 12:09:05,061   eval_loss = 0.4406091936126158
2021-08-14 12:09:05,061   global_step = 37199
2021-08-14 12:09:05,061   loss = 0.20099556676387872
2021-08-14 12:09:05,061   rep_loss = 0.0
2021-08-14 12:09:52,575 ***** Running evaluation *****
2021-08-14 12:09:52,576   Epoch = 2 iter 37499 step
2021-08-14 12:09:52,576   Num examples = 9832
2021-08-14 12:09:52,576   Batch size = 32
2021-08-14 12:10:02,112 ***** Eval results *****
2021-08-14 12:10:02,112   acc = 0.8346216436126932
2021-08-14 12:10:02,112   att_loss = 0.0
2021-08-14 12:10:02,112   cls_loss = 0.20102747777508775
2021-08-14 12:10:02,113   eval_loss = 0.43882528958576067
2021-08-14 12:10:02,113   global_step = 37499
2021-08-14 12:10:02,113   loss = 0.20102747777508775
2021-08-14 12:10:02,113   rep_loss = 0.0
2021-08-14 12:10:32,042 ***** Running evaluation *****
2021-08-14 12:10:32,044   Epoch = 2 iter 21999 step
2021-08-14 12:10:32,044   Num examples = 9832
2021-08-14 12:10:32,044   Batch size = 32
2021-08-14 12:10:41,714 ***** Eval results *****
2021-08-14 12:10:41,714   acc = 0.11065907241659886
2021-08-14 12:10:41,714   att_loss = 0.0
2021-08-14 12:10:41,714   cls_loss = 0.07829012841304993
2021-08-14 12:10:41,714   eval_loss = 1.1319100856781006
2021-08-14 12:10:41,714   global_step = 21999
2021-08-14 12:10:41,714   loss = 0.07829012841304993
2021-08-14 12:10:41,714   rep_loss = 0.0
2021-08-14 12:10:41,714 ***** Save model *****
2021-08-14 12:10:42,904 Writing example 0 of 9832
2021-08-14 12:10:42,904 *** Example ***
2021-08-14 12:10:42,905 guid: dev_matched-0
2021-08-14 12:10:42,905 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:10:42,905 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:10:42,905 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:10:42,905 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:10:42,905 label: contradiction
2021-08-14 12:10:42,905 label_id: 2
2021-08-14 12:10:47,444 ***** Running mm evaluation *****
2021-08-14 12:10:47,444   Num examples = 9832
2021-08-14 12:10:47,444   Batch size = 32
2021-08-14 12:10:49,622 ***** Running evaluation *****
2021-08-14 12:10:49,625   Epoch = 2 iter 37799 step
2021-08-14 12:10:49,625   Num examples = 9832
2021-08-14 12:10:49,625   Batch size = 32
2021-08-14 12:10:57,062 ***** Eval results *****
2021-08-14 12:10:57,064   acc = 0.11065907241659886
2021-08-14 12:10:57,064   eval_loss = 1.1319100856781006
2021-08-14 12:10:57,064   global_step = 21999
2021-08-14 12:10:59,097 ***** Eval results *****
2021-08-14 12:10:59,098   acc = 0.8346216436126932
2021-08-14 12:10:59,098   att_loss = 0.0
2021-08-14 12:10:59,098   cls_loss = 0.20095583354006577
2021-08-14 12:10:59,098   eval_loss = 0.4340539300306277
2021-08-14 12:10:59,098   global_step = 37799
2021-08-14 12:10:59,098   loss = 0.20095583354006577
2021-08-14 12:10:59,098   rep_loss = 0.0
2021-08-14 12:11:44,996 ***** Running evaluation *****
2021-08-14 12:11:44,997   Epoch = 2 iter 38099 step
2021-08-14 12:11:44,997   Num examples = 9832
2021-08-14 12:11:44,997   Batch size = 32
2021-08-14 12:11:54,452 ***** Eval results *****
2021-08-14 12:11:54,453   acc = 0.8335028478437754
2021-08-14 12:11:54,453   att_loss = 0.0
2021-08-14 12:11:54,453   cls_loss = 0.20098488360492153
2021-08-14 12:11:54,453   eval_loss = 0.4395687177099965
2021-08-14 12:11:54,453   global_step = 38099
2021-08-14 12:11:54,453   loss = 0.20098488360492153
2021-08-14 12:11:54,453   rep_loss = 0.0
2021-08-14 12:12:41,939 ***** Running evaluation *****
2021-08-14 12:12:41,939   Epoch = 2 iter 38399 step
2021-08-14 12:12:41,939   Num examples = 9832
2021-08-14 12:12:41,939   Batch size = 32
2021-08-14 12:12:53,046 ***** Eval results *****
2021-08-14 12:12:53,047   acc = 0.8379780309194467
2021-08-14 12:12:53,047   att_loss = 0.0
2021-08-14 12:12:53,047   cls_loss = 0.2010476740711527
2021-08-14 12:12:53,047   eval_loss = 0.4371717534959316
2021-08-14 12:12:53,047   global_step = 38399
2021-08-14 12:12:53,047   loss = 0.2010476740711527
2021-08-14 12:12:53,047   rep_loss = 0.0
2021-08-14 12:12:53,047 ***** Save model *****
2021-08-14 12:12:55,340 Writing example 0 of 9832
2021-08-14 12:12:55,341 *** Example ***
2021-08-14 12:12:55,341 guid: dev_matched-0
2021-08-14 12:12:55,341 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:12:55,341 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:12:55,341 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:12:55,341 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:12:55,341 label: contradiction
2021-08-14 12:12:55,341 label_id: 2
2021-08-14 12:12:59,821 ***** Running mm evaluation *****
2021-08-14 12:12:59,822   Num examples = 9832
2021-08-14 12:12:59,822   Batch size = 32
2021-08-14 12:13:09,311 ***** Eval results *****
2021-08-14 12:13:09,312   acc = 0.8379780309194467
2021-08-14 12:13:09,312   eval_loss = 0.4371717534959316
2021-08-14 12:13:09,312   global_step = 38399
2021-08-14 12:13:55,217 ***** Running evaluation *****
2021-08-14 12:13:55,217   Epoch = 2 iter 38699 step
2021-08-14 12:13:55,217   Num examples = 9832
2021-08-14 12:13:55,217   Batch size = 32
2021-08-14 12:14:06,307 ***** Eval results *****
2021-08-14 12:14:06,307   acc = 0.8355370219690805
2021-08-14 12:14:06,307   att_loss = 0.0
2021-08-14 12:14:06,308   cls_loss = 0.20098986756830137
2021-08-14 12:14:06,308   eval_loss = 0.435163468703047
2021-08-14 12:14:06,308   global_step = 38699
2021-08-14 12:14:06,308   loss = 0.20098986756830137
2021-08-14 12:14:06,308   rep_loss = 0.0
2021-08-14 12:14:53,730 ***** Running evaluation *****
2021-08-14 12:14:53,730   Epoch = 2 iter 38999 step
2021-08-14 12:14:53,730   Num examples = 9832
2021-08-14 12:14:53,730   Batch size = 32
2021-08-14 12:15:03,220 ***** Eval results *****
2021-08-14 12:15:03,220   acc = 0.8351301871440195
2021-08-14 12:15:03,220   att_loss = 0.0
2021-08-14 12:15:03,220   cls_loss = 0.20093605561577177
2021-08-14 12:15:03,220   eval_loss = 0.4336859343887924
2021-08-14 12:15:03,220   global_step = 38999
2021-08-14 12:15:03,220   loss = 0.20093605561577177
2021-08-14 12:15:03,220   rep_loss = 0.0
2021-08-14 12:15:50,654 ***** Running evaluation *****
2021-08-14 12:15:50,655   Epoch = 2 iter 39299 step
2021-08-14 12:15:50,655   Num examples = 9832
2021-08-14 12:15:50,655   Batch size = 32
2021-08-14 12:16:00,117 ***** Eval results *****
2021-08-14 12:16:00,118   acc = 0.8358421480878763
2021-08-14 12:16:00,118   att_loss = 0.0
2021-08-14 12:16:00,118   cls_loss = 0.2009306253124704
2021-08-14 12:16:00,118   eval_loss = 0.43593627785320405
2021-08-14 12:16:00,118   global_step = 39299
2021-08-14 12:16:00,118   loss = 0.2009306253124704
2021-08-14 12:16:00,118   rep_loss = 0.0
2021-08-14 12:16:42,521 ***** Running evaluation *****
2021-08-14 12:16:42,542   Epoch = 2 iter 23999 step
2021-08-14 12:16:42,542   Num examples = 9832
2021-08-14 12:16:42,542   Batch size = 32
2021-08-14 12:16:47,485 ***** Running evaluation *****
2021-08-14 12:16:47,498   Epoch = 2 iter 39599 step
2021-08-14 12:16:47,498   Num examples = 9832
2021-08-14 12:16:47,498   Batch size = 32
2021-08-14 12:16:53,715 ***** Eval results *****
2021-08-14 12:16:53,727   acc = 0.09743694060211554
2021-08-14 12:16:53,727   att_loss = 0.0
2021-08-14 12:16:53,727   cls_loss = 0.07828321158632102
2021-08-14 12:16:53,727   eval_loss = 1.1403412230603107
2021-08-14 12:16:53,727   global_step = 23999
2021-08-14 12:16:53,727   loss = 0.07828321158632102
2021-08-14 12:16:53,727   rep_loss = 0.0
2021-08-14 12:16:53,728 ***** Save model *****
2021-08-14 12:16:56,029 Writing example 0 of 9832
2021-08-14 12:16:56,030 *** Example ***
2021-08-14 12:16:56,030 guid: dev_matched-0
2021-08-14 12:16:56,030 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:16:56,030 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:16:56,030 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:16:56,030 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:16:56,030 label: contradiction
2021-08-14 12:16:56,030 label_id: 2
2021-08-14 12:16:56,963 ***** Eval results *****
2021-08-14 12:16:56,965   acc = 0.8354353132628153
2021-08-14 12:16:56,966   att_loss = 0.0
2021-08-14 12:16:56,966   cls_loss = 0.2009280204426627
2021-08-14 12:16:56,966   eval_loss = 0.43228295842161424
2021-08-14 12:16:56,966   global_step = 39599
2021-08-14 12:16:56,966   loss = 0.2009280204426627
2021-08-14 12:16:56,966   rep_loss = 0.0
2021-08-14 12:17:00,597 ***** Running mm evaluation *****
2021-08-14 12:17:00,599   Num examples = 9832
2021-08-14 12:17:00,600   Batch size = 32
2021-08-14 12:17:10,270 ***** Eval results *****
2021-08-14 12:17:10,270   acc = 0.09743694060211554
2021-08-14 12:17:10,270   eval_loss = 1.1403412230603107
2021-08-14 12:17:10,270   global_step = 23999
2021-08-14 12:17:45,804 ***** Running evaluation *****
2021-08-14 12:17:45,807   Epoch = 2 iter 39899 step
2021-08-14 12:17:45,807   Num examples = 9832
2021-08-14 12:17:45,808   Batch size = 32
2021-08-14 12:17:55,316 ***** Eval results *****
2021-08-14 12:17:55,317   acc = 0.8360455655004069
2021-08-14 12:17:55,317   att_loss = 0.0
2021-08-14 12:17:55,317   cls_loss = 0.2008667042474208
2021-08-14 12:17:55,317   eval_loss = 0.4344844285447102
2021-08-14 12:17:55,317   global_step = 39899
2021-08-14 12:17:55,317   loss = 0.2008667042474208
2021-08-14 12:17:55,317   rep_loss = 0.0
2021-08-14 12:18:41,254 ***** Running evaluation *****
2021-08-14 12:18:41,254   Epoch = 2 iter 40199 step
2021-08-14 12:18:41,254   Num examples = 9832
2021-08-14 12:18:41,254   Batch size = 32
2021-08-14 12:18:52,278 ***** Eval results *****
2021-08-14 12:18:52,279   acc = 0.8348250610252238
2021-08-14 12:18:52,279   att_loss = 0.0
2021-08-14 12:18:52,279   cls_loss = 0.2007888461907326
2021-08-14 12:18:52,279   eval_loss = 0.4326596421661315
2021-08-14 12:18:52,279   global_step = 40199
2021-08-14 12:18:52,279   loss = 0.2007888461907326
2021-08-14 12:18:52,279   rep_loss = 0.0
2021-08-14 12:19:38,185 ***** Running evaluation *****
2021-08-14 12:19:38,186   Epoch = 2 iter 40499 step
2021-08-14 12:19:38,186   Num examples = 9832
2021-08-14 12:19:38,186   Batch size = 32
2021-08-14 12:19:47,696 ***** Eval results *****
2021-08-14 12:19:47,696   acc = 0.8377746135069162
2021-08-14 12:19:47,696   att_loss = 0.0
2021-08-14 12:19:47,696   cls_loss = 0.2006894721216825
2021-08-14 12:19:47,696   eval_loss = 0.4322730844187272
2021-08-14 12:19:47,696   global_step = 40499
2021-08-14 12:19:47,696   loss = 0.2006894721216825
2021-08-14 12:19:47,697   rep_loss = 0.0
2021-08-14 12:20:36,813 ***** Running evaluation *****
2021-08-14 12:20:36,814   Epoch = 2 iter 40799 step
2021-08-14 12:20:36,814   Num examples = 9832
2021-08-14 12:20:36,814   Batch size = 32
2021-08-14 12:20:46,311 ***** Eval results *****
2021-08-14 12:20:46,312   acc = 0.838486574450773
2021-08-14 12:20:46,312   att_loss = 0.0
2021-08-14 12:20:46,312   cls_loss = 0.20070433610323446
2021-08-14 12:20:46,312   eval_loss = 0.4356588156676138
2021-08-14 12:20:46,312   global_step = 40799
2021-08-14 12:20:46,312   loss = 0.20070433610323446
2021-08-14 12:20:46,312   rep_loss = 0.0
2021-08-14 12:20:46,312 ***** Save model *****
2021-08-14 12:20:47,790 Writing example 0 of 9832
2021-08-14 12:20:47,791 *** Example ***
2021-08-14 12:20:47,791 guid: dev_matched-0
2021-08-14 12:20:47,791 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:20:47,791 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:20:47,791 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:20:47,791 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:20:47,791 label: contradiction
2021-08-14 12:20:47,791 label_id: 2
2021-08-14 12:20:52,279 ***** Running mm evaluation *****
2021-08-14 12:20:52,279   Num examples = 9832
2021-08-14 12:20:52,279   Batch size = 32
2021-08-14 12:21:03,302 ***** Eval results *****
2021-08-14 12:21:03,302   acc = 0.838486574450773
2021-08-14 12:21:03,302   eval_loss = 0.4356588156676138
2021-08-14 12:21:03,302   global_step = 40799
2021-08-14 12:21:50,859 ***** Running evaluation *****
2021-08-14 12:21:50,860   Epoch = 2 iter 41099 step
2021-08-14 12:21:50,860   Num examples = 9832
2021-08-14 12:21:50,860   Batch size = 32
2021-08-14 12:22:00,361 ***** Eval results *****
2021-08-14 12:22:00,361   acc = 0.83533360455655
2021-08-14 12:22:00,361   att_loss = 0.0
2021-08-14 12:22:00,361   cls_loss = 0.20066998254828838
2021-08-14 12:22:00,362   eval_loss = 0.435123353277321
2021-08-14 12:22:00,362   global_step = 41099
2021-08-14 12:22:00,362   loss = 0.20066998254828838
2021-08-14 12:22:00,362   rep_loss = 0.0
2021-08-14 12:22:49,480 ***** Running evaluation *****
2021-08-14 12:22:49,480   Epoch = 2 iter 41399 step
2021-08-14 12:22:49,481   Num examples = 9832
2021-08-14 12:22:49,481   Batch size = 32
2021-08-14 12:23:00,265 ***** Running evaluation *****
2021-08-14 12:23:00,314   Epoch = 2 iter 25999 step
2021-08-14 12:23:00,314   Num examples = 9832
2021-08-14 12:23:00,314   Batch size = 32
2021-08-14 12:23:00,472 ***** Eval results *****
2021-08-14 12:23:00,474   acc = 0.8338079739625712
2021-08-14 12:23:00,474   att_loss = 0.0
2021-08-14 12:23:00,474   cls_loss = 0.20058079889741684
2021-08-14 12:23:00,474   eval_loss = 0.43500667046029845
2021-08-14 12:23:00,474   global_step = 41399
2021-08-14 12:23:00,474   loss = 0.20058079889741684
2021-08-14 12:23:00,474   rep_loss = 0.0
2021-08-14 12:23:09,975 ***** Eval results *****
2021-08-14 12:23:10,011   acc = 0.08787632221318145
2021-08-14 12:23:10,011   att_loss = 0.0
2021-08-14 12:23:10,011   cls_loss = 0.07827849083456466
2021-08-14 12:23:10,011   eval_loss = 1.14904900611221
2021-08-14 12:23:10,011   global_step = 25999
2021-08-14 12:23:10,011   loss = 0.07827849083456466
2021-08-14 12:23:10,012   rep_loss = 0.0
2021-08-14 12:23:10,012 ***** Save model *****
2021-08-14 12:23:11,134 Writing example 0 of 9832
2021-08-14 12:23:11,134 *** Example ***
2021-08-14 12:23:11,134 guid: dev_matched-0
2021-08-14 12:23:11,135 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:23:11,135 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:23:11,135 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:23:11,135 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:23:11,135 label: contradiction
2021-08-14 12:23:11,135 label_id: 2
2021-08-14 12:23:15,664 ***** Running mm evaluation *****
2021-08-14 12:23:15,664   Num examples = 9832
2021-08-14 12:23:15,664   Batch size = 32
2021-08-14 12:23:25,248 ***** Eval results *****
2021-08-14 12:23:25,248   acc = 0.08787632221318145
2021-08-14 12:23:25,248   eval_loss = 1.14904900611221
2021-08-14 12:23:25,248   global_step = 25999
2021-08-14 12:23:47,992 ***** Running evaluation *****
2021-08-14 12:23:48,028   Epoch = 2 iter 41699 step
2021-08-14 12:23:48,028   Num examples = 9832
2021-08-14 12:23:48,028   Batch size = 32
2021-08-14 12:23:57,556 ***** Eval results *****
2021-08-14 12:23:57,556   acc = 0.835740439381611
2021-08-14 12:23:57,556   att_loss = 0.0
2021-08-14 12:23:57,556   cls_loss = 0.20053519786000074
2021-08-14 12:23:57,556   eval_loss = 0.4329472046297092
2021-08-14 12:23:57,556   global_step = 41699
2021-08-14 12:23:57,556   loss = 0.20053519786000074
2021-08-14 12:23:57,556   rep_loss = 0.0
2021-08-14 12:24:45,153 ***** Running evaluation *****
2021-08-14 12:24:45,171   Epoch = 2 iter 41999 step
2021-08-14 12:24:45,171   Num examples = 9832
2021-08-14 12:24:45,171   Batch size = 32
2021-08-14 12:24:54,703 ***** Eval results *****
2021-08-14 12:24:54,704   acc = 0.8348250610252238
2021-08-14 12:24:54,704   att_loss = 0.0
2021-08-14 12:24:54,704   cls_loss = 0.2005370379763361
2021-08-14 12:24:54,704   eval_loss = 0.4334696962365082
2021-08-14 12:24:54,704   global_step = 41999
2021-08-14 12:24:54,704   loss = 0.2005370379763361
2021-08-14 12:24:54,704   rep_loss = 0.0
2021-08-14 12:25:45,564 ***** Running evaluation *****
2021-08-14 12:25:45,564   Epoch = 2 iter 42299 step
2021-08-14 12:25:45,564   Num examples = 9832
2021-08-14 12:25:45,564   Batch size = 32
2021-08-14 12:25:55,025 ***** Eval results *****
2021-08-14 12:25:55,025   acc = 0.8356387306753458
2021-08-14 12:25:55,025   att_loss = 0.0
2021-08-14 12:25:55,025   cls_loss = 0.2005215636675106
2021-08-14 12:25:55,025   eval_loss = 0.4313746751709418
2021-08-14 12:25:55,025   global_step = 42299
2021-08-14 12:25:55,025   loss = 0.2005215636675106
2021-08-14 12:25:55,025   rep_loss = 0.0
2021-08-14 12:26:42,539 ***** Running evaluation *****
2021-08-14 12:26:42,552   Epoch = 2 iter 42599 step
2021-08-14 12:26:42,552   Num examples = 9832
2021-08-14 12:26:42,552   Batch size = 32
2021-08-14 12:26:52,028 ***** Eval results *****
2021-08-14 12:26:52,029   acc = 0.8346216436126932
2021-08-14 12:26:52,029   att_loss = 0.0
2021-08-14 12:26:52,029   cls_loss = 0.20050574845480268
2021-08-14 12:26:52,029   eval_loss = 0.43356483923150346
2021-08-14 12:26:52,029   global_step = 42599
2021-08-14 12:26:52,029   loss = 0.20050574845480268
2021-08-14 12:26:52,029   rep_loss = 0.0
2021-08-14 12:27:39,528 ***** Running evaluation *****
2021-08-14 12:27:39,529   Epoch = 2 iter 42899 step
2021-08-14 12:27:39,529   Num examples = 9832
2021-08-14 12:27:39,529   Batch size = 32
2021-08-14 12:27:50,541 ***** Eval results *****
2021-08-14 12:27:50,541   acc = 0.8359438567941416
2021-08-14 12:27:50,541   att_loss = 0.0
2021-08-14 12:27:50,541   cls_loss = 0.2004928125484975
2021-08-14 12:27:50,541   eval_loss = 0.43281406420585394
2021-08-14 12:27:50,541   global_step = 42899
2021-08-14 12:27:50,541   loss = 0.2004928125484975
2021-08-14 12:27:50,541   rep_loss = 0.0
2021-08-14 12:28:39,761 ***** Running evaluation *****
2021-08-14 12:28:39,787   Epoch = 2 iter 43199 step
2021-08-14 12:28:39,787   Num examples = 9832
2021-08-14 12:28:39,787   Batch size = 32
2021-08-14 12:28:49,320 ***** Eval results *****
2021-08-14 12:28:49,320   acc = 0.8362489829129374
2021-08-14 12:28:49,320   att_loss = 0.0
2021-08-14 12:28:49,320   cls_loss = 0.2004949885943753
2021-08-14 12:28:49,320   eval_loss = 0.4320079881649513
2021-08-14 12:28:49,320   global_step = 43199
2021-08-14 12:28:49,320   loss = 0.2004949885943753
2021-08-14 12:28:49,320   rep_loss = 0.0
2021-08-14 12:29:09,852 ***** Running evaluation *****
2021-08-14 12:29:09,865   Epoch = 2 iter 27999 step
2021-08-14 12:29:09,865   Num examples = 9832
2021-08-14 12:29:09,866   Batch size = 32
2021-08-14 12:29:19,468 ***** Eval results *****
2021-08-14 12:29:19,468   acc = 0.08960537021969081
2021-08-14 12:29:19,468   att_loss = 0.0
2021-08-14 12:29:19,468   cls_loss = 0.07827682716324764
2021-08-14 12:29:19,468   eval_loss = 1.1430421065200458
2021-08-14 12:29:19,468   global_step = 27999
2021-08-14 12:29:19,469   loss = 0.07827682716324764
2021-08-14 12:29:19,469   rep_loss = 0.0
2021-08-14 12:29:19,469 ***** Save model *****
2021-08-14 12:29:22,156 Writing example 0 of 9832
2021-08-14 12:29:22,157 *** Example ***
2021-08-14 12:29:22,157 guid: dev_matched-0
2021-08-14 12:29:22,157 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:29:22,157 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:29:22,157 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:29:22,157 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:29:22,157 label: contradiction
2021-08-14 12:29:22,157 label_id: 2
2021-08-14 12:29:26,694 ***** Running mm evaluation *****
2021-08-14 12:29:26,694   Num examples = 9832
2021-08-14 12:29:26,694   Batch size = 32
2021-08-14 12:29:37,027 ***** Running evaluation *****
2021-08-14 12:29:37,047   Epoch = 2 iter 43499 step
2021-08-14 12:29:37,047   Num examples = 9832
2021-08-14 12:29:37,047   Batch size = 32
2021-08-14 12:29:38,006 ***** Eval results *****
2021-08-14 12:29:38,027   acc = 0.08960537021969081
2021-08-14 12:29:38,027   eval_loss = 1.1430421065200458
2021-08-14 12:29:38,027   global_step = 27999
2021-08-14 12:29:48,103 ***** Eval results *****
2021-08-14 12:29:48,129   acc = 0.8374694873881204
2021-08-14 12:29:48,129   att_loss = 0.0
2021-08-14 12:29:48,129   cls_loss = 0.20050595400038562
2021-08-14 12:29:48,130   eval_loss = 0.43199142363744897
2021-08-14 12:29:48,130   global_step = 43499
2021-08-14 12:29:48,130   loss = 0.20050595400038562
2021-08-14 12:29:48,130   rep_loss = 0.0
2021-08-14 12:30:35,795 ***** Running evaluation *****
2021-08-14 12:30:35,847   Epoch = 2 iter 43799 step
2021-08-14 12:30:35,847   Num examples = 9832
2021-08-14 12:30:35,847   Batch size = 32
2021-08-14 12:30:36,857 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-14 12:30:39,636 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-14 12:30:45,381 ***** Eval results *****
2021-08-14 12:30:45,382   acc = 0.8362489829129374
2021-08-14 12:30:45,382   att_loss = 0.0
2021-08-14 12:30:45,382   cls_loss = 0.20047389325583304
2021-08-14 12:30:45,382   eval_loss = 0.43146643545720487
2021-08-14 12:30:45,382   global_step = 43799
2021-08-14 12:30:45,382   loss = 0.20047389325583304
2021-08-14 12:30:45,382   rep_loss = 0.0
2021-08-14 12:30:58,455 loading model...
2021-08-14 12:30:58,509 done!
2021-08-14 12:30:58,510 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-14 12:30:58,510 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-14 12:31:34,692 ***** Running evaluation *****
2021-08-14 12:31:34,832   Epoch = 2 iter 44099 step
2021-08-14 12:31:34,833   Num examples = 9832
2021-08-14 12:31:34,833   Batch size = 32
2021-08-14 12:31:45,895 ***** Eval results *****
2021-08-14 12:31:45,895   acc = 0.8361472742066721
2021-08-14 12:31:45,895   att_loss = 0.0
2021-08-14 12:31:45,895   cls_loss = 0.20049758695854356
2021-08-14 12:31:45,895   eval_loss = 0.43385072821727044
2021-08-14 12:31:45,895   global_step = 44099
2021-08-14 12:31:45,895   loss = 0.20049758695854356
2021-08-14 12:31:45,895   rep_loss = 0.0
2021-08-14 12:32:33,464 ***** Running evaluation *****
2021-08-14 12:32:33,486   Epoch = 2 iter 44399 step
2021-08-14 12:32:33,486   Num examples = 9832
2021-08-14 12:32:33,486   Batch size = 32
2021-08-14 12:32:42,974 ***** Eval results *****
2021-08-14 12:32:42,974   acc = 0.8373677786818552
2021-08-14 12:32:42,974   att_loss = 0.0
2021-08-14 12:32:42,974   cls_loss = 0.20047087600307895
2021-08-14 12:32:42,974   eval_loss = 0.43207534073622195
2021-08-14 12:32:42,974   global_step = 44399
2021-08-14 12:32:42,974   loss = 0.20047087600307895
2021-08-14 12:32:42,974   rep_loss = 0.0
2021-08-14 12:33:30,454 ***** Running evaluation *****
2021-08-14 12:33:30,477   Epoch = 2 iter 44699 step
2021-08-14 12:33:30,477   Num examples = 9832
2021-08-14 12:33:30,478   Batch size = 32
2021-08-14 12:33:41,507 ***** Eval results *****
2021-08-14 12:33:41,507   acc = 0.8380797396257119
2021-08-14 12:33:41,507   att_loss = 0.0
2021-08-14 12:33:41,507   cls_loss = 0.20045505877347392
2021-08-14 12:33:41,507   eval_loss = 0.43205944535794194
2021-08-14 12:33:41,507   global_step = 44699
2021-08-14 12:33:41,507   loss = 0.20045505877347392
2021-08-14 12:33:41,507   rep_loss = 0.0
2021-08-14 12:34:30,719 ***** Running evaluation *****
2021-08-14 12:34:30,720   Epoch = 2 iter 44999 step
2021-08-14 12:34:30,720   Num examples = 9832
2021-08-14 12:34:30,720   Batch size = 32
2021-08-14 12:34:40,284 ***** Eval results *****
2021-08-14 12:34:40,284   acc = 0.8369609438567941
2021-08-14 12:34:40,284   att_loss = 0.0
2021-08-14 12:34:40,284   cls_loss = 0.2004308175246395
2021-08-14 12:34:40,284   eval_loss = 0.43339576635074306
2021-08-14 12:34:40,284   global_step = 44999
2021-08-14 12:34:40,284   loss = 0.2004308175246395
2021-08-14 12:34:40,284   rep_loss = 0.0
2021-08-14 12:35:22,648 ***** Running evaluation *****
2021-08-14 12:35:22,694   Epoch = 2 iter 29999 step
2021-08-14 12:35:22,694   Num examples = 9832
2021-08-14 12:35:22,694   Batch size = 32
2021-08-14 12:35:27,980 ***** Running evaluation *****
2021-08-14 12:35:28,000   Epoch = 2 iter 45299 step
2021-08-14 12:35:28,000   Num examples = 9832
2021-08-14 12:35:28,000   Batch size = 32
2021-08-14 12:35:32,316 ***** Eval results *****
2021-08-14 12:35:32,341   acc = 0.09031733116354759
2021-08-14 12:35:32,341   att_loss = 0.0
2021-08-14 12:35:32,341   cls_loss = 0.07827537447947927
2021-08-14 12:35:32,342   eval_loss = 1.144955402457869
2021-08-14 12:35:32,342   global_step = 29999
2021-08-14 12:35:32,342   loss = 0.07827537447947927
2021-08-14 12:35:32,342   rep_loss = 0.0
2021-08-14 12:35:32,342 ***** Save model *****
2021-08-14 12:35:33,304 Writing example 0 of 9832
2021-08-14 12:35:33,304 *** Example ***
2021-08-14 12:35:33,304 guid: dev_matched-0
2021-08-14 12:35:33,304 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:35:33,304 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:35:33,304 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:35:33,305 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:35:33,305 label: contradiction
2021-08-14 12:35:33,305 label_id: 2
2021-08-14 12:35:37,823 ***** Running mm evaluation *****
2021-08-14 12:35:37,823   Num examples = 9832
2021-08-14 12:35:37,823   Batch size = 32
2021-08-14 12:35:39,033 ***** Eval results *****
2021-08-14 12:35:39,057   acc = 0.8366558177379984
2021-08-14 12:35:39,057   att_loss = 0.0
2021-08-14 12:35:39,057   cls_loss = 0.20042108231710273
2021-08-14 12:35:39,057   eval_loss = 0.43207222542592455
2021-08-14 12:35:39,057   global_step = 45299
2021-08-14 12:35:39,057   loss = 0.20042108231710273
2021-08-14 12:35:39,057   rep_loss = 0.0
2021-08-14 12:35:47,481 ***** Eval results *****
2021-08-14 12:35:47,512   acc = 0.09031733116354759
2021-08-14 12:35:47,512   eval_loss = 1.144955402457869
2021-08-14 12:35:47,512   global_step = 29999
2021-08-14 12:36:26,598 ***** Running evaluation *****
2021-08-14 12:36:26,624   Epoch = 2 iter 45599 step
2021-08-14 12:36:26,624   Num examples = 9832
2021-08-14 12:36:26,624   Batch size = 32
2021-08-14 12:36:36,173 ***** Eval results *****
2021-08-14 12:36:36,173   acc = 0.8380797396257119
2021-08-14 12:36:36,173   att_loss = 0.0
2021-08-14 12:36:36,173   cls_loss = 0.20038599856817696
2021-08-14 12:36:36,173   eval_loss = 0.43113928302735477
2021-08-14 12:36:36,173   global_step = 45599
2021-08-14 12:36:36,173   loss = 0.20038599856817696
2021-08-14 12:36:36,173   rep_loss = 0.0
2021-08-14 12:37:25,334 ***** Running evaluation *****
2021-08-14 12:37:25,335   Epoch = 2 iter 45899 step
2021-08-14 12:37:25,335   Num examples = 9832
2021-08-14 12:37:25,335   Batch size = 32
2021-08-14 12:37:36,375 ***** Eval results *****
2021-08-14 12:37:36,375   acc = 0.8372660699755899
2021-08-14 12:37:36,375   att_loss = 0.0
2021-08-14 12:37:36,375   cls_loss = 0.2003734500968422
2021-08-14 12:37:36,375   eval_loss = 0.43242180957035586
2021-08-14 12:37:36,375   global_step = 45899
2021-08-14 12:37:36,375   loss = 0.2003734500968422
2021-08-14 12:37:36,375   rep_loss = 0.0
2021-08-14 12:38:23,751 ***** Running evaluation *****
2021-08-14 12:38:23,751   Epoch = 2 iter 46199 step
2021-08-14 12:38:23,751   Num examples = 9832
2021-08-14 12:38:23,751   Batch size = 32
2021-08-14 12:38:33,275 ***** Eval results *****
2021-08-14 12:38:33,275   acc = 0.8366558177379984
2021-08-14 12:38:33,275   att_loss = 0.0
2021-08-14 12:38:33,275   cls_loss = 0.20034922471436165
2021-08-14 12:38:33,275   eval_loss = 0.43239490566896155
2021-08-14 12:38:33,275   global_step = 46199
2021-08-14 12:38:33,275   loss = 0.20034922471436165
2021-08-14 12:38:33,275   rep_loss = 0.0
2021-08-14 12:39:20,795 ***** Running evaluation *****
2021-08-14 12:39:20,796   Epoch = 2 iter 46499 step
2021-08-14 12:39:20,796   Num examples = 9832
2021-08-14 12:39:20,796   Batch size = 32
2021-08-14 12:39:31,826 ***** Eval results *****
2021-08-14 12:39:31,826   acc = 0.8367575264442636
2021-08-14 12:39:31,826   att_loss = 0.0
2021-08-14 12:39:31,826   cls_loss = 0.20033977202366704
2021-08-14 12:39:31,826   eval_loss = 0.43240088916250635
2021-08-14 12:39:31,826   global_step = 46499
2021-08-14 12:39:31,827   loss = 0.20033977202366704
2021-08-14 12:39:31,827   rep_loss = 0.0
2021-08-14 12:40:20,965 ***** Running evaluation *****
2021-08-14 12:40:20,986   Epoch = 2 iter 46799 step
2021-08-14 12:40:20,986   Num examples = 9832
2021-08-14 12:40:20,986   Batch size = 32
2021-08-14 12:40:30,479 ***** Eval results *****
2021-08-14 12:40:30,479   acc = 0.8362489829129374
2021-08-14 12:40:30,479   att_loss = 0.0
2021-08-14 12:40:30,479   cls_loss = 0.20029976766693477
2021-08-14 12:40:30,479   eval_loss = 0.43189320147231025
2021-08-14 12:40:30,479   global_step = 46799
2021-08-14 12:40:30,480   loss = 0.20029976766693477
2021-08-14 12:40:30,480   rep_loss = 0.0
2021-08-14 12:41:17,997 ***** Running evaluation *****
2021-08-14 12:41:18,014   Epoch = 2 iter 47099 step
2021-08-14 12:41:18,014   Num examples = 9832
2021-08-14 12:41:18,015   Batch size = 32
2021-08-14 12:41:27,323 ***** Running evaluation *****
2021-08-14 12:41:27,342   Epoch = 3 iter 31999 step
2021-08-14 12:41:27,342   Num examples = 9832
2021-08-14 12:41:27,342   Batch size = 32
2021-08-14 12:41:29,031 ***** Eval results *****
2021-08-14 12:41:29,033   acc = 0.8370626525630595
2021-08-14 12:41:29,033   att_loss = 0.0
2021-08-14 12:41:29,033   cls_loss = 0.2003131974326298
2021-08-14 12:41:29,033   eval_loss = 0.43194998249218064
2021-08-14 12:41:29,034   global_step = 47099
2021-08-14 12:41:29,034   loss = 0.2003131974326298
2021-08-14 12:41:29,034   rep_loss = 0.0
2021-08-14 12:41:36,992 ***** Eval results *****
2021-08-14 12:41:37,072   acc = 0.09001220504475183
2021-08-14 12:41:37,073   att_loss = 0.0
2021-08-14 12:41:37,073   cls_loss = 0.07821286551044242
2021-08-14 12:41:37,073   eval_loss = 1.1453977997426863
2021-08-14 12:41:37,073   global_step = 31999
2021-08-14 12:41:37,073   loss = 0.07821286551044242
2021-08-14 12:41:37,073   rep_loss = 0.0
2021-08-14 12:41:37,074 ***** Save model *****
2021-08-14 12:41:43,629 Writing example 0 of 9832
2021-08-14 12:41:43,630 *** Example ***
2021-08-14 12:41:43,630 guid: dev_matched-0
2021-08-14 12:41:43,630 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:41:43,630 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:41:43,630 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:41:43,630 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:41:43,630 label: contradiction
2021-08-14 12:41:43,630 label_id: 2
2021-08-14 12:41:48,142 ***** Running mm evaluation *****
2021-08-14 12:41:48,142   Num examples = 9832
2021-08-14 12:41:48,142   Batch size = 32
2021-08-14 12:41:59,422 ***** Eval results *****
2021-08-14 12:41:59,422   acc = 0.09001220504475183
2021-08-14 12:41:59,422   eval_loss = 1.1453977997426863
2021-08-14 12:41:59,422   global_step = 31999
2021-08-14 12:47:46,704 ***** Running evaluation *****
2021-08-14 12:47:46,724   Epoch = 3 iter 33999 step
2021-08-14 12:47:46,724   Num examples = 9832
2021-08-14 12:47:46,724   Batch size = 32
2021-08-14 12:47:57,484 ***** Eval results *****
2021-08-14 12:47:57,484   acc = 0.09601301871440195
2021-08-14 12:47:57,484   att_loss = 0.0
2021-08-14 12:47:57,484   cls_loss = 0.07821725031087362
2021-08-14 12:47:57,484   eval_loss = 1.1417828403510057
2021-08-14 12:47:57,484   global_step = 33999
2021-08-14 12:47:57,484   loss = 0.07821725031087362
2021-08-14 12:47:57,484   rep_loss = 0.0
2021-08-14 12:47:57,558 ***** Save model *****
2021-08-14 12:48:03,751 Writing example 0 of 9832
2021-08-14 12:48:03,752 *** Example ***
2021-08-14 12:48:03,752 guid: dev_matched-0
2021-08-14 12:48:03,752 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:48:03,752 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:48:03,752 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:48:03,752 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:48:03,752 label: contradiction
2021-08-14 12:48:03,752 label_id: 2
2021-08-14 12:48:08,359 ***** Running mm evaluation *****
2021-08-14 12:48:08,359   Num examples = 9832
2021-08-14 12:48:08,360   Batch size = 32
2021-08-14 12:48:18,031 ***** Eval results *****
2021-08-14 12:48:18,031   acc = 0.09601301871440195
2021-08-14 12:48:18,031   eval_loss = 1.1417828403510057
2021-08-14 12:48:18,031   global_step = 33999
2021-08-14 12:54:03,109 ***** Running evaluation *****
2021-08-14 12:54:03,109   Epoch = 3 iter 35999 step
2021-08-14 12:54:03,110   Num examples = 9832
2021-08-14 12:54:03,110   Batch size = 32
2021-08-14 12:54:12,807 ***** Eval results *****
2021-08-14 12:54:12,807   acc = 0.08940195280716029
2021-08-14 12:54:12,807   att_loss = 0.0
2021-08-14 12:54:12,807   cls_loss = 0.07822007398909395
2021-08-14 12:54:12,807   eval_loss = 1.141109873722126
2021-08-14 12:54:12,807   global_step = 35999
2021-08-14 12:54:12,807   loss = 0.07822007398909395
2021-08-14 12:54:12,807   rep_loss = 0.0
2021-08-14 12:54:12,808 ***** Save model *****
2021-08-14 12:54:14,305 Writing example 0 of 9832
2021-08-14 12:54:14,305 *** Example ***
2021-08-14 12:54:14,305 guid: dev_matched-0
2021-08-14 12:54:14,305 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 12:54:14,306 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:54:14,306 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:54:14,306 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 12:54:14,306 label: contradiction
2021-08-14 12:54:14,306 label_id: 2
2021-08-14 12:54:18,819 ***** Running mm evaluation *****
2021-08-14 12:54:18,820   Num examples = 9832
2021-08-14 12:54:18,820   Batch size = 32
2021-08-14 12:54:28,436 ***** Eval results *****
2021-08-14 12:54:28,436   acc = 0.08940195280716029
2021-08-14 12:54:28,436   eval_loss = 1.141109873722126
2021-08-14 12:54:28,437   global_step = 35999
2021-08-14 13:00:11,461 ***** Running evaluation *****
2021-08-14 13:00:11,461   Epoch = 3 iter 37999 step
2021-08-14 13:00:11,461   Num examples = 9832
2021-08-14 13:00:11,461   Batch size = 32
2021-08-14 13:00:23,130 ***** Eval results *****
2021-08-14 13:00:23,130   acc = 0.0870626525630594
2021-08-14 13:00:23,130   att_loss = 0.0
2021-08-14 13:00:23,130   cls_loss = 0.0782209546241968
2021-08-14 13:00:23,130   eval_loss = 1.1495335210453381
2021-08-14 13:00:23,130   global_step = 37999
2021-08-14 13:00:23,130   loss = 0.0782209546241968
2021-08-14 13:00:23,131   rep_loss = 0.0
2021-08-14 13:00:23,159 ***** Save model *****
2021-08-14 13:00:30,473 Writing example 0 of 9832
2021-08-14 13:00:30,474 *** Example ***
2021-08-14 13:00:30,474 guid: dev_matched-0
2021-08-14 13:00:30,474 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:00:30,474 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:00:30,474 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:00:30,474 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:00:30,474 label: contradiction
2021-08-14 13:00:30,474 label_id: 2
2021-08-14 13:00:35,002 ***** Running mm evaluation *****
2021-08-14 13:00:35,002   Num examples = 9832
2021-08-14 13:00:35,002   Batch size = 32
2021-08-14 13:00:44,609 ***** Eval results *****
2021-08-14 13:00:44,609   acc = 0.0870626525630594
2021-08-14 13:00:44,609   eval_loss = 1.1495335210453381
2021-08-14 13:00:44,609   global_step = 37999
2021-08-14 13:06:25,674 ***** Running evaluation *****
2021-08-14 13:06:25,674   Epoch = 3 iter 39999 step
2021-08-14 13:06:25,674   Num examples = 9832
2021-08-14 13:06:25,674   Batch size = 32
2021-08-14 13:06:35,159 ***** Eval results *****
2021-08-14 13:06:35,160   acc = 0.09255492270138324
2021-08-14 13:06:35,160   att_loss = 0.0
2021-08-14 13:06:35,160   cls_loss = 0.07822321716389684
2021-08-14 13:06:35,160   eval_loss = 1.146091117100282
2021-08-14 13:06:35,160   global_step = 39999
2021-08-14 13:06:35,160   loss = 0.07822321716389684
2021-08-14 13:06:35,160   rep_loss = 0.0
2021-08-14 13:06:35,182 ***** Save model *****
2021-08-14 13:06:42,146 Writing example 0 of 9832
2021-08-14 13:06:42,146 *** Example ***
2021-08-14 13:06:42,146 guid: dev_matched-0
2021-08-14 13:06:42,147 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:06:42,147 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:06:42,147 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:06:42,147 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:06:42,147 label: contradiction
2021-08-14 13:06:42,147 label_id: 2
2021-08-14 13:06:46,713 ***** Running mm evaluation *****
2021-08-14 13:06:46,714   Num examples = 9832
2021-08-14 13:06:46,714   Batch size = 32
2021-08-14 13:06:56,214 ***** Eval results *****
2021-08-14 13:06:56,215   acc = 0.09255492270138324
2021-08-14 13:06:56,215   eval_loss = 1.146091117100282
2021-08-14 13:06:56,215   global_step = 39999
2021-08-14 13:12:38,554 ***** Running evaluation *****
2021-08-14 13:12:38,555   Epoch = 3 iter 41999 step
2021-08-14 13:12:38,555   Num examples = 9832
2021-08-14 13:12:38,555   Batch size = 32
2021-08-14 13:12:48,107 ***** Eval results *****
2021-08-14 13:12:48,107   acc = 0.09306346623270952
2021-08-14 13:12:48,107   att_loss = 0.0
2021-08-14 13:12:48,107   cls_loss = 0.07822389303449025
2021-08-14 13:12:48,107   eval_loss = 1.1437688330551246
2021-08-14 13:12:48,107   global_step = 41999
2021-08-14 13:12:48,108   loss = 0.07822389303449025
2021-08-14 13:12:48,108   rep_loss = 0.0
2021-08-14 13:12:48,124 ***** Save model *****
2021-08-14 13:12:57,328 Writing example 0 of 9832
2021-08-14 13:12:57,329 *** Example ***
2021-08-14 13:12:57,329 guid: dev_matched-0
2021-08-14 13:12:57,329 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:12:57,329 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:12:57,329 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:12:57,329 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:12:57,329 label: contradiction
2021-08-14 13:12:57,329 label_id: 2
2021-08-14 13:13:01,825 ***** Running mm evaluation *****
2021-08-14 13:13:01,825   Num examples = 9832
2021-08-14 13:13:01,825   Batch size = 32
2021-08-14 13:13:12,933 ***** Eval results *****
2021-08-14 13:13:12,933   acc = 0.09306346623270952
2021-08-14 13:13:12,933   eval_loss = 1.1437688330551246
2021-08-14 13:13:12,933   global_step = 41999
2021-08-14 13:18:54,648 ***** Running evaluation *****
2021-08-14 13:18:54,668   Epoch = 4 iter 43999 step
2021-08-14 13:18:54,668   Num examples = 9832
2021-08-14 13:18:54,668   Batch size = 32
2021-08-14 13:19:05,797 ***** Eval results *****
2021-08-14 13:19:05,797   acc = 0.08757119609438568
2021-08-14 13:19:05,797   att_loss = 0.0
2021-08-14 13:19:05,797   cls_loss = 0.07818285442733433
2021-08-14 13:19:05,797   eval_loss = 1.1499261364534303
2021-08-14 13:19:05,797   global_step = 43999
2021-08-14 13:19:05,798   loss = 0.07818285442733433
2021-08-14 13:19:05,798   rep_loss = 0.0
2021-08-14 13:19:05,805 ***** Save model *****
2021-08-14 13:19:16,574 Writing example 0 of 9832
2021-08-14 13:19:16,574 *** Example ***
2021-08-14 13:19:16,574 guid: dev_matched-0
2021-08-14 13:19:16,574 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:19:16,575 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:19:16,575 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:19:16,575 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:19:16,575 label: contradiction
2021-08-14 13:19:16,575 label_id: 2
2021-08-14 13:19:21,071 ***** Running mm evaluation *****
2021-08-14 13:19:21,072   Num examples = 9832
2021-08-14 13:19:21,072   Batch size = 32
2021-08-14 13:19:30,857 ***** Eval results *****
2021-08-14 13:19:30,857   acc = 0.08757119609438568
2021-08-14 13:19:30,857   eval_loss = 1.1499261364534303
2021-08-14 13:19:30,857   global_step = 43999
2021-08-14 13:25:16,120 ***** Running evaluation *****
2021-08-14 13:25:16,141   Epoch = 4 iter 45999 step
2021-08-14 13:25:16,141   Num examples = 9832
2021-08-14 13:25:16,141   Batch size = 32
2021-08-14 13:25:25,580 ***** Eval results *****
2021-08-14 13:25:25,580   acc = 0.08177379983726607
2021-08-14 13:25:25,580   att_loss = 0.0
2021-08-14 13:25:25,580   cls_loss = 0.078185909237396
2021-08-14 13:25:25,580   eval_loss = 1.1463805208732556
2021-08-14 13:25:25,580   global_step = 45999
2021-08-14 13:25:25,580   loss = 0.078185909237396
2021-08-14 13:25:25,580   rep_loss = 0.0
2021-08-14 13:25:25,581 ***** Save model *****
2021-08-14 13:25:27,257 Writing example 0 of 9832
2021-08-14 13:25:27,258 *** Example ***
2021-08-14 13:25:27,258 guid: dev_matched-0
2021-08-14 13:25:27,258 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:25:27,258 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:25:27,258 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:25:27,258 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:25:27,258 label: contradiction
2021-08-14 13:25:27,258 label_id: 2
2021-08-14 13:25:31,763 ***** Running mm evaluation *****
2021-08-14 13:25:31,763   Num examples = 9832
2021-08-14 13:25:31,763   Batch size = 32
2021-08-14 13:25:41,235 ***** Eval results *****
2021-08-14 13:25:41,235   acc = 0.08177379983726607
2021-08-14 13:25:41,235   eval_loss = 1.1463805208732556
2021-08-14 13:25:41,236   global_step = 45999
2021-08-14 13:31:22,723 ***** Running evaluation *****
2021-08-14 13:31:22,723   Epoch = 4 iter 47999 step
2021-08-14 13:31:22,723   Num examples = 9832
2021-08-14 13:31:22,724   Batch size = 32
2021-08-14 13:31:32,272 ***** Eval results *****
2021-08-14 13:31:32,272   acc = 0.09214808787632221
2021-08-14 13:31:32,272   att_loss = 0.0
2021-08-14 13:31:32,272   cls_loss = 0.07819300062168885
2021-08-14 13:31:32,272   eval_loss = 1.1444278595509467
2021-08-14 13:31:32,272   global_step = 47999
2021-08-14 13:31:32,272   loss = 0.07819300062168885
2021-08-14 13:31:32,272   rep_loss = 0.0
2021-08-14 13:31:32,284 ***** Save model *****
2021-08-14 13:31:34,993 Writing example 0 of 9832
2021-08-14 13:31:34,993 *** Example ***
2021-08-14 13:31:34,993 guid: dev_matched-0
2021-08-14 13:31:34,993 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:31:34,993 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:31:34,994 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:31:34,994 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:31:34,994 label: contradiction
2021-08-14 13:31:34,994 label_id: 2
2021-08-14 13:31:39,554 ***** Running mm evaluation *****
2021-08-14 13:31:39,554   Num examples = 9832
2021-08-14 13:31:39,554   Batch size = 32
2021-08-14 13:31:49,081 ***** Eval results *****
2021-08-14 13:31:49,082   acc = 0.09214808787632221
2021-08-14 13:31:49,082   eval_loss = 1.1444278595509467
2021-08-14 13:31:49,082   global_step = 47999
2021-08-14 13:37:36,426 ***** Running evaluation *****
2021-08-14 13:37:36,427   Epoch = 4 iter 49999 step
2021-08-14 13:37:36,427   Num examples = 9832
2021-08-14 13:37:36,427   Batch size = 32
2021-08-14 13:37:46,053 ***** Eval results *****
2021-08-14 13:37:46,053   acc = 0.09408055329536208
2021-08-14 13:37:46,053   att_loss = 0.0
2021-08-14 13:37:46,053   cls_loss = 0.07819619325000621
2021-08-14 13:37:46,053   eval_loss = 1.1397124103137426
2021-08-14 13:37:46,053   global_step = 49999
2021-08-14 13:37:46,053   loss = 0.07819619325000621
2021-08-14 13:37:46,053   rep_loss = 0.0
2021-08-14 13:37:46,116 ***** Save model *****
2021-08-14 13:37:52,411 Writing example 0 of 9832
2021-08-14 13:37:52,412 *** Example ***
2021-08-14 13:37:52,412 guid: dev_matched-0
2021-08-14 13:37:52,412 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:37:52,412 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:37:52,412 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:37:52,412 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:37:52,412 label: contradiction
2021-08-14 13:37:52,412 label_id: 2
2021-08-14 13:37:56,927 ***** Running mm evaluation *****
2021-08-14 13:37:56,928   Num examples = 9832
2021-08-14 13:37:56,928   Batch size = 32
2021-08-14 13:38:07,982 ***** Eval results *****
2021-08-14 13:38:07,982   acc = 0.09408055329536208
2021-08-14 13:38:07,982   eval_loss = 1.1397124103137426
2021-08-14 13:38:07,982   global_step = 49999
2021-08-14 13:43:46,115 ***** Running evaluation *****
2021-08-14 13:43:46,116   Epoch = 4 iter 51999 step
2021-08-14 13:43:46,116   Num examples = 9832
2021-08-14 13:43:46,116   Batch size = 32
2021-08-14 13:43:57,067 ***** Eval results *****
2021-08-14 13:43:57,067   acc = 0.09387713588283157
2021-08-14 13:43:57,067   att_loss = 0.0
2021-08-14 13:43:57,067   cls_loss = 0.07819468536787924
2021-08-14 13:43:57,067   eval_loss = 1.1434165645729413
2021-08-14 13:43:57,067   global_step = 51999
2021-08-14 13:43:57,067   loss = 0.07819468536787924
2021-08-14 13:43:57,067   rep_loss = 0.0
2021-08-14 13:43:57,097 ***** Save model *****
2021-08-14 13:43:57,925 Writing example 0 of 9832
2021-08-14 13:43:57,926 *** Example ***
2021-08-14 13:43:57,926 guid: dev_matched-0
2021-08-14 13:43:57,926 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2021-08-14 13:43:57,926 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:43:57,926 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:43:57,926 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-14 13:43:57,926 label: contradiction
2021-08-14 13:43:57,926 label_id: 2
2021-08-14 13:44:02,416 ***** Running mm evaluation *****
2021-08-14 13:44:02,417   Num examples = 9832
2021-08-14 13:44:02,417   Batch size = 32
2021-08-14 13:44:11,841 ***** Eval results *****
2021-08-14 13:44:11,841   acc = 0.09387713588283157
2021-08-14 13:44:11,841   eval_loss = 1.1434165645729413
2021-08-14 13:44:11,841   global_step = 51999
2021-08-15 10:17:36,951 The args: Namespace(aug_train=False, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=True, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/eval', pred_distill=False, seed=42, similarity_distill=False, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/final', task_name='MNLI', teacher_model=None, temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-15 10:17:37,071 device: cuda n_gpu: 2
2021-08-15 10:17:37,902 Writing example 0 of 9815
2021-08-15 10:17:37,902 *** Example ***
2021-08-15 10:17:37,902 guid: dev_matched-0
2021-08-15 10:17:37,902 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-15 10:17:37,903 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:17:37,903 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:17:37,903 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:17:37,903 label: neutral
2021-08-15 10:17:37,903 label_id: 1
2021-08-15 10:17:42,769 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-15 10:17:44,409 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/final/pytorch_model.bin
2021-08-15 10:17:46,579 loading model...
2021-08-15 10:17:46,601 done!
2021-08-15 10:17:54,026 ***** Running evaluation *****
2021-08-15 10:17:54,026   Num examples = 9815
2021-08-15 10:17:54,026   Batch size = 32
2021-08-15 10:18:05,366 ***** Eval results *****
2021-08-15 10:18:05,366   acc = 0.8305654610290372
2021-08-15 10:18:05,366   eval_loss = 0.4499730447410372
2021-08-15 10:24:14,950 The args: Namespace(aug_train=True, cache_dir='', data_dir='/home/mcao610/scratch/glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='/home/mcao610/scratch/TinyBERT_TEST/MNLI/similarity', pred_distill=False, seed=42, similarity_distill=True, student_model='/home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate', task_name='MNLI', teacher_model='/home/mcao610/scratch/huggingface/MNLI/uncased/', temperature=1.0, train_batch_size=128, warmup_proportion=0.1, weight_decay=0.0001)
2021-08-15 10:24:15,041 device: cuda n_gpu: 2
2021-08-15 10:24:23,291 Writing example 0 of 505555
2021-08-15 10:24:23,293 *** Example ***
2021-08-15 10:24:23,293 guid: aug-0
2021-08-15 10:24:23,293 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2021-08-15 10:24:23,293 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:24:23,293 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:24:23,293 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:24:23,293 label: neutral
2021-08-15 10:24:23,293 label_id: 1
2021-08-15 10:24:28,030 Writing example 10000 of 505555
2021-08-15 10:24:32,599 Writing example 20000 of 505555
2021-08-15 10:24:37,087 Writing example 30000 of 505555
2021-08-15 10:24:42,225 Writing example 40000 of 505555
2021-08-15 10:24:47,158 Writing example 50000 of 505555
2021-08-15 10:24:51,635 Writing example 60000 of 505555
2021-08-15 10:24:56,240 Writing example 70000 of 505555
2021-08-15 10:25:00,783 Writing example 80000 of 505555
2021-08-15 10:25:05,682 Writing example 90000 of 505555
2021-08-15 10:25:10,243 Writing example 100000 of 505555
2021-08-15 10:25:14,799 Writing example 110000 of 505555
2021-08-15 10:25:19,135 Writing example 120000 of 505555
2021-08-15 10:25:23,754 Writing example 130000 of 505555
2021-08-15 10:25:29,212 Writing example 140000 of 505555
2021-08-15 10:25:33,793 Writing example 150000 of 505555
2021-08-15 10:25:38,535 Writing example 160000 of 505555
2021-08-15 10:25:43,147 Writing example 170000 of 505555
2021-08-15 10:25:47,697 Writing example 180000 of 505555
2021-08-15 10:25:52,266 Writing example 190000 of 505555
2021-08-15 10:25:57,044 Writing example 200000 of 505555
2021-08-15 10:26:02,619 Writing example 210000 of 505555
2021-08-15 10:26:07,040 Writing example 220000 of 505555
2021-08-15 10:26:11,637 Writing example 230000 of 505555
2021-08-15 10:26:16,134 Writing example 240000 of 505555
2021-08-15 10:26:20,997 Writing example 250000 of 505555
2021-08-15 10:26:25,530 Writing example 260000 of 505555
2021-08-15 10:26:30,201 Writing example 270000 of 505555
2021-08-15 10:26:34,675 Writing example 280000 of 505555
2021-08-15 10:26:40,315 Writing example 290000 of 505555
2021-08-15 10:26:45,106 Writing example 300000 of 505555
2021-08-15 10:26:49,806 Writing example 310000 of 505555
2021-08-15 10:26:54,187 Writing example 320000 of 505555
2021-08-15 10:26:58,857 Writing example 330000 of 505555
2021-08-15 10:27:03,254 Writing example 340000 of 505555
2021-08-15 10:27:07,735 Writing example 350000 of 505555
2021-08-15 10:27:12,570 Writing example 360000 of 505555
2021-08-15 10:27:16,955 Writing example 370000 of 505555
2021-08-15 10:27:21,407 Writing example 380000 of 505555
2021-08-15 10:27:27,294 Writing example 390000 of 505555
2021-08-15 10:27:31,870 Writing example 400000 of 505555
2021-08-15 10:27:36,682 Writing example 410000 of 505555
2021-08-15 10:27:41,227 Writing example 420000 of 505555
2021-08-15 10:27:45,577 Writing example 430000 of 505555
2021-08-15 10:27:49,953 Writing example 440000 of 505555
2021-08-15 10:27:54,582 Writing example 450000 of 505555
2021-08-15 10:27:59,289 Writing example 460000 of 505555
2021-08-15 10:28:03,944 Writing example 470000 of 505555
2021-08-15 10:28:08,809 Writing example 480000 of 505555
2021-08-15 10:28:13,355 Writing example 490000 of 505555
2021-08-15 10:28:17,868 Writing example 500000 of 505555
2021-08-15 10:28:24,974 Writing example 0 of 9815
2021-08-15 10:28:24,974 *** Example ***
2021-08-15 10:28:24,974 guid: dev_matched-0
2021-08-15 10:28:24,974 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2021-08-15 10:28:24,974 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:28:24,974 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:28:24,974 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2021-08-15 10:28:24,975 label: neutral
2021-08-15 10:28:24,975 label_id: 1
2021-08-15 10:28:29,425 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2021-08-15 10:28:31,844 Loading model /home/mcao610/scratch/huggingface/MNLI/uncased/pytorch_model.bin
2021-08-15 10:28:35,195 loading model...
2021-08-15 10:28:35,228 done!
2021-08-15 10:28:35,228 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2021-08-15 10:28:35,228 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2021-08-15 10:28:38,113 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-08-15 10:28:39,614 Loading model /home/mcao610/scratch/TinyBERT_TEST/MNLI/intermediate/pytorch_model.bin
2021-08-15 10:28:41,445 loading model...
2021-08-15 10:28:41,457 done!
2021-08-15 10:28:41,524 ***** Running training *****
2021-08-15 10:28:41,524   Num examples = 505555
2021-08-15 10:28:41,524   Batch size = 128
2021-08-15 10:28:41,524   Num steps = 19745
2021-08-15 10:28:41,525 n: module.bert.embeddings.word_embeddings.weight
2021-08-15 10:28:41,525 n: module.bert.embeddings.position_embeddings.weight
2021-08-15 10:28:41,526 n: module.bert.embeddings.token_type_embeddings.weight
2021-08-15 10:28:41,526 n: module.bert.embeddings.LayerNorm.weight
2021-08-15 10:28:41,526 n: module.bert.embeddings.LayerNorm.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.query.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.query.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.key.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.key.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.value.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.self.value.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.output.dense.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.output.dense.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.intermediate.dense.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.intermediate.dense.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.output.dense.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.output.dense.bias
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2021-08-15 10:28:41,526 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.query.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.query.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.key.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.key.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.value.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.self.value.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.output.dense.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.output.dense.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.intermediate.dense.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.intermediate.dense.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.output.dense.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.output.dense.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.2.attention.self.query.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.2.attention.self.query.bias
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.2.attention.self.key.weight
2021-08-15 10:28:41,527 n: module.bert.encoder.layer.2.attention.self.key.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.self.value.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.self.value.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.output.dense.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.output.dense.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.intermediate.dense.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.intermediate.dense.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.output.dense.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.output.dense.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.query.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.query.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.key.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.key.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.value.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.self.value.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.output.dense.weight
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.output.dense.bias
2021-08-15 10:28:41,528 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.intermediate.dense.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.intermediate.dense.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.output.dense.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.output.dense.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.query.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.query.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.key.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.key.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.value.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.self.value.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.output.dense.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.output.dense.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.intermediate.dense.weight
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.intermediate.dense.bias
2021-08-15 10:28:41,529 n: module.bert.encoder.layer.4.output.dense.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.4.output.dense.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.query.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.query.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.key.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.key.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.value.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.self.value.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.output.dense.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.output.dense.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.intermediate.dense.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.intermediate.dense.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.output.dense.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.output.dense.bias
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2021-08-15 10:28:41,530 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2021-08-15 10:28:41,530 n: module.bert.pooler.dense.weight
2021-08-15 10:28:41,531 n: module.bert.pooler.dense.bias
2021-08-15 10:28:41,531 n: module.classifier.weight
2021-08-15 10:28:41,531 n: module.classifier.bias
2021-08-15 10:28:41,531 n: module.fit_dense.weight
2021-08-15 10:28:41,531 n: module.fit_dense.bias
2021-08-15 10:28:41,531 Total parameters: 67547907
